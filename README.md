> **æœ€è¿‘æ›´æ–° 2020 å¹´ 11 æœˆ 9 æ—¥ï¼š10 æœˆè®ºæ–‡æ›´æ–° +22**

# Visual_SLAM_Related_Research
> @Authorï¼šå´è‰³æ•    
@E-mail ï¼šwuyanminmax@gmail.com    
@github ï¼š[yanmin-wu](https://github.com/yanmin-wu) | [wuxiaolang](https://github.com/wuxiaolang)

## å‰è¨€
&emsp;&emsp;ä»¥ä¸‹æ”¶é›†çš„è®ºæ–‡ã€ä»£ç ç­‰èµ„æ–™ä¸»è¦ä¸æœ¬äººçš„å­¦ä¹ æ–¹å‘ **è§†è§‰ SLAMã€å¢å¼ºç°å®** ç›¸å…³ã€‚    
&emsp;&emsp;ç›®å‰**é‡ç‚¹å…³æ³¨ VOã€ç‰©ä½“çº§ SLAM å’Œè¯­ä¹‰æ•°æ®å…³è”**ï¼Œ å¯¹ä¼ æ„Ÿå™¨èåˆã€ç¨ å¯†å»ºå›¾ä¹Ÿç•¥æœ‰å…³æ³¨ï¼Œæ‰€ä»¥èµ„æ–™çš„æ”¶é›†èŒƒå›´ä¹Ÿä¸è‡ªå·±çš„å…´è¶£æ¯”è¾ƒä¸€è‡´ï¼Œæ— æ³•æ¶µç›–è§†è§‰ SLAM çš„æ‰€æœ‰ç ”ç©¶ï¼Œ**è¯·è°¨æ…å‚è€ƒ**ã€‚ä¸»è¦å†…å®¹åŒ…æ‹¬ï¼š   
`1. å¼€æºä»£ç ` ï¼šç»å…¸ã€ä¼˜ç§€çš„**å¼€æºå·¥ç¨‹**    
`2. ä¼˜ç§€ä½œè€…ä¸å®éªŒå®¤` ï¼šåœ¨è‡ªå·±æ„Ÿå…´è¶£é¢†åŸŸæ¯”è¾ƒ**ä¼˜ç§€çš„å€¼å¾—å…³æ³¨çš„å›¢é˜Ÿæˆ–ä¸ªäºº**    
`3. SLAM å­¦ä¹ èµ„æ–™` ï¼š**SLAM ç›¸å…³å­¦ä¹ èµ„æ–™**ã€è§†é¢‘ã€æ•°æ®é›†ã€å…¬ä¼—å·å’Œä»£ç æ³¨é‡Š    
`4. è¿‘æœŸè®ºæ–‡` ï¼šè‡ªå·±æ„Ÿå…´è¶£æ–¹å‘çš„**æœ€æ–°è®ºæ–‡**ï¼Œå¤§æ¦‚**ä¸€ä¸ªæœˆä¸€æ›´æ–°**ï¼ˆéƒ¨åˆ†è®ºæ–‡è´¨é‡æ— æ³•ä¿è¯ï¼Œä¸»è¦ä»¥å¯¹ç°é˜¶æ®µçš„å·¥ä½œå¯èƒ½æœ‰ç”¨ä¸ºæ”¶å½•åŸåˆ™ï¼Œè¯·è°¨æ…å‚è€ƒï¼‰ã€‚éƒ¨åˆ†è®ºæ–‡çš„è¯¦/æ³›è¯»ç¬”è®°æ”¾åœ¨æˆ‘çš„[åšå®¢](https://wym.netlify.com/post/)/[List](https://zhuanlan.zhihu.com/p/138737870)ä¸Šã€‚    
&emsp;&emsp;**`æ³¨ï¼šè‹¥æœ¬ä»“åº“å†…å®¹å‡ºç°ä»»ä½•é”™è¯¯è¯·æ‰¹è¯„æŒ‡å‡ºï¼Œå®šåŠæ—¶ä¿®æ”¹ã€‚`**    
&emsp;&emsp;æœ¬ä»“åº“äº 2019 å¹´ 3 æœˆï¼ˆç ”ä¸€ä¸‹ï¼‰å¼€å§‹æ•´ç†ï¼ˆç§å¯†ï¼‰ğŸŒšã€‚    
&emsp;&emsp;æœ¬ä»“åº“äº 2020 å¹´ 3 æœˆï¼ˆç ”äºŒä¸‹ï¼‰å…¬å¼€ï¼Œæ­£å¥½ä¸€å‘¨å¹´ğŸŒã€‚    
  > **ä¸€äº›æ€»ç»“æ€§åšå®¢æˆ–åŸåˆ›æ€§å·¥ä½œï¼š**    
  > **1.** [100 é¡¹å¼€æºè§†è§‰ SLAM é¡¹ç›®å¤Ÿä½ ç”¨äº†å—ï¼Ÿ](https://zhuanlan.zhihu.com/p/115599978) 2020 å¹´ 3 æœˆ 31 æ—¥å‘å¸ƒäº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/115599978)å’Œå…¬ä¼—å·[3D è§†è§‰å·¥åŠ] &emsp; &emsp; | [PDF ç‰ˆæœ¬](https://gitee.com/wuxiaolang2008/Notes_of_wu/raw/master/blog/20200330_%E5%BC%80%E6%BA%90SLAM%E4%BB%A3%E7%A0%81%E5%90%88%E9%9B%86_by_%E5%90%B4%E8%89%B3%E6%95%8F.pdf)    
  > **2.** [SLAM é¢†åŸŸå›½å†…å¤–ä¼˜ç§€å®éªŒå®¤æ±‡æ€»](https://zhuanlan.zhihu.com/p/130530891)  &emsp; &emsp; &nbsp; 2020 å¹´ 4 æœˆ 26 æ—¥å‘å¸ƒäº[çŸ¥ä¹](https://zhuanlan.zhihu.com/p/130530891)å’Œå…¬ä¼—å·[æ³¡æ³¡æœºå™¨äºº SLAM] | [PDF ç‰ˆæœ¬](https://gitee.com/wuxiaolang2008/Notes_of_wu/raw/master/blog/20200418_SLAM%E9%A2%86%E5%9F%9F%E5%9B%BD%E5%86%85%E5%A4%96%E4%BC%98%E7%A7%80%E5%AE%9E%E9%AA%8C%E5%AE%A4%E6%B1%87%E6%80%BB_by_%E5%90%B4%E8%89%B3%E6%95%8F.pdf)    
  > **3.** å•ç›®ç‰©ä½“çº§ SLAM ä¸­çš„æ•°æ®å…³è”é—®é¢˜ï¼š **EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association**, IROS 2020, Wu Y, Zhang Y, Zhu D, Feng Y, S Coleman, D Kerr [[**PDF**](https://arxiv.org/abs/2004.12730)] [[**Code**](https://github.com/yanmin-wu/EAO-SLAM)] [[**YouTube**](https://youtu.be/pvwdQoV1KBI)] [[**Bilibili**](https://www.bilibili.com/video/av94805216)] [[**Project page**](https://yanmin-wu.github.io/project/eaoslam/)]

## ç›®å½•
> <small>æ¨èä½¿ç”¨ [GayHub](https://github.com/jawil/GayHub) æ’ä»¶è‡ªåŠ¨åœ¨ä¾§æ å±•å¼€ç›®å½•</small>
+ [**1.å¼€æºä»£ç **](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#1-%E5%BC%80%E6%BA%90%E4%BB%A3%E7%A0%81)
  + [**1.1 Geometric SLAM**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-11-geometric-slam)
  + [**1.2 Semantic / Deep SLAM**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-12-semantic--deep-slam)
  + [**1.3 Multi-Landmarks / Object SLAM**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-13-multi-landmarks--object-slam)
  + [**1.4 Sensor Fusion**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-14-sensor-fusion)
  + [**1.5 Dynamic SLAM**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-15-dynamic-slam)
  + [**1.6 Mapping**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-16-mapping)
  + [**1.7 Optimization**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#smile-17-optimization)
+ [**2. ä¼˜ç§€ä½œè€…ä¸å®éªŒå®¤**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2-%E4%BC%98%E7%A7%80%E4%BD%9C%E8%80%85%E4%B8%8E%E5%AE%9E%E9%AA%8C%E5%AE%A4)
+ [**3. SLAM å­¦ä¹ èµ„æ–™**](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#3-SLAM-%E5%AD%A6%E4%B9%A0%E8%B5%84%E6%96%99)
+ [**4. è¿‘æœŸè®ºæ–‡æ›´æ–°**](https://github.com/wuxiaolang/Mark/blob/master/README.md#4-%E8%BF%91%E6%9C%9F%E8%AE%BA%E6%96%87%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0)
  + [2020 å¹´ 10 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-10-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B022-%E7%AF%87)
  + [2020 å¹´ 09 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-9-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B020-%E7%AF%87)
  + [2020 å¹´ 08 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ30 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-8-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B030-%E7%AF%87)
  + [2020 å¹´ 07 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-7-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B020-%E7%AF%87)
  + [2020 å¹´ 06 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-6-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B020-%E7%AF%87)
  + [2020 å¹´ 05 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-5-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B020-%E7%AF%87)
  + [2020 å¹´ 04 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-4-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B022-%E7%AF%87)
  + [2020 å¹´ 03 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ23 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-3-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B023-%E7%AF%87)
  + [2020 å¹´ 02 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-2-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B017-%E7%AF%87)
  + [2020 å¹´ 01 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ26 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2020-%E5%B9%B4-1-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B026-%E7%AF%87)
  + -- â†‘ 2020å¹´ â†‘ === â†“ 2019å¹´ â†“ --
  + [2019 å¹´ 12 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ23 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-12-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B023-%E7%AF%87)
  + [2019 å¹´ 11 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-11-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B017-%E7%AF%87)
  + [2019 å¹´ 10 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-10-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B022-%E7%AF%87) 
  + [2019 å¹´ 09 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ24 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-9-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B024-%E7%AF%87)
  + [2019 å¹´ 08 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ26 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-8-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B026-%E7%AF%87)
  + [2019 å¹´ 07 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ36 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-7-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B036-%E7%AF%87)
  + [2019 å¹´ 06 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ21 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-6-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B021-%E7%AF%87)
  + [2019 å¹´ 05 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ51 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-5-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B051-%E7%AF%87)
  + [2019 å¹´ 04 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-4-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B017-%E7%AF%87)
  + [2019 å¹´ 03 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ13 ç¯‡ï¼‰](https://github.com/wuxiaolang/Visual_SLAM_Related_Research#2019-%E5%B9%B4-3-%E6%9C%88%E8%AE%BA%E6%96%87%E6%9B%B4%E6%96%B013-%E7%AF%87)

---

## 1. å¼€æºä»£ç 
> è¿™ä¸€éƒ¨åˆ†æ•´ç†ä¹‹åå‘å¸ƒåœ¨çŸ¥ä¹ï¼ˆ2020 å¹´ 3 æœˆ 31 æ—¥ï¼‰ï¼šhttps://zhuanlan.zhihu.com/p/115599978/

### :smile: 1.1 Geometric SLAM

#### 1. PTAM 

+ **è®ºæ–‡**ï¼šKlein G, Murray D. [**Parallel tracking and mapping for small AR workspaces**](http://www.robots.ox.ac.uk/ActiveVision/Publications/klein_murray_ismar2007/klein_murray_ismar2007.pdf)[C]//Mixed and Augmented Reality, 2007. ISMAR 2007. 6th IEEE and ACM International Symposium on. IEEE, **2007**: 225-234.
+ **ä»£ç **ï¼šhttps://github.com/Oxford-PTAM/PTAM-GPL
+ å·¥ç¨‹åœ°å€ï¼šhttp://www.robots.ox.ac.uk/~gk/PTAM/
+ ä½œè€…å…¶ä»–ç ”ç©¶ï¼šhttp://www.robots.ox.ac.uk/~gk/publications.html

#### 2. S-PTAMï¼ˆåŒç›® PTAMï¼‰

+ **è®ºæ–‡**ï¼šTaihÃº Pire,Thomas Fischer, GastÃ³n Castro, Pablo De CristÃ³foris, Javier Civera and Julio Jacobo Berlles. [**S-PTAM: Stereo Parallel Tracking and Mapping**](http://webdiis.unizar.es/~jcivera/papers/pire_etal_ras17.pdf). Robotics and Autonomous Systems, **2017**.
+ **ä»£ç **ï¼šhttps://github.com/lrse/sptam
+ ä½œè€…å…¶ä»–è®ºæ–‡ï¼šCastro G, Nitsche M A, Pire T, et al. [**Efficient on-board Stereo SLAM through constrained-covisibility strategies**](https://www.researchgate.net/profile/Gaston_Castro/publication/332147108_Efficient_on-board_Stereo_SLAM_through_constrained-covisibility_strategies/links/5cacb327a6fdccfa0e7c3e4b/Efficient-on-board-Stereo-SLAM-through-constrained-covisibility-strategies.pdf)[J]. Robotics and Autonomous Systems, **2019**.

#### 3. MonoSLAM

+ **è®ºæ–‡**ï¼šDavison A J, Reid I D, Molton N D, et al. [**MonoSLAM: Real-time single camera SLAM**](https://ieeexplore.ieee.org/abstract/document/4160954/)[J]. IEEE transactions on pattern analysis and machine intelligence, **2007**, 29(6): 1052-1067.
+ **ä»£ç **ï¼šhttps://github.com/hanmekim/SceneLib2

#### 4. ORB-SLAM2

+ **è®ºæ–‡**ï¼šMur-Artal R, TardÃ³s J D. [**Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras**](https://github.com/raulmur/ORB_SLAM2)[J]. IEEE Transactions on Robotics, **2017**, 33(5): 1255-1262.
+ **ä»£ç **ï¼šhttps://github.com/raulmur/ORB_SLAM2
+ ä½œè€…å…¶ä»–è®ºæ–‡ï¼š
    + **å•ç›®åŠç¨ å¯†å»ºå›¾**ï¼šMur-Artal R, TardÃ³s J D. [Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM](https://www.researchgate.net/profile/Raul_Mur-Artal/publication/282807894_Probabilistic_Semi-Dense_Mapping_from_Highly_Accurate_Feature-Based_Monocular_SLAM/links/561cd04308ae6d17308ce267.pdf)[C]//Robotics: Science and Systems. **2015**, 2015.
    + **VIORB**ï¼šMur-Artal R, TardÃ³s J D. [Visual-inertial monocular SLAM with map reuse](https://arxiv.org/pdf/1610.05949.pdf)[J]. IEEE Robotics and Automation Letters, **2017**, 2(2): 796-803.
    + **å¤šåœ°å›¾**ï¼šElvira R, TardÃ³s J D, Montiel J M M. [ORBSLAM-Atlas: a robust and accurate multi-map system](https://arxiv.org/pdf/1908.11585)[J]. arXiv preprint arXiv:1908.11585, **2019**.

> ä»¥ä¸‹5, 6, 7, 8å‡ é¡¹æ˜¯ TUM è®¡ç®—æœºè§†è§‰ç»„å…¨å®¶æ¡¶ï¼Œå®˜æ–¹ä¸»é¡µï¼šhttps://vision.in.tum.de/research/vslam/dso

#### 5. DSO

+ **è®ºæ–‡**ï¼šEngel J, Koltun V, Cremers D. [**Direct sparse odometry**](https://ieeexplore.ieee.org/iel7/34/4359286/07898369.pdf)[J]. IEEE transactions on pattern analysis and machine intelligence, **2017**, 40(3): 611-625.
+ **ä»£ç **ï¼šhttps://github.com/JakobEngel/dso
+ **åŒç›® DSO**ï¼šWang R, Schworer M, Cremers D. [**Stereo DSO: Large-scale direct sparse visual odometry with stereo cameras**](http://openaccess.thecvf.com/content_ICCV_2017/papers/Wang_Stereo_DSO_Large-Scale_ICCV_2017_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2017**: 3903-3911.
+ **VI-DSO**ï¼šVon Stumberg L, Usenko V, Cremers D. [**Direct sparse visual-inertial odometry using dynamic marginalization**](https://arxiv.org/pdf/1804.05625)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 2510-2517.

#### 6. LDSO

+ é«˜ç¿”åœ¨ DSO ä¸Šæ·»åŠ é—­ç¯çš„å·¥ä½œ
+ **è®ºæ–‡**ï¼šGao X, Wang R, Demmel N, et al. [**LDSO: Direct sparse odometry with loop closure**](https://arxiv.org/pdf/1808.01111)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 2198-2204.
+ **ä»£ç **ï¼šhttps://github.com/tum-vision/LDSO

#### 7. LSD-SLAM

+ **è®ºæ–‡**ï¼šEngel J, SchÃ¶ps T, Cremers D. [**LSD-SLAM: Large-scale direct monocular SLAM**](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.7193&rep=rep1&type=pdf)[C]//European conference on computer vision. Springer, Cham, **2014**: 834-849.
+ **ä»£ç **ï¼šhttps://github.com/tum-vision/lsd_slam

#### 8. DVO-SLAM

+ **è®ºæ–‡**ï¼šKerl C, Sturm J, Cremers D. [**Dense visual SLAM for RGB-D cameras**]()[C]//2013 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, **2013**: 2100-2106.
+ **ä»£ç  1**ï¼šhttps://github.com/tum-vision/dvo_slam
+ **ä»£ç  2**ï¼šhttps://github.com/tum-vision/dvo
+ å…¶ä»–è®ºæ–‡ï¼š
    + Kerl C, Sturm J, Cremers D. [**Robust odometry estimation for RGB-D cameras**](https://vision.in.tum.de/_media/spezial/bib/kerl13icra.pdf)[C]//2013 IEEE international conference on robotics and automation. IEEE, **2013**: 3748-3754.
    + SteinbrÃ¼cker F, Sturm J, Cremers D. [**Real-time visual odometry from dense RGB-D images**](https://jsturm.de/publications/data/steinbruecker_sturm_cremers_iccv11.pdf)[C]//2011 IEEE international conference on computer vision workshops (ICCV Workshops). IEEE, **2011**: 719-722.

#### 9. SVO

+ [è‹é»ä¸–å¤§å­¦æœºå™¨äººä¸æ„ŸçŸ¥è¯¾é¢˜ç»„](http://rpg.ifi.uzh.ch/publications.html)
+ **è®ºæ–‡**ï¼šForster C, Pizzoli M, Scaramuzza D. [**SVO: Fast semi-direct monocular visual odometry**](https://www.zora.uzh.ch/id/eprint/125453/1/ICRA14_Forster.pdf)[C]//2014 IEEE international conference on robotics and automation (ICRA). IEEE, **2014**: 15-22.
+ **ä»£ç **ï¼šhttps://github.com/uzh-rpg/rpg_svo
+ Forster C, Zhang Z, Gassner M, et al. [**SVO: Semidirect visual odometry for monocular and multicamera systems**](https://www.zora.uzh.ch/id/eprint/127902/1/TRO16_Forster-SVO.pdf)[J]. IEEE Transactions on Robotics, **2016**, 33(2): 249-265.

#### 10. DSM

+ **è®ºæ–‡**ï¼šZubizarreta J, Aguinaga I, Montiel J M M. [**Direct sparse mapping**](https://arxiv.org/pdf/1904.06577)[J]. arXiv preprint arXiv:1904.06577, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/jzubizarreta/dsm ï¼›[Video](https://www.youtube.com/watch?v=sj1GIF-7BYo&feature=youtu.be)

#### 11. openvslam

+ è®ºæ–‡ï¼šSumikura S, Shibuya M, Sakurada K. [**OpenVSLAM: A Versatile Visual SLAM Framework**](https://dl.acm.org/ft_gateway.cfm?id=3350539&type=pdf)[C]//Proceedings of the 27th ACM International Conference on Multimedia. **2019**: 2292-2295.
+ ä»£ç ï¼šhttps://github.com/xdspacelab/openvslam ï¼›[æ–‡æ¡£](https://openvslam.readthedocs.io/en/master/)

#### 12. se2lamï¼ˆåœ°é¢è½¦è¾†ä½å§¿ä¼°è®¡çš„è§†è§‰é‡Œç¨‹è®¡ï¼‰

+ **è®ºæ–‡**ï¼šZheng F, Liu Y H. [**Visual-Odometric Localization and Mapping for Ground Vehicles Using SE (2)-XYZ Constraints**](https://ieeexplore.ieee.org/abstract/document/8793928)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 3556-3562.
+ **ä»£ç **ï¼šhttps://github.com/izhengfan/se2lam
+ ä½œè€…çš„å¦å¤–ä¸€é¡¹å·¥ä½œ
    + è®ºæ–‡ï¼šZheng F, Tang H, Liu Y H. [**Odometry-vision-based ground vehicle motion estimation with se (2)-constrained se (3) poses**](https://ieeexplore.ieee.org/abstract/document/8357438/)[J]. IEEE transactions on cybernetics, **2018**, 49(7): 2652-2663.
    + ä»£ç ï¼šhttps://github.com/izhengfan/se2clam

#### 13. GraphSfMï¼ˆåŸºäºå›¾çš„å¹¶è¡Œå¤§è§„æ¨¡ SFMï¼‰

+ è®ºæ–‡ï¼šChen Y, Shen S, Chen Y, et al. [**Graph-Based Parallel Large Scale Structure from Motion**](https://arxiv.org/pdf/1912.10659.pdf)[J]. arXiv preprint arXiv:1912.10659, **2019**.
+ ä»£ç ï¼šhttps://github.com/AIBluefisher/GraphSfM

#### 14. LCSD_SLAMï¼ˆæ¾è€¦åˆçš„åŠç›´æ¥æ³•å•ç›® SLAMï¼‰

+ **è®ºæ–‡**ï¼šLee S H, Civera J. [**Loosely-Coupled semi-direct monocular SLAM**](https://arxiv.org/pdf/1807.10073)[J]. IEEE Robotics and Automation Letters, **2018**, 4(2): 399-406.
+ **ä»£ç **ï¼šhttps://github.com/sunghoon031/LCSD_SLAM ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=FeMFP7EAAAAJ&hl=zh-CN&oi=sra) ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=j7WnU7ZpZ8c&feature=youtu.be)
+ ä½œè€…å¦å¤–ä¸€ç¯‡å…³äº**å•ç›®å°ºåº¦**çš„æ–‡ç«  [ä»£ç å¼€æº](https://github.com/sunghoon031/stability_scale) ï¼šLee S H, de Croon G. [**Stability-based scale estimation for monocular SLAM**](https://www.researchgate.net/profile/Seong_Hun_Lee3/publication/322260802_Stability-based_Scale_Estimation_for_Monocular_SLAM/links/5b3def9b0f7e9b0df5f42d67/Stability-based-Scale-Estimation-for-Monocular-SLAM.pdf)[J]. IEEE Robotics and Automation Letters, **2018**, 3(2): 780-787.

#### 15. RESLAMï¼ˆåŸºäºè¾¹çš„ SLAMï¼‰

+ **è®ºæ–‡**ï¼šSchenk F, Fraundorfer F. [**RESLAM: A real-time robust edge-based SLAM system**](https://ieeexplore.ieee.org/abstract/document/8794462/)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 154-160.
+ **ä»£ç **ï¼šhttps://github.com/fabianschenk/RESLAM ï¼› [é¡¹ç›®ä¸»é¡µ](https://graz.pure.elsevier.com/en/publications/reslam-a-real-time-robust-edge-based-slam-system)

#### 16. scale_optimizationï¼ˆå°†å•ç›® DSO æ‹“å±•åˆ°åŒç›®ï¼‰

+ **è®ºæ–‡**ï¼šMo J, Sattar J. [**Extending Monocular Visual Odometry to Stereo Camera System by Scale Optimization**](https://arxiv.org/pdf/1905.12723.pdf)[C]. International Conference on Intelligent Robots and Systems (**IROS**), **2019**.
+ **ä»£ç **ï¼šhttps://github.com/jiawei-mo/scale_optimization

#### 17. BAD-SLAMï¼ˆç›´æ¥æ³• RGB-D SLAMï¼‰

+ **è®ºæ–‡**ï¼šSchops T, Sattler T, Pollefeys M. [**BAD SLAM: Bundle Adjusted Direct RGB-D SLAM**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2019**: 134-144.
+ **ä»£ç **ï¼šhttps://github.com/ETH3D/badslam

#### 18. GSLAMï¼ˆé›†æˆ ORB-SLAM2ï¼ŒDSOï¼ŒSVO çš„é€šç”¨æ¡†æ¶ï¼‰

+ **è®ºæ–‡**ï¼šZhao Y, Xu S, Bu S, et al. [**GSLAM: A general SLAM framework and benchmark**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2019**: 1110-1120.
+ **ä»£ç **ï¼šhttps://github.com/zdzhaoyong/GSLAM

#### 19. ARM-VOï¼ˆè¿è¡Œäº ARM å¤„ç†å™¨ä¸Šçš„å•ç›® VOï¼‰

+ **è®ºæ–‡**ï¼šNejad Z Z, Ahmadabadian A H. [**ARM-VO: an efficient monocular visual odometry for ground vehicles on ARM CPUs**](https://link.springer.com/article/10.1007/s00138-019-01037-5)[J]. Machine Vision and Applications, **2019**: 1-10.
+ **ä»£ç **ï¼šhttps://github.com/zanazakaryaie/ARM-VO

#### 20. cvo-rgbdï¼ˆç›´æ¥æ³• RGB-D VOï¼‰

+ **è®ºæ–‡**ï¼šGhaffari M, Clark W, Bloch A, et al. [**Continuous Direct Sparse Visual Odometry from RGB-D Images**](https://arxiv.org/pdf/1904.02266.pdf)[J]. arXiv preprint arXiv:1904.02266, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/MaaniGhaffari/cvo-rgbd

#### 21. Map2DFusionï¼ˆå•ç›® SLAM æ— äººæœºå›¾åƒæ‹¼æ¥ï¼‰

+ **è®ºæ–‡**ï¼šBu S, Zhao Y, Wan G, et al. [**Map2DFusion: Real-time incremental UAV image mosaicing based on monocular slam**](http://www.adv-ci.com/publications/2016_IROS.pdf)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 4564-4571.
+ **ä»£ç **ï¼šhttps://github.com/zdzhaoyong/Map2DFusion

#### 22. CCM-SLAMï¼ˆå¤šæœºå™¨äººååŒå•ç›® SLAMï¼‰

+ **è®ºæ–‡**ï¼šSchmuck P, Chli M. [**CCMâ€SLAM: Robust and efficient centralized collaborative monocular simultaneous localization and mapping for robotic teams**](https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21854)[J]. Journal of Field Robotics, **2019**, 36(4): 763-781.
+ **ä»£ç **ï¼šhttps://github.com/VIS4ROB-lab/ccm_slam &emsp; [Video](https://www.youtube.com/watch?v=P3b7UiTlmbQ&feature=youtu.be)

#### 23. ORB-SLAM3

+ è®ºæ–‡ï¼šCarlos Campos, Richard Elvira, et al.[**ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM**](https://arxiv.org/abs/2007.11898)[J]. arXiv preprint arXiv:2007.11898, **2020**.
+ ä»£ç ï¼šhttps://github.com/UZ-SLAMLab/ORB_SLAM3 | [Video](https://www.youtube.com/channel/UCXVt-kXG6T95Z4tVaYlU80Q)

### :smile: 1.2 Semantic / Deep SLAM

#### 1. MsakFusion

+ **è®ºæ–‡**ï¼šRunz M, Buffier M, Agapito L. [**Maskfusion: Real-time recognition, tracking and reconstruction of multiple moving objects**](https://arxiv.org/pdf/1804.09194)[C]//2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, **2018**: 10-20.
+ **ä»£ç **ï¼šhttps://github.com/martinruenz/maskfusion

#### 2. SemanticFusion

+ **è®ºæ–‡**ï¼šMcCormac J, Handa A, Davison A, et al. [**Semanticfusion: Dense 3d semantic mapping with convolutional neural networks**](https://arxiv.org/pdf/1609.05130.pdf)[C]//2017 IEEE International Conference on Robotics and automation (ICRA). IEEE, **2017**: 4628-4635.
+ **ä»£ç **ï¼šhttps://github.com/seaun163/semanticfusion

#### 3. semantic_3d_mapping

+ **è®ºæ–‡**ï¼šYang S, Huang Y, Scherer S. [**Semantic 3D occupancy mapping through efficient high order CRFs**](https://arxiv.org/pdf/1707.07388.pdf)[C]//2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2017**: 590-597.
+ **ä»£ç **ï¼šhttps://github.com/shichaoy/semantic_3d_mapping

#### 4. Kimeraï¼ˆå®æ—¶åº¦é‡ä¸è¯­ä¹‰å®šä½å»ºå›¾å¼€æºåº“ï¼‰

+ è®ºæ–‡ï¼šRosinol A, Abate M, Chang Y, et al. [**Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping**](https://arxiv.org/pdf/1910.02490.pdf)[J]. arXiv preprint arXiv:1910.02490, **2019**.
+ ä»£ç ï¼šhttps://github.com/MIT-SPARK/Kimera ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=3lVD0i-5p10)

#### 5. NeuroSLAMï¼ˆè„‘å¯å‘å¼ SLAMï¼‰

+ **è®ºæ–‡**ï¼šYu F, Shang J, Hu Y, et al. [**NeuroSLAM: a brain-inspired SLAM system for 3D environments**](https://www.researchgate.net/profile/Fangwen_Yu/publication/336164484_NeuroSLAM_a_brain-inspired_SLAM_system_for_3D_environments/links/5d95f38d458515c1d3908a20/NeuroSLAM-a-brain-inspired-SLAM-system-for-3D-environments.pdf)[J]. Biological Cybernetics, **2019**: 1-31.
+ **ä»£ç **ï¼šhttps://github.com/cognav/NeuroSLAM
+ ç¬¬å››ä½œè€…å°±æ˜¯ Rat SLAM çš„ä½œè€…ï¼Œæ–‡ç« ä¹Ÿæ¯”è¾ƒäº†åä½™ç§è„‘å¯å‘å¼çš„ SLAM

#### 6. gradSLAMï¼ˆè‡ªåŠ¨åˆ†åŒºçš„ç¨ å¯† SLAMï¼‰

+ **è®ºæ–‡**ï¼šJatavallabhula K M, Iyer G, Paull L. [**gradSLAM: Dense SLAM meets Automatic Differentiation**](https://arxiv.org/pdf/1910.10672.pdf)[J]. arXiv preprint arXiv:1910.10672, **2019**.
+ **ä»£ç **ï¼ˆé¢„è®¡ 20 å¹´ 4 æœˆæ”¾å‡ºï¼‰ï¼šhttps://github.com/montrealrobotics/gradSLAM ï¼›[é¡¹ç›®ä¸»é¡µ](http://montrealrobotics.ca/gradSLAM/)ï¼Œ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=2ygtSJTmo08&feature=youtu.be)

#### 7. ORB-SLAM2 + ç›®æ ‡æ£€æµ‹/åˆ†å‰²çš„æ–¹æ¡ˆè¯­ä¹‰å»ºå›¾

+ https://github.com/floatlazer/semantic_slam
+ https://github.com/qixuxiang/orb-slam2_with_semantic_labelling
+ https://github.com/Ewenwan/ORB_SLAM2_SSD_Semantic

#### 8. SIVOï¼ˆè¯­ä¹‰è¾…åŠ©ç‰¹å¾é€‰æ‹©ï¼‰

+ **è®ºæ–‡**ï¼šGanti P, Waslander S. [**Network Uncertainty Informed Semantic Feature Selection for Visual SLAM**](https://ieeexplore.ieee.org/abstract/document/8781616)[C]//2019 16th Conference on Computer and Robot Vision (CRV). IEEE, **2019**: 121-128.
+ **ä»£ç **ï¼šhttps://github.com/navganti/SIVO

#### 9. FILDï¼ˆä¸´è¿‘å›¾å¢é‡å¼é—­ç¯æ£€æµ‹ï¼‰

+ **è®ºæ–‡**ï¼šShan An, Guangfu Che, Fangru Zhou, Xianglong Liu, Xin Ma, Yu Chen.[ **Fast and Incremental Loop Closure Detection using Proximity Graphs**](https://arxiv.org/pdf/1911.10752.pdf). pp. 378-385, The 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS **2019**)
+ **ä»£ç **ï¼šhttps://github.com/AnshanTJU/FILD

#### 10. object-detection-sptamï¼ˆç›®æ ‡æ£€æµ‹ä¸åŒç›® SLAMï¼‰

+ **è®ºæ–‡**ï¼šPire T, Corti J, Grinblat G. [**Online Object Detection and Localization on Stereo Visual SLAM System**](https://www.researchgate.net/profile/Taihu_Pire/publication/335432416_Online_Object_Detection_and_Localization_on_Stereo_Visual_SLAM_System/links/5d663a14a6fdccf343f93830/Online-Object-Detection-and-Localization-on-Stereo-Visual-SLAM-System.pdf)[J]. Journal of Intelligent & Robotic Systems, **2019**: 1-10.
+ **ä»£ç **ï¼šhttps://github.com/CIFASIS/object-detection-sptam

#### 11. Map Slammerï¼ˆå•ç›®æ·±åº¦ä¼°è®¡ + SLAMï¼‰

+ **è®ºæ–‡**ï¼šTorres-Camara J M, Escalona F, Gomez-Donoso F, et al. [**Map Slammer: Densifying Scattered KSLAM 3D Maps with Estimated Depth**](https://link.springer.com/chapter/10.1007/978-3-030-36150-1_46)[C]//Iberian Robotics conference. Springer, Cham, **2019**: 563-574.
+ **ä»£ç **ï¼šhttps://github.com/jmtc7/mapSlammer

#### 12. NOLBOï¼ˆå˜åˆ†æ¨¡å‹çš„æ¦‚ç‡ SLAMï¼‰

+ **è®ºæ–‡**ï¼šYu H, Lee B. [**Not Only Look But Observe: Variational Observation Model of Scene-Level 3D Multi-Object Understanding for Probabilistic SLAM**](https://arxiv.org/pdf/1907.09760.pdf)[J]. arXiv preprint arXiv:1907.09760, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/bogus2000/NOLBO

#### 13. GCNv2_SLAM ï¼ˆåŸºäºå›¾å·ç§¯ç¥ç»ç½‘ç»œ SLAMï¼‰

+ **è®ºæ–‡**ï¼šTang J, Ericson L, Folkesson J, et al. [**GCNv2: Efficient correspondence prediction for real-time SLAM**](https://ieeexplore.ieee.org/abstract/document/8758836/)[J]. IEEE Robotics and Automation Letters, **2019**, 4(4): 3505-3512.
+ **ä»£ç **ï¼šhttps://github.com/jiexiong2016/GCNv2_SLAM &emsp; [Video](https://www.youtube.com/watch?v=pz-gdnR9tAM)

#### 14. semantic_sumaï¼ˆæ¿€å…‰è¯­ä¹‰å»ºå›¾ï¼‰

+ **è®ºæ–‡**ï¼šChen X, Milioto A, Palazzolo E, et al. [**SuMa++: Efficient LiDAR-based semantic SLAM**](https://ieeexplore.ieee.org/abstract/document/8967704/)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 4530-4537.
+ **ä»£ç **ï¼šhttps://github.com/PRBonn/semantic_suma/ ï¼›[Video](https://www.youtube.com/watch?v=uo3ZuLuFAzk&feature=youtu.be)

#### 15. Neural-SLAMï¼ˆä¸»åŠ¨ç¥ç» SLAMï¼‰

+ **è®ºæ–‡**ï¼šChaplot D S, Gandhi D, Gupta S, et al. [**Learning to explore using active neural slam**](https://arxiv.org/abs/2004.05155)[C]. ICLR **2020**.
+ **ä»£ç **ï¼šhttps://github.com/devendrachaplot/Neural-SLAM

### :smile: 1.3 Multi-Landmarks / Object SLAM

#### 1. PL-SVOï¼ˆç‚¹çº¿ SVOï¼‰

+ **è®ºæ–‡**ï¼šGomez-Ojeda R, Briales J, Gonzalez-Jimenez J. [**PL-SVO: Semi-direct Monocular Visual Odometry by combining points and line segments**](http://mapir.isa.uma.es/rgomez/publications/iros16plsvo.pdf)[C]//Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, **2016**: 4211-4216.
+ **ä»£ç **ï¼šhttps://github.com/rubengooj/pl-svo

#### 2. stvo-plï¼ˆåŒç›®ç‚¹çº¿ VOï¼‰

+ **è®ºæ–‡**ï¼šGomez-Ojeda R, Gonzalez-Jimenez J. [**Robust stereo visual odometry through a probabilistic combination of points and line segments**](https://riuma.uma.es/xmlui/bitstream/handle/10630/11515/icra16rgo.pdf?sequence=1&isAllowed=y)[C]//2016 IEEE International Conference on Robotics and Automation (**ICRA**). IEEE, **2016**: 2521-2526.
+ **ä»£ç **ï¼šhttps://github.com/rubengooj/stvo-pl

#### 3. PL-SLAMï¼ˆç‚¹çº¿ SLAMï¼‰

+ **è®ºæ–‡**ï¼šGomez-Ojeda R, ZuÃ±iga-NoÃ«l D, Moreno F A, et al. [**PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments**](https://arxiv.org/pdf/1705.09479.pdf)[J]. arXiv preprint arXiv:1705.09479, **2017**.
+ **ä»£ç **ï¼šhttps://github.com/rubengooj/pl-slam
+ Gomez-Ojeda R, Moreno F A, ZuÃ±iga-NoÃ«l D, et al. [**PL-SLAM: a stereo SLAM system through the combination of points and line segments**](https://arxiv.org/pdf/1705.09479)[J]. IEEE Transactions on Robotics, **2019**, 35(3): 734-746.

#### 4. PL-VIO

+ **è®ºæ–‡**ï¼šHe Y, Zhao J, Guo Y, et al. [**PL-VIO: Tightly-coupled monocular visualâ€“inertial odometry using point and line features**](https://www.mdpi.com/1424-8220/18/4/1159)[J]. Sensors, **2018**, 18(4): 1159.
+ **ä»£ç **ï¼šhttps://github.com/HeYijia/PL-VIO
+ **VINS + çº¿æ®µ**ï¼šhttps://github.com/Jichao-Peng/VINS-Mono-Optimization

#### 5. lld-slamï¼ˆç”¨äº SLAM çš„å¯å­¦ä¹ å‹çº¿æ®µæè¿°ç¬¦ï¼‰

+ **è®ºæ–‡**ï¼šVakhitov A, Lempitsky V. [**Learnable line segment descriptor for visual SLAM**](https://ieeexplore.ieee.org/iel7/6287639/6514899/08651490.pdf)[J]. IEEE Access, **2019**, 7: 39923-39934.
+ **ä»£ç **ï¼šhttps://github.com/alexandervakhitov/lld-slam ï¼›[**Video**](https://www.youtube.com/watch?v=ntFFiwXIhoA)

> <small>ç‚¹çº¿ç»“åˆçš„å·¥ä½œè¿˜æœ‰å¾ˆå¤šï¼Œå›½å†…çš„æ¯”å¦‚        
> + ä¸Šäº¤é‚¹ä¸¹å¹³è€å¸ˆçš„ Zou D, Wu Y, Pei L, et al. [**StructVIO: visual-inertial odometry with structural regularity of man-made environments**](https://arxiv.org/pdf/1810.06796)[J]. IEEE Transactions on Robotics, **2019**, 35(4): 999-1013.         
> + æµ™å¤§çš„ Zuo X, Xie X, Liu Y, et al. [**Robust visual SLAM with point and line features**](https://arxiv.org/pdf/1711.08654)[C]//2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2017: 1775-1782.</small>

#### 6. PlaneSLAM

+ **è®ºæ–‡**ï¼šWietrzykowski J. [**On the representation of planes for efficient graph-based slam with high-level features**](https://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-7ac7a8f3-9caa-4a34-8a27-8f6c5f43408b)[J]. Journal of Automation Mobile Robotics and Intelligent Systems, **2016**, 10.
+ **ä»£ç **ï¼šhttps://github.com/LRMPUT/PlaneSLAM
+ ä½œè€…å¦å¤–ä¸€é¡¹å¼€æºä»£ç ï¼Œæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„è®ºæ–‡ï¼šhttps://github.com/LRMPUT/PUTSLAM

#### 7. Eigen-Factorsï¼ˆç‰¹å¾å› å­å¹³é¢å¯¹é½ï¼‰

+ **è®ºæ–‡**ï¼šFerrer G. [**Eigen-Factors: Plane Estimation for Multi-Frame and Time-Continuous Point Cloud Alignment**](http://sites.skoltech.ru/app/data/uploads/sites/50/2019/07/ferrer2019planes.pdf)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 1278-1284.
+ **ä»£ç **ï¼šhttps://gitlab.com/gferrer/eigen-factors-iros2019 ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=_1u_c43DFUE&feature=youtu.be)

#### 8. PlaneLoc

+ **è®ºæ–‡**ï¼šWietrzykowski J, SkrzypczyÅ„ski P. [**PlaneLoc: Probabilistic global localization in 3-D using local planar features**]()[J]. Robotics and Autonomous Systems, **2019**, 113: 160-173.
+ **ä»£ç **ï¼šhttps://github.com/LRMPUT/PlaneLoc

#### 9. Pop-up SLAM

+ **è®ºæ–‡**ï¼šYang S, Song Y, Kaess M, et al. [**Pop-up slam: Semantic monocular plane slam for low-texture environments**](https://arxiv.org/pdf/1703.07334.pdf)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 1222-1229.
+ **ä»£ç **ï¼šhttps://github.com/shichaoy/pop_up_slam

#### 10. Object SLAM

+ **è®ºæ–‡**ï¼šMu B, Liu S Y, Paull L, et al. [**Slam with objects using a nonparametric pose graph**](https://arxiv.org/pdf/1704.05959.pdf)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 4602-4609.
+ **ä»£ç **ï¼šhttps://github.com/BeipengMu/objectSLAM ï¼›[Video](https://www.youtube.com/watch?v=YANUWdVLJD4&feature=youtu.be)

#### 11. voxblox-plusplusï¼ˆç‰©ä½“çº§ä½“ç´ å»ºå›¾ï¼‰

+ **è®ºæ–‡**ï¼šGrinvald M, Furrer F, Novkovic T, et al. [**Volumetric instance-aware semantic mapping and 3D object discovery**](https://arxiv.org/pdf/1903.00268.pdf)[J]. IEEE Robotics and Automation Letters, **2019**, 4(3): 3037-3044.
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/voxblox-plusplus

#### 12. Cube SLAM

+ **è®ºæ–‡**ï¼šYang S, Scherer S. [**Cubeslam: Monocular 3-d object slam**](https://arxiv.org/pdf/1806.00557)[J]. IEEE Transactions on Robotics, **2019**, 35(4): 925-938.
+ **ä»£ç **ï¼šhttps://github.com/shichaoy/cube_slam
+ å¯¹ï¼Œè¿™å°±æ˜¯å¸¦æˆ‘å…¥å‘çš„ä¸€é¡¹å·¥ä½œï¼Œ2018 å¹´ 11 æœˆä»½çœ‹åˆ°è¿™ç¯‡è®ºæ–‡ï¼ˆå½“æ—¶æ˜¯é¢„å°ç‰ˆï¼‰ä¹‹åå¼€å§‹å­¦ä¹ ç‰©ä½“çº§ SLAMï¼Œä¸ªäººå¯¹ Cube SLAM çš„ä¸€äº›æ³¨é‡Šå’Œæ€»ç»“ï¼š[é“¾æ¥](https://www.wuxiaolang.cn/slam/)ã€‚
+ ä¹Ÿæœ‰å¾ˆå¤šæœ‰æ„æ€çš„ä½†æ²¡å¼€æºçš„ç‰©ä½“çº§ SLAM
    + Ok K, Liu K, Frey K, et al. [**Robust Object-based SLAM for High-speed Autonomous Navigation**](http://groups.csail.mit.edu/rrg/papers/OkLiu19icra.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 669-675.
    + Li J, Meger D, Dudek G. [**Semantic Mapping for View-Invariant Relocalization**](https://www.cim.mcgill.ca/~mrl/pubs/jimmy/li2019icra.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 7108-7115.
    + Nicholson L, Milford M, SÃ¼nderhauf N. [**Quadricslam: Dual quadrics from object detections as landmarks in object-oriented slam**](https://arxiv.org/pdf/1804.04011)[J]. IEEE Robotics and Automation Letters, **2018**, 4(1): 1-8.

#### 13. VPS-SLAMï¼ˆå¹³é¢è¯­ä¹‰ SLAMï¼‰
+ **è®ºæ–‡**ï¼šBavle H, De La Puente P, How J, et al. [**VPS-SLAM: Visual Planar Semantic SLAM for Aerial Robotic Systems**](https://ieeexplore.ieee.org/iel7/6287639/8948470/09045978.pdf)[J]. IEEE Access, **2020**.
+ **ä»£ç **ï¼šhttps://bitbucket.org/hridaybavle/semantic_slam/src/master/

#### 14. Structure-SLAM ï¼ˆä½çº¹ç†ç¯å¢ƒä¸‹ç‚¹çº¿ SLAMï¼‰
+ **è®ºæ–‡**ï¼šLi Y, Brasch N, Wang Y, et al. [**Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments**](https://ieeexplore.ieee.org/abstract/document/9165014)[J]. IEEE Robotics and Automation Letters, **2020**, 5(4): 6583-6590.
+ **ä»£ç **ï¼šhttps://github.com/yanyan-li/Structure-SLAM-PointLine

### :smile: 1.4 Sensor Fusion

#### 1. msckf_vio

+ **è®ºæ–‡**ï¼šSun K, Mohta K, Pfrommer B, et al. [**Robust stereo visual inertial odometry for fast autonomous flight**](https://arxiv.org/pdf/1712.00036)[J]. IEEE Robotics and Automation Letters, **2018**, 3(2): 965-972.
+ **ä»£ç **ï¼šhttps://github.com/KumarRobotics/msckf_vio ï¼›[Video](https://www.youtube.com/watch?v=jxfJFgzmNSw&t)

#### 2. rovio

+ **è®ºæ–‡**ï¼šBloesch M, Omari S, Hutter M, et al. [**Robust visual inertial odometry using a direct EKF-based approach**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/155340/1/eth-48374-01.pdf)[C]//2015 IEEE/RSJ international conference on intelligent robots and systems (IROS). IEEE, **2015**: 298-304.
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/rovio ï¼›[Video](https://www.youtube.com/watch?v=ZMAISVy-6ao&feature=youtu.be)

#### 3. R-VIO

+ **è®ºæ–‡**ï¼šHuai Z, Huang G. [**Robocentric visual-inertial odometry**](https://arxiv.org/pdf/1805.04031)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 6319-6326.
+ **ä»£ç **ï¼šhttps://github.com/rpng/R-VIO ï¼›[Video](https://www.youtube.com/watch?v=l9IC2ddBEYQ)
+ **VI_ORB_SLAM2**ï¼šhttps://github.com/YoujieXia/VI_ORB_SLAM2

#### 4. okvis

+ **è®ºæ–‡**ï¼šLeutenegger S, Lynen S, Bosse M, et al. [**Keyframe-based visualâ€“inertial odometry using nonlinear optimization**](https://spiral.imperial.ac.uk/bitstream/10044/1/23413/2/ijrr2014_revision_1.pdf)[J]. The International Journal of Robotics Research, **2015**, 34(3): 314-334.
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/okvis

#### 5. VIORB

+ **è®ºæ–‡**ï¼šMur-Artal R, TardÃ³s J D. [Visual-inertial monocular SLAM with map reuse](https://arxiv.org/pdf/1610.05949.pdf)[J]. IEEE Robotics and Automation Letters, **2017**, 2(2): 796-803.
+ **ä»£ç **ï¼šhttps://github.com/jingpang/LearnVIORB ï¼ˆVIORB æœ¬èº«æ˜¯æ²¡æœ‰å¼€æºçš„ï¼Œè¿™æ˜¯ç‹äº¬å¤§ä½¬å¤ç°çš„ä¸€ä¸ªç‰ˆæœ¬ï¼‰

#### 6. VINS-mono

+ **è®ºæ–‡**ï¼šQin T, Li P, Shen S. [**Vins-mono: A robust and versatile monocular visual-inertial state estimator**](https://arxiv.org/pdf/1708.03852)[J]. IEEE Transactions on Robotics, **2018**, 34(4): 1004-1020.
+ **ä»£ç **ï¼šhttps://github.com/HKUST-Aerial-Robotics/VINS-Mono
+ åŒç›®ç‰ˆ **VINS-Fusion**ï¼šhttps://github.com/HKUST-Aerial-Robotics/VINS-Fusion
+ ç§»åŠ¨æ®µ **VINS-mobile**ï¼šhttps://github.com/HKUST-Aerial-Robotics/VINS-Mobile

#### 7. VINS-RGBD

+ **è®ºæ–‡**ï¼šShan Z, Li R, Schwertfeger S. [**RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots**](https://www.mdpi.com/1424-8220/19/10/2251)[J]. Sensors, **2019**, 19(10): 2251.
+ **ä»£ç **ï¼šhttps://github.com/STAR-Center/VINS-RGBD ï¼›[**Video**](https://robotics.shanghaitech.edu.cn/datasets/VINS-RGBD)

#### 8. Open-VINS

+ **è®ºæ–‡**ï¼šGeneva P, Eckenhoff K, Lee W, et al. [**Openvins: A research platform for visual-inertial estimation**](https://pdfs.semanticscholar.org/cb63/60f21255834297e32826bff6366a769b49e9.pdf)[C]//IROS 2019 Workshop on Visual-Inertial Navigation: Challenges and Applications, Macau, China. **IROS 2019**.
+ **ä»£ç **ï¼šhttps://github.com/rpng/open_vins

#### 9. versavisï¼ˆå¤šåŠŸèƒ½çš„è§†æƒ¯ä¼ æ„Ÿå™¨ç³»ç»Ÿï¼‰

+ è®ºæ–‡ï¼šTschopp F, Riner M, Fehr M, et al. [**VersaVISâ€”An Open Versatile Multi-Camera Visual-Inertial Sensor Suite**](https://www.mdpi.com/1424-8220/20/5/1439)[J]. Sensors, **2020**, 20(5): 1439.
+ ä»£ç ï¼šhttps://github.com/ethz-asl/versavis

#### 10. CPIï¼ˆè§†æƒ¯èåˆçš„å°é—­å¼é¢„ç§¯åˆ†ï¼‰

+ **è®ºæ–‡**ï¼šEckenhoff K, Geneva P, Huang G. [**Closed-form preintegration methods for graph-based visualâ€“inertial navigation**](http://sage.cnpereading.com/paragraph/article/10.1177/0278364919835021)[J]. The International Journal of Robotics Research, 2018.
+ **ä»£ç **ï¼šhttps://github.com/rpng/cpi ï¼›[**Video**](https://www.youtube.com/watch?v=yIgQX2SH_pI)

#### 11. TUM Basalt

+ **è®ºæ–‡**ï¼šUsenko V, Demmel N, Schubert D, et al. [Visual-inertial mapping with non-linear factor recovery](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1904.06504.pdf)[J]. IEEE Robotics and Automation Letters, **2019**.
+ **ä»£ç **ï¼š[https://github.com/VladyslavUsenko/basalt-mirror](https://link.zhihu.com/?target=https%3A//github.com/VladyslavUsenko/basalt-mirror) ï¼›[Video](https://link.zhihu.com/?target=https%3A//www.youtube.com/watch%3Fv%3Dr3CJ2JP75Tc)ï¼›[Project Page](https://link.zhihu.com/?target=https%3A//vision.in.tum.de/research/vslam/basalt)

#### 12. Limoï¼ˆæ¿€å…‰å•ç›®è§†è§‰é‡Œç¨‹è®¡ï¼‰

+ **è®ºæ–‡**ï¼šGraeter J, Wilczynski A, Lauer M. [**Limo: Lidar-monocular visual odometry**](https://arxiv.org/pdf/1807.07524)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 7872-7879.
+ **ä»£ç **ï¼šhttps://github.com/johannes-graeter/limo ï¼› [**Video**](https://www.youtube.com/watch?v=wRemjJBjp64&feature=youtu.be)

#### 13. LARVIOï¼ˆå¤šçŠ¶æ€çº¦æŸå¡å°”æ›¼æ»¤æ³¢çš„å•ç›® VIOï¼‰

+ **è®ºæ–‡**ï¼šQiu X, Zhang H, Fu W, et al. [Monocular Visual-Inertial Odometry with an Unbiased Linear System Model and Robust Feature Tracking Front-End](https://www.mdpi.com/1424-8220/19/8/1941)[J]. Sensors, **2019**, 19(8): 1941.
+ **ä»£ç **ï¼šhttps://github.com/PetWorm/LARVIO
+ åŒ—èˆªé‚±ç¬‘æ™¨åšå£«çš„ä¸€é¡¹å·¥ä½œ

#### 14. vig-initï¼ˆå‚ç›´è¾¹ç¼˜åŠ é€Ÿè§†æƒ¯åˆå§‹åŒ–ï¼‰

+ **è®ºæ–‡**ï¼šLi J, Bao H, Zhang G. [**Rapid and Robust Monocular Visual-Inertial Initialization with Gravity Estimation via Vertical Edges**](http://www.cad.zju.edu.cn/home/gfzhang/projects/iros2019-vi-initialization.pdf)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 6230-6236.
+ **ä»£ç **ï¼šhttps://github.com/zju3dv/vig-init
+ æµ™å¤§ç« å›½å³°è€å¸ˆç»„çš„ä¸€é¡¹å·¥ä½œ

#### 15. vilibï¼ˆVIO å‰ç«¯åº“ï¼‰

+ **è®ºæ–‡**ï¼šNagy B, Foehn P, Scaramuzza D. [**Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO**](https://arxiv.org/pdf/2003.13493)[J]. arXiv preprint arXiv:2003.13493, **2020**.
+ **ä»£ç **ï¼šhttps://github.com/uzh-rpg/vilib

#### 16. Kimera-VIO

+ **è®ºæ–‡**ï¼šA. Rosinol, M. Abate, Y. Chang, L. Carlone, [**Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping**](https://arxiv.org/abs/1910.02490). IEEE Intl. Conf. on Robotics and Automation (ICRA), 2020.
+ **ä»£ç **ï¼šhttps://github.com/MIT-SPARK/Kimera-VIO

#### 17. maplabï¼ˆè§†æƒ¯å»ºå›¾æ¡†æ¶ï¼‰
+ **è®ºæ–‡**ï¼šSchneider T, Dymczyk M, Fehr M, et al. [**maplab: An open framework for research in visual-inertial mapping and localization**](https://arxiv.org/pdf/1711.10250)[J]. IEEE Robotics and Automation Letters, **2018**, 3(3): 1418-1425. 
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/maplab
+ å¤šä¼šè¯å»ºå›¾ï¼Œåœ°å›¾åˆå¹¶ï¼Œè§†è§‰æƒ¯æ€§æ‰¹å¤„ç†ä¼˜åŒ–å’Œé—­ç¯

### :smile: 1.5 Dynamic SLAM

#### 1. DynamicSemanticMappingï¼ˆåŠ¨æ€è¯­ä¹‰å»ºå›¾ï¼‰

+ **è®ºæ–‡**ï¼šKochanov D, OÅ¡ep A, StÃ¼ckler J, et al. [**Scene flow propagation for semantic mapping and object discovery in dynamic street scenes**](http://web-info8.informatik.rwth-aachen.de/media/papers/paper_compressed.pdf)[C]//Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, **2016**: 1785-1792.
+ **ä»£ç **ï¼šhttps://github.com/ganlumomo/DynamicSemanticMapping ï¼›[wiki](https://github.com/ganlumomo/DynamicSemanticMapping/wiki)

#### 2. DS-SLAMï¼ˆåŠ¨æ€è¯­ä¹‰ SLAMï¼‰

+ **è®ºæ–‡**ï¼šYu C, Liu Z, Liu X J, et al. [**DS-SLAM: A semantic visual SLAM towards dynamic environments**](https://arxiv.org/pdf/1809.08379)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 1168-1174.
+ **ä»£ç **ï¼šhttps://github.com/ivipsourcecode/DS-SLAM

#### 3. Co-Fusionï¼ˆå®æ—¶åˆ†å‰²ä¸è·Ÿè¸ªå¤šç‰©ä½“ï¼‰

+ **è®ºæ–‡**ï¼šRÃ¼nz M, Agapito L. [**Co-fusion: Real-time segmentation, tracking and fusion of multiple objects**](https://ieeexplore.ieee.org/abstract/document/7989518)[C]//2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017: 4471-4478.
+ **ä»£ç **ï¼šhttps://github.com/martinruenz/co-fusion ï¼› [Video](http://visual.cs.ucl.ac.uk/pubs/cofusion/index.html)

#### 4. DynamicFusion

+ **è®ºæ–‡**ï¼šNewcombe R A, Fox D, Seitz S M. [**Dynamicfusion: Reconstruction and tracking of non-rigid scenes in real-time**](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Newcombe_DynamicFusion_Reconstruction_and_2015_CVPR_paper.pdf)[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. **2015**: 343-352.
+ **ä»£ç **ï¼šhttps://github.com/mihaibujanca/dynamicfusion

#### 5. ReFusionï¼ˆåŠ¨æ€åœºæ™¯åˆ©ç”¨æ®‹å·®ä¸‰ç»´é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šPalazzolo E, Behley J, Lottes P, et al. [**ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals**](https://arxiv.org/pdf/1905.02082.pdf)[J]. arXiv preprint arXiv:1905.02082, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/PRBonn/refusion ï¼›[**Video**](https://www.youtube.com/watch?v=1P9ZfIS5-p4&feature=youtu.be)

#### 6. DynSLAMï¼ˆå®¤å¤–å¤§è§„æ¨¡ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šBÃ¢rsan I A, Liu P, Pollefeys M, et al. [**Robust dense mapping for large-scale dynamic environments**](https://arxiv.org/pdf/1905.02781.pdf?utm_term)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 7510-7517.
+ **ä»£ç **ï¼šhttps://github.com/AndreiBarsan/DynSLAM
+ **ä½œè€…åšå£«å­¦ä½è®ºæ–‡**ï¼šBarsan I A. [**Simultaneous localization and mapping in dynamic scenes**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/202829/1/Barsan_Ioan.pdf)[D]. ETH Zurich, Department of Computer Science, **2017**.

#### 7. VDO-SLAMï¼ˆåŠ¨æ€ç‰©ä½“æ„ŸçŸ¥çš„ SLAMï¼‰

+ **è®ºæ–‡**ï¼šZhang J, Henein M, Mahony R, et al. [**VDO-SLAM: A Visual Dynamic Object-aware SLAM System**](https://arxiv.org/abs/2005.11052)[J]. arXiv preprint arXiv:2005.11052, **2020**.ï¼ˆIJRR Under Reviewï¼‰
  + ç›¸å…³è®ºæ–‡
    + IROS 2020 [Robust Ego and Object 6-DoF Motion Estimation and Tracking](https://arxiv.org/abs/2007.13993)
    + ICRA 2020 [Dynamic SLAM: The Need For Speed](https://arxiv.org/abs/2002.08584)
+ **ä»£ç **ï¼šhttps://github.com/halajun/VDO_SLAM | [video](https://drive.google.com/file/d/1PbL4KiJ3sUhxyJSQPZmRP6mgi9dIC0iu/view)

### :smile: 1.6 Mapping

#### 1. InfiniTAMï¼ˆè·¨å¹³å° CPU å®æ—¶é‡å»ºï¼‰

+ è®ºæ–‡ï¼šPrisacariu V A, KÃ¤hler O, Golodetz S, et al. [**Infinitam v3: A framework for large-scale 3d reconstruction with loop closure**](https://arxiv.org/pdf/1708.00783)[J]. arXiv preprint arXiv:1708.00783, **2017**.
+ ä»£ç ï¼šhttps://github.com/victorprad/InfiniTAM ï¼›[project page](http://www.robots.ox.ac.uk/~victor/infinitam/)

#### 2. BundleFusion

+ **è®ºæ–‡**ï¼šDai A, NieÃŸner M, ZollhÃ¶fer M, et al. [**Bundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface reintegration**](https://arxiv.org/pdf/1604.01093.pdf)[J]. ACM Transactions on Graphics (TOG), **2017**, 36(4): 76a.
+ **ä»£ç **ï¼šhttps://github.com/niessner/BundleFusion ï¼›[å·¥ç¨‹åœ°å€](http://graphics.stanford.edu/projects/bundlefusion/)
  
#### 3. KinectFusion

+ **è®ºæ–‡**ï¼šNewcombe R A, Izadi S, Hilliges O, et al. [**KinectFusion: Real-time dense surface mapping and tracking**](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/kinectfusion-uist-comp.pdf)[C]//2011 10th IEEE International Symposium on Mixed and Augmented Reality. IEEE, **2011**: 127-136.
+ **ä»£ç **ï¼šhttps://github.com/chrdiller/KinectFusionApp

#### 4. ElasticFusion

+ **è®ºæ–‡**ï¼šWhelan T, Salas-Moreno R F, Glocker B, et al. [**ElasticFusion: Real-time dense SLAM and light source estimation**](https://spiral.imperial.ac.uk/bitstream/10044/1/39502/4/Whelan16ijrr.pdf)[J]. The International Journal of Robotics Research, **2016**, 35(14): 1697-1716.
+ **ä»£ç **ï¼šhttps://github.com/mp3guy/ElasticFusion

#### 5. Kintinuous

+ ElasticFusion åŒä¸€ä¸ªå›¢é˜Ÿçš„å·¥ä½œï¼Œå¸å›½ç†å·¥ Stefan Leutenegger [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=SmGQ48gAAAAJ&hl=zh-CN&oi=sra)
+ **è®ºæ–‡**ï¼šWhelan T, Kaess M, Johannsson H, et al. [**Real-time large-scale dense RGB-D SLAM with volumetric fusion**](https://dspace.mit.edu/bitstream/handle/1721.1/97583/Leonard_Real-time.pdf%3Bjsessionid%3D8C351776D7D5E5C614AF641625837212?sequence%3D1)[J]. The International Journal of Robotics Research, **2015**, 34(4-5): 598-626.
+ **ä»£ç **ï¼šhttps://github.com/mp3guy/Kintinuous

#### 6. ElasticReconstruction

+ **è®ºæ–‡**ï¼šChoi S, Zhou Q Y, Koltun V. [**Robust reconstruction of indoor scenes**](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Choi_Robust_Reconstruction_of_2015_CVPR_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2015**: 5556-5565.
+ **ä»£ç **ï¼šhttps://github.com/qianyizh/ElasticReconstruction ï¼›[ä½œè€…ä¸»é¡µ](http://qianyi.info/publication.html)

#### 7. FlashFusion

+ **è®ºæ–‡**ï¼šHan L, Fang L. [**FlashFusion: Real-time Globally Consistent Dense 3D Reconstruction using CPU Computing**](http://www.roboticsproceedings.org/rss14/p06.pdf)[C]. RSS, 2018.
+ **ä»£ç **ï¼ˆä¸€ç›´æ²¡æ”¾å‡ºæ¥ï¼‰ï¼šhttps://github.com/lhanaf/FlashFusion ï¼› [Project Page](http://www.luvision.net/FlashFusion/?tdsourcetag=s_pctim_aiomsg)

#### 8. RTAB-Mapï¼ˆæ¿€å…‰è§†è§‰ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šLabbÃ© M, Michaud F. [**RTABâ€Map as an openâ€source lidar and visual simultaneous localization and mapping library for largeâ€scale and longâ€term online operation**](https://pdfs.semanticscholar.org/3957/7f85f3b1a16f496a2160d1a71894d12c1acc.pdf)[J]. Journal of Field Robotics, **2019**, 36(2): 416-446.
+ **ä»£ç **ï¼šhttps://github.com/introlab/rtabmap ï¼›[Video](https://www.youtube.com/user/matlabbe) ï¼›[project page](http://introlab.github.io/rtabmap/)

#### 9. RobustPCLReconstructionï¼ˆæˆ·å¤–ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šLan Z, Yew Z J, Lee G H. [**Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Lan_Robust_Point_Cloud_Based_Reconstruction_of_Large-Scale_Outdoor_Scenes_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2019**: 9690-9698.
+ **ä»£ç **ï¼šhttps://github.com/ziquan111/RobustPCLReconstruction ï¼›[Video](https://www.youtube.com/watch?v=ZZQT_REkItU)

#### 10. plane-opt-rgbdï¼ˆå®¤å†…å¹³é¢é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šWang C, Guo X. [**Efficient Plane-Based Optimization of Geometry and Texture for Indoor RGB-D Reconstruction**](http://openaccess.thecvf.com/content_CVPRW_2019/papers/SUMO/Wang_Efficient_Plane-Based_Optimization_of_Geometry_and_Texture_for_Indoor_RGB-D_CVPRW_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. **2019**: 49-53.
+ **ä»£ç **ï¼šhttps://github.com/chaowang15/plane-opt-rgbd

#### 11. DenseSurfelMappingï¼ˆç¨ å¯†è¡¨é¢é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šWang K, Gao F, Shen S. [**Real-time scalable dense surfel mapping**](https://arxiv.org/pdf/1909.04250.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 6919-6925.
+ **ä»£ç **ï¼šhttps://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping

#### 12. surfelmeshingï¼ˆç½‘æ ¼é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šSchÃ¶ps T, Sattler T, Pollefeys M. [**Surfelmeshing: Online surfel-based mesh reconstruction**](https://arxiv.org/pdf/1810.00729.pdf)[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/puzzlepaint/surfelmeshing

#### 13. DPPTAMï¼ˆå•ç›®ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šConcha Belenguer A, Civera Sancho J. [**DPPTAM: Dense piecewise planar tracking and mapping from a monocular sequence**](https://zaguan.unizar.es/record/36752/files/texto_completo.pdf)[C]//Proc. IEEE/RSJ Int. Conf. Intell. Rob. Syst. **2015** (ART-2015-92153).
+ **ä»£ç **ï¼šhttps://github.com/alejocb/dpptam
+ **ç›¸å…³ç ”ç©¶**ï¼šåŸºäºè¶…åƒç´ çš„å•ç›® SLAMï¼š[**Using Superpixels in Monocular SLAM**](http://webdiis.unizar.es/~jcivera/papers/concha_civera_icra14.pdf) ICRA 2014 ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=GIaG3CsAAAAJ&hl=zh-CN&oi=sra)

#### 14. VI-MEANï¼ˆå•ç›®è§†æƒ¯ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šYang Z, Gao F, Shen S. [**Real-time monocular dense mapping on aerial robots using visual-inertial fusion**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7989529)[C]//2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2017: 4552-4559.
+ **ä»£ç **ï¼šhttps://github.com/dvorak0/VI-MEAN ï¼›[Video](https://www.youtube.com/watch?v=M4BMks6bQbc)

#### 15. REMODEï¼ˆå•ç›®æ¦‚ç‡ç¨ å¯†é‡å»ºï¼‰

+ **è®ºæ–‡**ï¼šPizzoli M, Forster C, Scaramuzza D. [**REMODE: Probabilistic, monocular dense reconstruction in real time**](https://files.ifi.uzh.ch/rpg/website/rpg.ifi.uzh.ch/html/docs/ICRA14_Pizzoli.pdf)[C]//2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014: 2609-2616.
+ **åŸå§‹å¼€æºä»£ç **ï¼šhttps://github.com/uzh-rpg/rpg_open_remode
+ **ä¸ ORB-SLAM2 ç»“åˆç‰ˆæœ¬**ï¼šhttps://github.com/ayushgaud/ORB_SLAM2  https://github.com/ayushgaud/ORB_SLAM2

#### 16. DeepFactorsï¼ˆå®æ—¶çš„æ¦‚ç‡å•ç›®ç¨ å¯† SLAMï¼‰

+ å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤
+ **è®ºæ–‡**ï¼šCzarnowski J, Laidlow T, Clark R, et al. [**DeepFactors: Real-Time Probabilistic Dense Monocular SLAM**](https://arxiv.org/pdf/2001.05049.pdf)[J]. arXiv preprint arXiv:2001.05049, **2020**.
+ **ä»£ç **ï¼šhttps://github.com/jczarnowski/DeepFactors ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ å…¶ä»–è®ºæ–‡ï¼šBloesch M, Czarnowski J, Clark R, et al. [**CodeSLAMâ€”learning a compact, optimisable representation for dense visual SLAM**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. **2018**: 2560-2568.

#### 17. probabilistic_mappingï¼ˆå•ç›®æ¦‚ç‡ç¨ å¯†é‡å»ºï¼‰

+ æ¸¯ç§‘æ²ˆé‚µåŠ¼è€å¸ˆå›¢é˜Ÿ
+ **è®ºæ–‡**ï¼šLing Y, Wang K, Shen S. [**Probabilistic dense reconstruction from a moving camera**](https://arxiv.org/pdf/1903.10673.pdf)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 6364-6371.
+ **ä»£ç **ï¼šhttps://github.com/ygling2008/probabilistic_mapping
+ å¦å¤–ä¸€ç¯‡ç¨ å¯†é‡å»ºæ–‡ç« çš„ä»£ç ä¸€ç›´æ²¡æ”¾å‡ºæ¥ [Github](https://github.com/ygling2008/dense_mapping) ï¼šLing Y, Shen S. [**Realâ€time dense mapping for online processing and navigation**](https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.21868)[J]. Journal of Field Robotics, **2019**, 36(5): 1004-1036.

#### 18. ORB-SLAM2 å•ç›®åŠç¨ å¯†å»ºå›¾

+ **è®ºæ–‡**ï¼šMur-Artal R, TardÃ³s J D. [Probabilistic Semi-Dense Mapping from Highly Accurate Feature-Based Monocular SLAM](https://www.researchgate.net/profile/Raul_Mur-Artal/publication/282807894_Probabilistic_Semi-Dense_Mapping_from_Highly_Accurate_Feature-Based_Monocular_SLAM/links/561cd04308ae6d17308ce267.pdf)[C]//Robotics: Science and Systems. **2015**, 2015.
+ **ä»£ç **ï¼ˆæœ¬èº«æ²¡æœ‰å¼€æºï¼Œè´ºåšå¤ç°çš„ä¸€ä¸ªç‰ˆæœ¬ï¼‰ï¼šhttps://github.com/HeYijia/ORB_SLAM2
+ åŠ ä¸Šçº¿æ®µä¹‹åçš„åŠç¨ å¯†å»ºå›¾
    + **è®ºæ–‡**ï¼šHe S, Qin X, Zhang Z, et al. [**Incremental 3d line segment extraction from semi-dense slam**](https://arxiv.org/pdf/1708.03275)[C]//2018 24th International Conference on Pattern Recognition (ICPR). IEEE, **2018**: 1658-1663.
    + **ä»£ç **ï¼šhttps://github.com/shidahe/semidense-lines
    + ä½œè€…åœ¨æ­¤åŸºç¡€ä¸Šç”¨äºæŒ‡å¯¼è¿œç¨‹æŠ“å–æ“ä½œçš„ä¸€é¡¹å·¥ä½œï¼šhttps://github.com/atlas-jj/ORB-SLAM-free-space-carving

#### 19. Voxgraphï¼ˆSDF ä½“ç´ å»ºå›¾ï¼‰

+ **è®ºæ–‡**ï¼šReijgwart V, Millane A, Oleynikova H, et al. [**Voxgraph: Globally Consistent, Volumetric Mapping Using Signed Distance Function Submaps**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/385682/1/Voxgraph-ETHpreprintversion.pdf)[J]. IEEE Robotics and Automation Letters, **2019**, 5(1): 227-234.
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/voxgraph

#### 20. SegMapï¼ˆä¸‰ç»´åˆ†å‰²å»ºå›¾ï¼‰
+ **è®ºæ–‡**ï¼šDubÃ© R, Cramariuc A, Dugas D, et al. [**SegMap: 3d segment mapping using data-driven descriptors**](https://arxiv.org/pdf/1804.09557)[J]. arXiv preprint arXiv:1804.09557, **2018**.
+ **ä»£ç **ï¼šhttps://github.com/ethz-asl/segmap

### :smile: 1.7 Optimization

#### 1. åç«¯ä¼˜åŒ–åº“

+ **GTSAM**ï¼šhttps://github.com/borglab/gtsam ï¼›[å®˜ç½‘](https://gtsam.org/)
+ **g2o**ï¼šhttps://github.com/RainerKuemmerle/g2o
+ **ceres**ï¼šhttp://ceres-solver.org/

#### 2. ICE-BA

+ **è®ºæ–‡**ï¼šLiu H, Chen M, Zhang G, et al. [**Ice-ba: Incremental, consistent and efficient bundle adjustment for visual-inertial slam**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2018**: 1974-1982.
+ **ä»£ç **ï¼šhttps://github.com/baidu/ICE-BA

#### 3. minisamï¼ˆå› å­å›¾æœ€å°äºŒä¹˜ä¼˜åŒ–æ¡†æ¶ï¼‰

+ **è®ºæ–‡**ï¼šDong J, Lv Z. [**miniSAM: A Flexible Factor Graph Non-linear Least Squares Optimization Framework**](https://arxiv.org/pdf/1909.00903.pdf)[J]. arXiv preprint arXiv:1909.00903, **2019**.
+ **ä»£ç **ï¼šhttps://github.com/dongjing3309/minisam ï¼› [æ–‡æ¡£](https://minisam.readthedocs.io/)

#### 4. SA-SHAGOï¼ˆå‡ ä½•åŸºå…ƒå›¾ä¼˜åŒ–ï¼‰

+ **è®ºæ–‡**ï¼šAloise I, Della Corte B, Nardi F, et al. [**Systematic Handling of Heterogeneous Geometric Primitives in Graph-SLAM Optimization**](http://rss2019.informatik.uni-freiburg.de/papers/0285_FI.pdf)[J]. IEEE Robotics and Automation Letters, **2019**, 4(3): 2738-2745.
+ **ä»£ç **ï¼šhttps://srrg.gitlab.io/sashago-website/index.html#

#### 5. MH-iSAM2ï¼ˆSLAM ä¼˜åŒ–å™¨ï¼‰

+ **è®ºæ–‡**ï¼šHsiao M, Kaess M. [**MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree**](http://www.cs.cmu.edu/~kaess/pub/Hsiao19icra.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 1274-1280.
+ **ä»£ç **ï¼šhttps://bitbucket.org/rpl_cmu/mh-isam2_lib/src/master/

#### 6. MOLAï¼ˆç”¨äºå®šä½å’Œå»ºå›¾çš„æ¨¡å—åŒ–ä¼˜åŒ–æ¡†æ¶ï¼‰

+ **è®ºæ–‡**ï¼šBlanco-Claraco J L. [**A Modular Optimization Framework for Localization and Mapping**](http://roboticsproceedings.org/rss15/p43.pdf)[J]. Proc. of Robotics: Science and Systems (RSS), FreiburgimBreisgau, Germany, **2019**, 2.
+ **ä»£ç **ï¼šhttps://github.com/MOLAorg/mola ï¼›[**Video**](https://www.youtube.com/watch?v=Bb92aMBJR44) ï¼›[ä½¿ç”¨æ–‡æ¡£](https://docs.mola-slam.org/latest/)

---
## 2. ä¼˜ç§€ä½œè€…ä¸å®éªŒå®¤
> è¿™ä¸€éƒ¨åˆ†æ•´ç†ä¹‹åå‘å¸ƒåœ¨çŸ¥ä¹ï¼ˆ2020 å¹´ 4 æœˆ 19 æ—¥ï¼‰ï¼šhttps://zhuanlan.zhihu.com/p/130530891

#### 1. ç¾å›½å¡è€åŸºæ¢…é™‡å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€

+ **ç ”ç©¶æ–¹å‘**ï¼šæœºå™¨äººæ„ŸçŸ¥ã€ç»“æ„ï¼ŒæœåŠ¡å‹ã€è¿è¾“ã€åˆ¶é€ ä¸šã€ç°åœºæœºå™¨
+ **ç ”ç©¶æ‰€ä¸»é¡µ**ï¼šhttps://www.ri.cmu.edu/
+ ä¸‹å± **Field Robotic Center** ä¸»é¡µï¼šhttps://frc.ri.cmu.edu/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttps://www.ri.cmu.edu/pubs/
+ ğŸ‘¦ **Michael Kaess**ï¼š[ä¸ªäººä¸»é¡µ](https://www.ri.cmu.edu/ri-faculty/michael-kaess/) ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=27eupmsAAAAJ&hl=zh-CN&oi=ao)
+ ğŸ‘¦ **Sebastian Scherer**ï¼š[ä¸ªäººä¸»é¡µ](https://www.ri.cmu.edu/ri-faculty/sebastian-scherer/) ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=gxoPfIYAAAAJ)
+ ğŸ“œ Kaess M, Ranganathan A, Dellaert F. [**iSAM: Incremental smoothing and mapping**](https://smartech.gatech.edu/bitstream/handle/1853/26572/kaess_michael_200812_phd.pdf?sequence=1&isAllowed=y)[J]. IEEE Transactions on Robotics, **2008**, 24(6): 1365-1378.
+ ğŸ“œ Hsiao M, Westman E, Zhang G, et al. [**Keyframe-based dense planar SLAM**](http://www.cs.cmu.edu/~kaess/pub/Hsiao17icra.pdf)[C]//2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2017**: 5110-5117.
+ ğŸ“œ Kaess M. [**Simultaneous localization and mapping with infinite planes**](https://www.cs.cmu.edu/~kaess/pub/Kaess15icra.pdf)[C]//2015 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2015**: 4605-4611.

#### 2. ç¾å›½åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡è¯­å¢ƒæœºå™¨äººç ”ç©¶æ‰€

+ **ç ”ç©¶æ–¹å‘**ï¼šå¤šæ¨¡æ€ç¯å¢ƒç†è§£ï¼Œè¯­ä¹‰å¯¼èˆªï¼Œè‡ªä¸»ä¿¡æ¯è·å–
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://existentialrobotics.org/index.html
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://existentialrobotics.org/pages/publications.html
+ ğŸ‘¦ **Nikolay Atanasov**ï¼š[ä¸ªäººä¸»é¡µ](https://natanaso.github.io/)&emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=RTkSatQAAAAJ&hl=zh-CN&oi=sra)
    + æœºå™¨äººçŠ¶æ€ä¼°è®¡ä¸æ„ŸçŸ¥è¯¾ç¨‹ pptï¼šhttps://natanaso.github.io/ece276a2019/schedule.html
+ ğŸ“œ **è¯­ä¹‰ SLAM ç»å…¸è®ºæ–‡**ï¼šBowman S L, Atanasov N, Daniilidis K, et al. [**Probabilistic data association for semantic slam**](http://erl.ucsd.edu/ref/Bowman_SemanticSLAM_ICRA17.pdf)[C]//2017 IEEE international conference on robotics and automation (ICRA). IEEE, **2017**: 1722-1729.
+ ğŸ“œ **å®ä¾‹ç½‘æ ¼æ¨¡å‹å®šä½ä¸å»ºå›¾**ï¼šFeng Q, Meng Y, Shan M, et al. [**Localization and Mapping using Instance-specific Mesh Models**](http://erl.ucsd.edu/ref/Feng_DeformableMeshModel_IROS19.pdf)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 4985-4991.
+ ğŸ“œ **åŸºäºäº‹ä»¶ç›¸æœºçš„ VIO**ï¼šZihao Zhu A, Atanasov N, Daniilidis K. [**Event-based visual inertial odometry**](http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhu_Event-Based_Visual_Inertial_CVPR_2017_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2017**: 5391-5399.

#### 3. ç¾å›½ç‰¹æ‹‰åå¤§å­¦æœºå™¨äººæ„ŸçŸ¥ä¸å¯¼èˆªç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMã€VINSã€è¯­ä¹‰å®šä½ä¸å»ºå›¾ç­‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://sites.udel.edu/robot/
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://sites.udel.edu/robot/publications/
+ **Github åœ°å€**ï¼šhttps://github.com/rpng?page=2
+ ğŸ“œ Geneva P, Eckenhoff K, Lee W, et al. [**Openvins: A research platform for visual-inertial estimation**](https://pdfs.semanticscholar.org/cb63/60f21255834297e32826bff6366a769b49e9.pdf)[C]//IROS 2019 Workshop on Visual-Inertial Navigation: Challenges and Applications, Macau, China. **IROS 2019**.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/rpng/open_vins ï¼‰
+ ğŸ“œ Huai Z, Huang G. [**Robocentric visual-inertial odometry**](https://arxiv.org/pdf/1805.04031)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 6319-6326.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/rpng/R-VIO ï¼‰
+ ğŸ“œ Zuo X, Geneva P, Yang Y, et al. [**Visual-Inertial Localization With Prior LiDAR Map Constraints**](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1911.05787)[J]. IEEE Robotics and Automation Letters, **2019**, 4(4): 3394-3401.
+ ğŸ“œ Zuo X, Ye W, Yang Y, et al. [**Multimodal localization: Stereo over LiDAR map**](https://link.zhihu.com/?target=https%3A//onlinelibrary.wiley.com/doi/abs/10.1002/rob.21936)[J]. Journal of Field Robotics, **2020** ï¼ˆ å·¦æ˜Ÿæ˜Ÿåšå£«[è°·æ­Œå­¦æœ¯](https://link.zhihu.com/?target=https%3A//scholar.google.com/citations%3Fuser%3DCePv8agAAAAJ%26hl%3Dzh-CN%26oi%3Dao)ï¼‰
+ ğŸ‘¦ [é»„å›½æƒæ•™æˆä¸»é¡µ](http://udel.edu/~ghuang/index.html)

#### 4. ç¾å›½éº»çœç†å·¥å­¦é™¢èˆªç©ºèˆªå¤©å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šä½å§¿ä¼°è®¡ä¸å¯¼èˆªï¼Œè·¯å¾„è§„åˆ’ï¼Œæ§åˆ¶ä¸å†³ç­–ï¼Œæœºå™¨å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ 
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://acl.mit.edu/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttp://acl.mit.edu/publications ï¼ˆå®éªŒå®¤çš„**å­¦ä½è®ºæ–‡**ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼‰
+ ğŸ‘¦ **Jonathan P. How** æ•™æˆï¼š[ä¸ªäººä¸»é¡µ](http://www.mit.edu/people/jhow/) &emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=gX7rSCcAAAAJ&hl=en)
+ ğŸ‘¦ **Kasra Khosoussi**ï¼ˆSLAM å›¾ä¼˜åŒ–ï¼‰ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=SRCCuo0AAAAJ&hl=zh-CN&oi=sra)
+ ğŸ“œ **ç‰©ä½“çº§ SLAM**ï¼šMu B, Liu S Y, Paull L, et al. [**Slam with objects using a nonparametric pose graph**](https://arxiv.org/pdf/1704.05959.pdf?source=post_page---------------------------)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 4602-4609.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/BeipengMu/objectSLAMï¼‰
+ ğŸ“œ **ç‰©ä½“çº§ SLAM å¯¼èˆª**ï¼šOk K, Liu K, Frey K, et al. [**Robust Object-based SLAM for High-speed Autonomous Navigation**](http://groups.csail.mit.edu/rrg/papers/OkLiu19icra.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 669-675.
+ ğŸ“œ **SLAM çš„å›¾ä¼˜åŒ–**ï¼šKhosoussi, K., Giamou, M., Sukhatme, G., Huang, S., Dissanayake, G., and How, J. P., [**Reliable Graphs for SLAM**](https://journals.sagepub.com/doi/full/10.1177/0278364918823086) [C]//International Journal of Robotics Research (IJRR), 2019.

#### 5. ç¾å›½éº»çœç†å·¥å­¦é™¢ SPARK å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šç§»åŠ¨æœºå™¨äººç¯å¢ƒæ„ŸçŸ¥
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://web.mit.edu/sparklab/
+ ğŸ‘¦ **Luca Carlone** æ•™æˆï¼š[ä¸ªäººä¸»é¡µ](https://lucacarlone.mit.edu/) &emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=U4kKRdMAAAAJ&hl=zh-CN&oi=sra)
+ ğŸ“œ **SLAM ç»å…¸ç»¼è¿°**ï¼šCadena C, Carlone L, Carrillo H, et al. [**Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age**](https://arxiv.org/pdf/1606.05830)[J]. IEEE Transactions on robotics, **2016**, 32(6): 1309-1332.
+ ğŸ“œ **VIO æµå½¢é¢„ç§¯åˆ†**ï¼šForster C, Carlone L, Dellaert F, et al. [**On-Manifold Preintegration for Real-Time Visual--Inertial Odometry**](https://arxiv.org/pdf/1512.02363)[J]. IEEE Transactions on Robotics, **2016**, 33(1): 1-21.
+ ğŸ“œ **å¼€æºè¯­ä¹‰ SLAM**ï¼šRosinol A, Abate M, Chang Y, et al. [**Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping**](https://arxiv.org/pdf/1910.02490)[J]. arXiv preprint arXiv:1910.02490, **2019**.ï¼ˆä»£ç ï¼šhttps://github.com/MIT-SPARK/Kimera ï¼‰

#### 6. ç¾å›½éº»çœç†å·¥å­¦é™¢æµ·æ´‹æœºå™¨äººç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šæ°´ä¸‹æˆ–é™†åœ°ç§»åŠ¨æœºå™¨äººå¯¼èˆªä¸å»ºå›¾
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://marinerobotics.mit.edu/ ï¼ˆéš¶å±äº MIT [è®¡ç®—æœºç§‘å­¦ä¸äººå·¥æ™ºèƒ½å®éªŒå®¤](https://www.csail.mit.edu/)ï¼‰
+ ğŸ‘¦ **John Leonard æ•™æˆ**ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=WPe7vWwAAAAJ&hl=zh-CN&authuser=1&oi=ao)
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://marinerobotics.mit.edu/biblio
+ ğŸ“œ **é¢å‘ç‰©ä½“çš„ SLAM**ï¼šFinman R, Paull L, Leonard J J. [**Toward object-based place recognition in dense rgb-d maps**](http://marinerobotics.mit.edu/sites/default/files/icra2015.pdf)[C]//ICRA Workshop Visual Place Recognition in Changing Environments, Seattle, WA. **2015**.
+ ğŸ“œ **æ‹“å±• KinectFusion**ï¼šWhelan T, Kaess M, Fallon M, et al. [**Kintinuous: Spatially extended kinectfusion**]()[J]. **2012**.
+ ğŸ“œ **è¯­ä¹‰ SLAM æ¦‚ç‡æ•°æ®å…³è”**ï¼šDoherty K, Fourie D, Leonard J. [**Multimodal semantic slam with probabilistic data association**](https://marinerobotics.mit.edu/sites/default/files/doherty_icra2019_revised.pdf)[C]//2019 international conference on robotics and automation (ICRA). IEEE, **2019**: 2419-2425.

#### 7. ç¾å›½æ˜å°¼è‹è¾¾å¤§å­¦å¤šå…ƒè‡ªä¸»æœºå™¨äººç³»ç»Ÿå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šè§†è§‰ã€æ¿€å…‰ã€æƒ¯æ€§å¯¼èˆªç³»ç»Ÿï¼Œç§»åŠ¨è®¾å¤‡å¤§è§„æ¨¡ä¸‰ç»´å»ºæ¨¡ä¸å®šä½
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://mars.cs.umn.edu/index.php
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttp://mars.cs.umn.edu/publications.php
+ ğŸ‘¦ **Stergios I. Roumeliotis**ï¼š[ä¸ªäººä¸»é¡µ](https://www-users.cs.umn.edu/~stergios/) ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=c5HeXxsAAAAJ&hl=zh-CN&oi=ao)
+ ğŸ“œ **ç§»åŠ¨è®¾å¤‡ VIO**ï¼šWu K, Ahmed A, Georgiou G A, et al. [**A Square Root Inverse Filter for Efficient Vision-aided Inertial Navigation on Mobile Devices**](http://roboticsproceedings.org/rss11/p08.pdf)[C]//Robotics: Science and Systems. **2015**, 2.ï¼ˆ**é¡¹ç›®ä¸»é¡µ**ï¼šhttp://mars.cs.umn.edu/research/sriswf.php ï¼‰
+ ğŸ“œ **ç§»åŠ¨è®¾å¤‡å¤§è§„æ¨¡ä¸‰ç»´åŠç¨ å¯†å»ºå›¾**ï¼šGuo C X, Sartipi K, DuToit R C, et al. [**Resource-aware large-scale cooperative three-dimensional mapping using multiple mobile devices**](https://pdfs.semanticscholar.org/e0fd/6d963307a0d5d6dfb6f05ad21845dd4f40c8.pdf)[J]. IEEE Transactions on Robotics, **2018**, 34(5): 1349-1369. ï¼ˆ**é¡¹ç›®ä¸»é¡µ**ï¼šhttp://mars.cs.umn.edu/research/semi_dense_mapping.php ï¼‰
+ ğŸ“œ **VIO ç›¸å…³ç ”ç©¶**ï¼šhttp://mars.cs.umn.edu/research/vins_overview.php

#### 8. ç¾å›½å®¾å¤•æ³•å°¼äºšå¤§å­¦ Vijay Kumar å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šè‡ªä¸»å¾®å‹æ— äººæœº
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.kumarrobotics.org/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttps://www.kumarrobotics.org/publications/
+ **ç ”ç©¶æˆæœè§†é¢‘**ï¼šhttps://www.youtube.com/user/KumarLabPenn/videos
+ ğŸ“œ **æ— äººæœºåŠç¨ å¯† VIO**ï¼šLiu W, Loianno G, Mohta K, et al. [**Semi-Dense Visual-Inertial Odometry and Mapping for Quadrotors with SWAP Constraints**](https://www.cis.upenn.edu/~kostas/mypub.dir/wenxin18icra.pdf)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 1-6.
+ ğŸ“œ **è¯­ä¹‰æ•°æ®å…³è”**ï¼šLiu X, Chen S W, Liu C, et al. [**Monocular Camera Based Fruit Counting and Mapping with Semantic Data Association**](https://arxiv.org/pdf/1811.01417)[J]. IEEE Robotics and Automation Letters, **2019**, 4(3): 2296-2303.

#### 9. Srikumar Ramalingamï¼ˆç¾å›½çŠ¹ä»–å¤§å­¦è®¡ç®—æœºå­¦é™¢ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šä¸‰ç»´é‡æ„ã€è¯­ä¹‰åˆ†å‰²ã€è§†è§‰ SLAMã€å›¾åƒå®šä½ã€æ·±åº¦ç¥ç»ç½‘ç»œ
+ ğŸ‘¦ **Srikumar Ramalingam**ï¼š[ä¸ªäººä¸»é¡µ](https://www.cs.utah.edu/~srikumar/) &emsp; [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=6m1ptOgAAAAJ&hl=en/)
+ ğŸ“œ **ç‚¹é¢ SLAM**ï¼šTaguchi Y, Jian Y D, Ramalingam S, et al. [**Point-plane SLAM for hand-held 3D sensors**](https://merl.com/publications/docs/TR2013-031.pdf)[C]//2013 IEEE international conference on robotics and automation. IEEE, **2013**: 5182-5189.
+ ğŸ“œ **ç‚¹çº¿å®šä½**ï¼šRamalingam S, Bouaziz S, Sturm P. [**Pose estimation using both points and lines for geo-localization**](https://hal.inria.fr/inria-00590279/document)[C]//2011 IEEE International Conference on Robotics and Automation. IEEE, **2011**: 4716-4723.ï¼ˆ[è§†é¢‘](https://www.youtube.com/watch?v=wc7hK0zEkCw&feature=emb_logo)ï¼‰
+ ğŸ“œ **2D 3D å®šä½**ï¼šAtaer-Cansizoglu E, Taguchi Y, Ramalingam S. [**Pinpoint SLAM: A hybrid of 2D and 3D simultaneous localization and mapping for RGB-D sensors**](http://yuichitaguchi.com/pub/16ICRA_PinpointSLAM.pdf)[C]//2016 IEEE international conference on robotics and automation (ICRA). IEEE, **2016**: 1300-1307.ï¼ˆ[è§†é¢‘](https://www.youtube.com/watch?v=iZ1psxcMvrQ&feature=emb_logo)ï¼‰

#### 10. Frank Dellaertï¼ˆç¾å›½ä½æ²»äºšç†å·¥å­¦é™¢æœºå™¨äººä¸æ™ºèƒ½æœºå™¨ç ”ç©¶ä¸­å¿ƒï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMï¼Œå›¾åƒæ—¶ç©ºé‡æ„
+ ğŸ‘¦ [ä¸ªäººä¸»é¡µ](https://www.cc.gatech.edu/~dellaert/FrankDellaert/Frank_Dellaert/Frank_Dellaert.html)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=ZxXBaswAAAAJ&hl=en)
+ ğŸ“œ **å› å­å›¾**ï¼šDellaert F. [**Factor graphs and GTSAM: A hands-on introduction**](https://smartech.gatech.edu/handle/1853/45226)[R]. Georgia Institute of Technology, **2012**. ï¼ˆGTSAM ä»£ç ï¼šhttp://borg.cc.gatech.edu/ ï¼‰
+ ğŸ“œ **å¤šæœºå™¨äººåˆ†å¸ƒå¼ SLAM**ï¼šCunningham A, Wurm K M, Burgard W, et al. [**Fully distributed scalable smoothing and mapping with robust multi-robot data association**](https://smartech.gatech.edu/bitstream/handle/1853/44686/Cunningham12icra.pdf?sequence=1&isAllowed=y)[C]//2012 IEEE International Conference on Robotics and Automation. IEEE, **2012**: 1093-1100.
+ ğŸ“œ Choudhary S, Trevor A J B, Christensen H I, et al. [**SLAM with object discovery, modeling and mapping**](https://smartech.gatech.edu/bitstream/handle/1853/53723/Choudhary14iros.pdf)[C]//2014 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, **2014**: 1018-1025.

#### 11. èµµè½¶ç’ï¼ˆç¾å›½ä½æ²»äºšç†å·¥å­¦é™¢æ™ºèƒ½è§†è§‰ä¸è‡ªåŠ¨åŒ–å®éªŒå®¤ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šè§†è§‰ SLAMã€ä¸‰ç»´é‡å»ºã€å¤šç›®æ ‡è·Ÿè¸ª
+ ğŸ‘¦ [ä¸ªäººä¸»é¡µ](https://sites.google.com/site/zhaoyipu/home?authuser=0) &emsp; [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=HiM_WcYAAAAJ&hl=zh-CN&authuser=1&oi=ao)
+ ğŸ“œ Zhao Y, Smith J S, Karumanchi S H, et al. [**Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy**](https://arxiv.org/pdf/2003.01317)[J]. arXiv preprint arXiv:2003.01317, **2020**.
+ ğŸ“œ Zhao Y, Vela P A. [**Good feature selection for least squares pose optimization in VO/VSLAM**](https://arxiv.org/pdf/1905.07807)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 1183-1189.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/ivalab/FullResults_GoodFeature ï¼‰
+ ğŸ“œ Zhao Y, Vela P A. [**Good line cutting: Towards accurate pose tracking of line-assisted VO/VSLAM**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Yipu_Zhao_Good_Line_Cutting_ECCV_2018_paper.pdf)[C]//Proceedings of the European Conference on Computer Vision (ECCV). **2018**: 516-531. ï¼ˆ**ä»£ç **ï¼šhttps://github.com/ivalab/GF_PL_SLAM ï¼‰

#### 12. åŠ æ‹¿å¤§è’™ç‰¹åˆ©å°”å¤§å­¦ æœºå™¨äººä¸åµŒå…¥å¼ AI å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMï¼Œä¸ç¡®å®šæ€§å»ºæ¨¡
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://montrealrobotics.ca/
+ ğŸ‘¦ **Liam Paull** æ•™æˆï¼š[ä¸ªäººä¸»é¡µ](https://liampaull.ca/index.html)&emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=H9xADK0AAAAJ&hl=zh-CN&oi=ao)
+ ğŸ“œ Mu B, Liu S Y, Paull L, et al. [**Slam with objects using a nonparametric pose graph**](https://arxiv.org/pdf/1704.05959.pdf?source=post_page---------------------------)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 4602-4609.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/BeipengMu/objectSLAMï¼‰
+ ğŸ“œ Murthy Jatavallabhula K, Iyer G, Paull L. [**gradSLAM: Dense SLAM meets Automatic Differentiation**](http://adsabs.harvard.edu/abs/2019arXiv191010672M)[J]. arXiv preprint arXiv:1910.10672, **2019**.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/montrealrobotics/gradSLAM ï¼‰

#### 13. åŠ æ‹¿å¤§èˆå¸ƒé²å…‹å¤§å­¦æ™ºèƒ½ã€äº¤äº’ã€ç»¼åˆã€è·¨å­¦ç§‘æœºå™¨äººå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šç§»åŠ¨æœºå™¨äººè½¯ç¡¬ä»¶è®¾è®¡
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://introlab.3it.usherbrooke.ca/
+ ğŸ“œ **æ¿€å…‰è§†è§‰ç¨ å¯†é‡å»º**ï¼šLabbÃ© M, Michaud F. [**RTABâ€Map as an openâ€source lidar and visual simultaneous localization and mapping library for largeâ€scale and longâ€term online operation**](https://pdfs.semanticscholar.org/3957/7f85f3b1a16f496a2160d1a71894d12c1acc.pdf)[J]. Journal of Field Robotics, **2019**, 36(2): 416-446.
    + ä»£ç ï¼šhttps://github.com/introlab/rtabmap
    + é¡¹ç›®ä¸»é¡µï¼šhttp://introlab.github.io/rtabmap/
    
#### 14. ç‘å£«è‹é»ä¸–å¤§å­¦æœºå™¨äººä¸æ„ŸçŸ¥è¯¾é¢˜ç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šç§»åŠ¨æœºå™¨äººã€æ— äººæœºç¯å¢ƒæ„ŸçŸ¥ä¸å¯¼èˆªï¼Œ**VISLAM**ï¼Œ**äº‹ä»¶ç›¸æœº**
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://rpg.ifi.uzh.ch/index.html
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttp://rpg.ifi.uzh.ch/publications.html
+ **Github ä»£ç å…¬å¼€åœ°å€**ï¼šhttps://github.com/uzh-rpg
+ ğŸ“œ Forster C, Pizzoli M, Scaramuzza D. [**SVO: Fast semi-direct monocular visual odometry**](https://www.zora.uzh.ch/id/eprint/125453/1/ICRA14_Forster.pdf)[C]//2014 IEEE international conference on robotics and automation (ICRA). IEEE, **2014**: 15-22.
+ ğŸ“œ VO/VIO è½¨è¿¹è¯„ä¼°å·¥å…· **rpg_trajectory_evaluation**ï¼šhttps://github.com/uzh-rpg/rpg_trajectory_evaluation
+ ğŸ“œ äº‹ä»¶ç›¸æœºé¡¹ç›®ä¸»é¡µï¼šhttp://rpg.ifi.uzh.ch/research_dvs.html
+ ğŸ‘¦ **äººç‰©**ï¼š[Davide Scaramuzza](http://rpg.ifi.uzh.ch/people_scaramuzza.html) &emsp;[å¼ å­æ½®](https://www.ifi.uzh.ch/en/rpg/people/zichao.html)

#### 15. ç‘å£«è‹é»ä¸–è”é‚¦ç†å·¥è®¡ç®—æœºè§†è§‰ä¸å‡ ä½•å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šå®šä½ã€ä¸‰ç»´é‡å»ºã€è¯­ä¹‰åˆ†å‰²ã€æœºå™¨äººè§†è§‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://www.cvg.ethz.ch/index.php
+ **å‘è¡¨è®ºæ–‡**ï¼šhttp://www.cvg.ethz.ch/publications/
+ ğŸ“œ **è§†è§‰è¯­ä¹‰é‡Œç¨‹è®¡**ï¼šLianos K N, Schonberger J L, Pollefeys M, et al. [**Vso: Visual semantic odometry**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Konstantinos-Nektarios_Lianos_VSO_Visual_Semantic_ECCV_2018_paper.pdf)[C]//Proceedings of the European conference on computer vision (ECCV). **2018**: 234-250.
+ ğŸ“œ **è§†è§‰è¯­ä¹‰å®šä½**ï¼šCVPR 2018 [**Semantic visual localization**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Schonberger_Semantic_Visual_Localization_CVPR_2018_paper.pdf)
    + **ä½œè€…åšå£«å­¦ä½è®ºæ–‡**ï¼š2018 [**Robust Methods for Accurate and Efficient 3D Modeling from Unstructured Imagery**](https://www.research-collection.ethz.ch/handle/20.500.11850/295763)
+ ğŸ“œ **å¤§è§„æ¨¡æˆ·å¤–å»ºå›¾**ï¼šBÃ¢rsan I A, Liu P, Pollefeys M, et al. [**Robust dense mapping for large-scale dynamic environments**](https://arxiv.org/pdf/1905.02781.pdf?utm_term)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 7510-7517.
    + **ä»£ç **ï¼šhttps://github.com/AndreiBarsan/DynSLAM 
    + **ä½œè€…åšå£«å­¦ä½è®ºæ–‡**ï¼šBarsan I A. [**Simultaneous localization and mapping in dynamic scenes**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/202829/1/Barsan_Ioan.pdf)[D]. ETH Zurich, Department of Computer Science, **2017**.
+ ğŸ‘¦ **Marc Pollefeys**ï¼š[ä¸ªäººä¸»é¡µ](http://people.inf.ethz.ch/pomarc/index.html)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=YYH0BjEAAAAJ&hl=zh-CN&oi=ao)
+ ğŸ‘¦ **Johannes L. SchÃ¶nberger**ï¼š[ä¸ªäººä¸»é¡µ](https://demuc.de/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=MlcMCd0AAAAJ)

#### 16. è‹±å›½å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šæœºå™¨äººè§†è§‰åœºæ™¯ä¸ç‰©ä½“ç†è§£ã€æœºå™¨äººæ“çºµ
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.imperial.ac.uk/dyson-robotics-lab/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttps://www.imperial.ac.uk/dyson-robotics-lab/publications/
+ **ä»£è¡¨æ€§å·¥ä½œ**ï¼š**MonoSLAMã€CodeSLAMã€ElasticFusionã€KinectFusion**
    + ğŸ“œ **ElasticFusion**ï¼šWhelan T, Leutenegger S, Salas-Moreno R, et al. [**ElasticFusion: Dense SLAM without a pose graph**](https://spiral.imperial.ac.uk/bitstream/10044/1/23438/2/whelan2015rss.pdf)[C]. Robotics: Science and Systems, **2015**.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/mp3guy/ElasticFusion ï¼‰
    + ğŸ“œ **Semanticfusion**ï¼šMcCormac J, Handa A, Davison A, et al. [**Semanticfusion: Dense 3d semantic mapping with convolutional neural networks**](https://arxiv.org/pdf/1609.05130)[C]//2017 IEEE International Conference on Robotics and automation (ICRA). IEEE, **2017**: 4628-4635.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/seaun163/semanticfusion ï¼‰
    + ğŸ“œ **Code-SLAM**ï¼šBloesch M, Czarnowski J, Clark R, et al. [**CodeSLAMâ€”learning a compact, optimisable representation for dense visual SLAM**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. **2018**: 2560-2568.
+ ğŸ‘¦ **Andrew Davison**ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=A0ae1agAAAAJ&hl=zh-CN&oi=ao)

#### 17. è‹±å›½ç‰›æ´¥å¤§å­¦ä¿¡æ¯å·¥ç¨‹å­¦

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMã€ç›®æ ‡è·Ÿè¸ªã€è¿åŠ¨ç»“æ„ã€åœºæ™¯å¢å¼ºã€ç§»åŠ¨æœºå™¨äººè¿åŠ¨è§„åˆ’ã€å¯¼èˆªä¸å»ºå›¾ç­‰ç­‰ç­‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://www.robots.ox.ac.uk/
    + ä¸»åŠ¨è§†è§‰å®éªŒå®¤ï¼šhttp://www.robots.ox.ac.uk/ActiveVision/
    + ç‰›æ´¥æœºå™¨äººå­¦é™¢ï¼šhttps://ori.ox.ac.uk/
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼š
    + ä¸»åŠ¨è§†è§‰å®éªŒå®¤ï¼šhttp://www.robots.ox.ac.uk/ActiveVision/Publications/index.html
    + æœºå™¨äººå­¦é™¢ï¼šhttps://ori.ox.ac.uk/publications/papers/
+ **ä»£è¡¨æ€§å·¥ä½œ**ï¼š
    + ğŸ“œ Klein G, Murray D. [**PTAM: Parallel tracking and mapping for small AR workspaces**](https://dl.acm.org/ft_gateway.cfm?id=1514363&type=pdf)[C]//2007 6th IEEE and ACM international symposium on mixed and augmented reality. IEEE, **2007**: 225-234.
    + ğŸ“œ RobotCar æ•°æ®é›†ï¼šhttps://robotcar-dataset.robots.ox.ac.uk/
+ ğŸ‘¦ **äººç‰©**ï¼ˆè°·æ­Œå­¦æœ¯ï¼‰ï¼š[David Murray](https://scholar.google.com.hk/citations?hl=zh-CN&user=O5QreiwAAAAJ) &emsp; [Maurice Fallon](https://ori.ox.ac.uk/ori-people/maurice-fallon/)
+ éƒ¨åˆ†åšå£«å­¦ä½è®ºæ–‡å¯ä»¥åœ¨è¿™é‡Œæœåˆ°ï¼šhttps://ora.ox.ac.uk/

#### 18. å¾·å›½æ…•å°¼é»‘å·¥ä¸šå¤§å­¦è®¡ç®—æœºè§†è§‰ç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šä¸‰ç»´é‡å»ºã€æœºå™¨äººè§†è§‰ã€æ·±åº¦å­¦ä¹ ã€**è§†è§‰ SLAM** ç­‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://vision.in.tum.de/research/vslam
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://vision.in.tum.de/publications
+ **ä»£è¡¨ä½œ**ï¼šDSOã€LDSOã€LSD_SLAMã€DVO_SLAM
    + ğŸ“œ **DSO**ï¼šEngel J, Koltun V, Cremers D. [**Direct sparse odometry**](https://ieeexplore.ieee.org/iel7/34/4359286/07898369.pdf)[J]. IEEE transactions on pattern analysis and machine intelligence, **2017**, 40(3): 611-625.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/JakobEngel/dso ï¼‰
    + ğŸ“œ **LSD-SLAM**ï¼š Engel J, SchÃ¶ps T, Cremers D. [LSD-SLAM: Large-scale direct monocular SLAM](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.646.7193&rep=rep1&type=pdf)[C]//European conference on computer vision. Springer, Cham, **2014**: 834-849.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/tum-vision/lsd_slam ï¼‰2. 
+ **Github åœ°å€**ï¼šhttps://github.com/tum-vision
+ ğŸ‘¦ **Daniel Cremers** æ•™æˆï¼š[ä¸ªäººä¸»é¡µ](https://vision.in.tum.de/members/cremers) [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=cXQciMEAAAAJ)
+ ğŸ‘¦ **Jakob Engel**ï¼ˆLSD-SLAMï¼ŒDSO ä½œè€…ï¼‰ï¼š[ä¸ªäººä¸»é¡µ](https://jakobengel.github.io/) &emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.de/citations?user=ndOMZXMAAAAJ)

#### 19. å¾·å›½é©¬å…‹æ–¯æ™®æœ—å…‹æ™ºèƒ½ç³»ç»Ÿç ”ç©¶æ‰€åµŒå…¥å¼è§†è§‰ç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šæ™ºèƒ½ä½“è‡ªä¸»ç¯å¢ƒç†è§£ã€å¯¼èˆªä¸ç‰©ä½“æ“çºµ
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://ev.is.tuebingen.mpg.de/
+ ğŸ‘¦ è´Ÿè´£äºº **JÃ¶rg StÃ¼ckler**ï¼ˆå‰ TUM æ•™æˆï¼‰ï¼š[ä¸ªäººä¸»é¡µ](https://ev.is.tuebingen.mpg.de/person/jstueckler) &emsp; [è°·æ­Œå­¦æœ¯](https://scholar.google.de/citations?user=xrOzfucAAAAJ&hl=de)
+ ğŸ“œ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://ev.is.tuebingen.mpg.de/publications
+ Kasyanov A, Engelmann F, StÃ¼ckler J, et al. [**Keyframe-based visual-inertial online SLAM with relocalization**](https://arxiv.org/pdf/1702.02175)[C]//2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2017**: 6662-6669.
+ ğŸ“œ Strecke M, Stuckler J. [**EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Strecke_EM-Fusion_Dynamic_Object-Level_SLAM_With_Probabilistic_Data_Association_ICCV_2019_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2019**: 5865-5874.
+ ğŸ“œ Usenko, V., Demmel, N., Schubert, D., StÃ¼ckler, J., Cremers, D. [**Visual-Inertial Mapping with Non-Linear Factor Recovery**](https://arxiv.org/pdf/1904.06504) IEEE Robotics and Automation Letters (RA-L), 5, **2020**

#### 20. å¾·å›½å¼—è±å ¡å¤§å­¦æ™ºèƒ½è‡ªä¸»ç³»ç»Ÿå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šå¤šæœºå™¨äººå¯¼èˆªä¸åä½œï¼Œç¯å¢ƒå»ºæ¨¡ä¸çŠ¶æ€ä¼°è®¡
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://ais.informatik.uni-freiburg.de/index_en.php
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttp://ais.informatik.uni-freiburg.de/publications/index_en.php ï¼ˆå­¦ä½è®ºæ–‡ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°ï¼‰
+ ğŸ‘¦ **Wolfram Burgard**ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=zj6FavAAAAAJ&hl=zh-CN&oi=ao)
+ **å¼€æ”¾æ•°æ®é›†**ï¼šhttp://aisdatasets.informatik.uni-freiburg.de/
+ ğŸ“œ **RGB-D SLAM**ï¼šEndres F, Hess J, Sturm J, et al. [**3-D mapping with an RGB-D camera**](http://perpustakaan.unitomo.ac.id/repository/3-D%20Mapping%20With%20an%20RGB-D%20Camera06594910.pdf)[J]. IEEE transactions on robotics, **2013**, 30(1): 177-187.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/felixendres/rgbdslam_v2 ï¼‰
+ ğŸ“œ **è·¨å­£èŠ‚çš„ SLAM**ï¼šNaseer T, Ruhnke M, Stachniss C, et al. [**Robust visual SLAM across seasons**](http://ais.informatik.uni-freiburg.de/publications/papers/naseer15iros.pdf)[C]//2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2015**: 2529-2535.
+ ğŸ“œ **åšå£«å­¦ä½è®ºæ–‡**ï¼š[Robust Graph-Based Localization and Mapping](http://ais.informatik.uni-freiburg.de/publications/papers/agarwal15phd.pdf) 2015
+ ğŸ“œ **åšå£«å­¦ä½è®ºæ–‡**ï¼š[Discovering and Leveraging Deep Multimodal Structure for Reliable Robot Perception and Localization](http://ais.informatik.uni-freiburg.de/publications/papers/valada19phd.pdf) 2019
+ ğŸ“œ **åšå£«å­¦ä½è®ºæ–‡**ï¼š[Robot Localization and Mapping in Dynamic Environments](https://freidok.uni-freiburg.de/fedora/objects/freidok:149938/datastreams/FILE1/content) 2019

#### 21. è¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦æœºå™¨äººã€æ„ŸçŸ¥ä¸å®æ—¶ç»„ SLAM å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šè§†è§‰ SLAMã€ç‰©ä½“ SLAMã€éåˆšæ€§ SLAMã€æœºå™¨äººã€å¢å¼ºç°å®
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://robots.unizar.es/slamlab/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttp://robots.unizar.es/slamlab/?extra=3 ï¼ˆè®ºæ–‡å¥½åƒæ²¡æ›´æ–°ï¼Œå¯ä»¥è®¿é—®ä¸‹é¢å®éªŒå®¤å¤§ä½¬çš„è°·æ­Œå­¦æœ¯æŸ¥çœ‹æœ€æ–°è®ºæ–‡ï¼‰
+ ğŸ‘¦ **J. M. M. Montiel**ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=D99JRxwAAAAJ&hl=zh-CN&oi=sra)
+ ğŸ“œ Mur-Artal R, TardÃ³s J D. [**Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras**](https://github.com/raulmur/ORB_SLAM2)[J]. IEEE Transactions on Robotics, **2017**, 33(5): 1255-1262.
+ GÃ¡lvez-LÃ³pez D, Salas M, TardÃ³s J D, et al. [**Real-time monocular object slam**](https://arxiv.org/pdf/1504.02398.pdf)[J]. Robotics and Autonomous Systems, **2016**, 75: 435-449.
+ ğŸ“œ Strasdat H, Montiel J M M, Davison A J. [**Real-time monocular SLAM: Why filter?**](http://www.hauke.strasdat.net/files/strasdat2010icra.pdf)[C]//2010 IEEE International Conference on Robotics and Automation. IEEE, **2010**: 2657-2664.
+ ğŸ“œ Zubizarreta J, Aguinaga I, Montiel J M M. [**Direct sparse mapping**](https://arxiv.org/pdf/1904.06577)[J]. arXiv preprint arXiv:1904.06577, **2019**.
    + Elvira R, TardÃ³s J D, Montiel J M M. [**ORBSLAM-Atlas: a robust and accurate multi-map system**](https://arxiv.org/pdf/1908.11585)[J]. arXiv preprint arXiv:1908.11585, **2019**.
    
#### 22. è¥¿ç­ç‰™é©¬æ‹‰åŠ å¤§å­¦æœºå™¨æ„ŸçŸ¥ä¸æ™ºèƒ½æœºå™¨äººè¯¾é¢˜ç»„

+ **ç ”ç©¶æ–¹å‘**ï¼šè‡ªä¸»æœºå™¨äººã€äººå·¥å—…è§‰ã€è®¡ç®—æœºè§†è§‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://mapir.uma.es/mapirwebsite/index.php/topics-2.html
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttp://mapir.isa.uma.es/mapirwebsite/index.php/publications-menu-home.html
+ ğŸ“œ Gomez-Ojeda R, Moreno F A, ZuÃ±iga-NoÃ«l D, et al. [**PL-SLAM: a stereo SLAM system through the combination of points and line segments**](https://arxiv.org/pdf/1705.09479)[J]. IEEE Transactions on Robotics, **2019**, 35(3): 734-746.ï¼ˆä»£ç ï¼šhttps://github.com/rubengooj/pl-slam ï¼‰
+ ğŸ‘¦ [**Francisco-Angel Moreno**](http://mapir.isa.uma.es/mapirwebsite/index.php/people/199-francisco-moreno-due%C3%B1as)
+ ğŸ‘¦ [**Ruben Gomez-Ojeda**](https://scholar.google.com/citations?user=7jne0V4AAAAJ&hl=zh-CN&oi=sra) ç‚¹çº¿ SLAM
    + ğŸ“œ Gomez-Ojeda R, Briales J, Gonzalez-Jimenez J. [**PL-SVO: Semi-direct Monocular Visual Odometry by combining points and line segments**](http://mapir.isa.uma.es/rgomez/publications/iros16plsvo.pdf)[C]//Intelligent Robots and Systems (IROS), 2016 IEEE/RSJ International Conference on. IEEE, **2016**: 4211-4216.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/rubengooj/pl-svo ï¼‰
    + ğŸ“œ Gomez-Ojeda R, Gonzalez-Jimenez J. [**Robust stereo visual odometry through a probabilistic combination of points and line segments**](https://riuma.uma.es/xmlui/bitstream/handle/10630/11515/icra16rgo.pdf?sequence=1&isAllowed=y)[C]//2016 IEEE International Conference on Robotics and Automation (**ICRA**). IEEE, **2016**: 2521-2526.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/rubengooj/stvo-pl ï¼‰
    + ğŸ“œ Gomez-Ojeda R, ZuÃ±iga-NoÃ«l D, Moreno F A, et al. [**PL-SLAM: a Stereo SLAM System through the Combination of Points and Line Segments**](https://arxiv.org/pdf/1705.09479.pdf)[J]. arXiv preprint arXiv:1705.09479, **2017**.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/rubengooj/pl-slam ï¼‰

#### 23. Alejo Conchaï¼ˆOculus VRï¼Œè¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMï¼Œå•ç›®ç¨ å¯†é‡å»ºï¼Œä¼ æ„Ÿå™¨èåˆ
+ ğŸ‘¦ **ä¸ªäººä¸»é¡µ**ï¼šhttps://sites.google.com/view/alejoconcha/ &emsp; [**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=GIaG3CsAAAAJ&hl=zh-CN&oi=sra)
+ Githubï¼šhttps://github.com/alejocb
+ ğŸ“œ **IROS 2015** å•ç›®å¹³é¢é‡å»ºï¼š[**DPPTAM: Dense piecewise planar tracking and mapping from a monocular sequence**](https://zaguan.unizar.es/record/36752/files/texto_completo.pdf) ï¼ˆä»£ç ï¼šhttps://github.com/alejocb/dpptam ï¼‰
+ ğŸ“œ **IROS 2017** å¼€æº RGB-D SLAMï¼š[**RGBDTAM: A Cost-Effective and Accurate RGB-D Tracking and Mapping System**](http://webdiis.unizar.es/~jcivera/papers/concha_etal_icra16.pdf)ï¼ˆä»£ç ï¼šhttps://github.com/alejocb/rgbdtam ï¼‰
+ ğŸ“œ **ICRA 2016**ï¼š[**Visual-inertial direct SLAM**](http://webdiis.unizar.es/~jcivera/papers/concha_etal_icra16.pdf)
+ ğŸ“œ **ICRA 2014**ï¼š[**Using Superpixels in Monocular SLAM**](https://www.researchgate.net/profile/Alejo_Concha/publication/281559193_Using_superpixels_in_monocular_SLAM/links/55edffcb08aedecb68fc6ac2/Using-superpixels-in-monocular-SLAM.pdf)
+ **RSS 2014**ï¼š[**Manhattan and Piecewise-Planar Constraints for Dense Monocular Mapping**](http://roboticsproceedings.org/rss10/p16.pdf)

#### 24. å¥¥åœ°åˆ©æ ¼æ‹‰èŒ¨æŠ€æœ¯å¤§å­¦è®¡ç®—æœºå›¾å½¢å­¦ä¸è§†è§‰ç ”ç©¶æ‰€

+ **ç ”ç©¶æ–¹å‘**ï¼šAR/VRï¼Œæœºå™¨äººè§†è§‰ï¼Œæœºå™¨å­¦ä¹ ï¼Œç›®æ ‡è¯†åˆ«ä¸ä¸‰ç»´é‡å»º
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.tugraz.at/institutes/icg/home/
+ ğŸ‘¦ **Friedrich Fraundorfer** æ•™æˆï¼š[å›¢é˜Ÿä¸»é¡µ](https://www.tugraz.at/institutes/icg/research/team-fraundorfer/) &emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=M0boL5kAAAAJ&hl=zh-CN&oi=sra)
    + ğŸ“œ [**Visual Odometry: Part I The First 30 Years and Fundamentals**](http://www.eng.auburn.edu/~troppel/courses/7970%202015A%20AdvMobRob%20sp15/literature/vis%20odom%20tutor%20part1%20.pdf)
    + ğŸ“œ [**Visual Odometry: Part II: Matching, Robustness, Optimization, and Applications**](https://www.zora.uzh.ch/id/eprint/71030/1/Fraundorfer_Scaramuzza_Visual_odometry.pdf)
    + ğŸ“œ Schenk F, Fraundorfer F. [**RESLAM: A real-time robust edge-based SLAM system**](https://ieeexplore.ieee.org/abstract/document/8794462/)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 154-160.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/fabianschenk/RESLAM ï¼‰
+ ğŸ‘¦ **Dieter Schmalstieg** æ•™æˆï¼š[å›¢é˜Ÿä¸»é¡µ](https://www.tugraz.at/institutes/icg/research/team-schmalstieg/) &emsp;[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=xXu8K6IAAAAJ&hl=zh-CN&oi=ao)
    + ğŸ“œ æ•™ç§‘ä¹¦ï¼š[Augmented Reality: Principles and Practice](augmentedrealitybook.org)
    + ğŸ“œ Arth C, Pirchheim C, Ventura J, et al. [**Instant outdoor localization and slam initialization from 2.5 d maps**](https://ieeexplore.ieee.org/abstract/document/7164332/)[J]. IEEE transactions on visualization and computer graphics, **2015**, 21(11): 1309-1318.
    + ğŸ“œ Hachiuma R, Pirchheim C, Schmalstieg D, et al. [**DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic Objects in Real-time SLAM**](https://arxiv.org/pdf/1907.09127)[J]. arXiv preprint arXiv:1907.09127, **2019**.
    
#### 25. æ³¢å…°æ³¢å…¹å—å·¥ä¸šå¤§å­¦ç§»åŠ¨æœºå™¨äººå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMï¼Œæœºå™¨äººè¿åŠ¨è§„åˆ’ï¼Œæ§åˆ¶
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://lrm.put.poznan.pl/
+ **Github ä¸»é¡µ**ï¼šhttps://github.com/LRMPUT
+ ğŸ“œ Wietrzykowski J. [**On the representation of planes for efficient graph-based slam with high-level features**](https://yadda.icm.edu.pl/baztech/element/bwmeta1.element.baztech-7ac7a8f3-9caa-4a34-8a27-8f6c5f43408b)[J]. Journal of Automation Mobile Robotics and Intelligent Systems, **2016**, 10.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/LRMPUT/PlaneSLAM ï¼‰
+ ğŸ“œ Wietrzykowski J, SkrzypczyÅ„ski P. [**PlaneLoc: Probabilistic global localization in 3-D using local planar features**](https://www.sciencedirect.com/science/article/pii/S0921889018303701)[J]. Robotics and Autonomous Systems, **2019**.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/LRMPUT/PlaneLoc ï¼‰
+ ğŸ“œ **PUTSLAM**ï¼šhttp://lrm.put.poznan.pl/putslam/

#### 26. Alexander Vakhitovï¼ˆä¸‰æ˜Ÿè«æ–¯ç§‘ AI ä¸­å¿ƒï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šSLAMï¼Œå‡ ä½•è§†è§‰
+ ğŸ‘¦ **ä¸ªäººä¸»é¡µ**ï¼šhttps://alexandervakhitov.github.io/ ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.ru/citations?user=g_2iut0AAAAJ&hl=ru%22)
+ ğŸ“œ **ç‚¹çº¿ SLAM**ï¼šICRA 2017 [**PL-SLAM: Real-time monocular visual SLAM with points and lines**](https://upcommons.upc.edu/bitstream/handle/2117/110259/1836-PL-SLAM--Real-Time-Monocular-Visual-SLAM-with-Points-and-Lines.pdf)
+ ğŸ“œ **ç‚¹çº¿å®šä½**ï¼šPumarola A, Vakhitov A, Agudo A, et al. [**Relative localization for aerial manipulation with PL-SLAM**](https://upcommons.upc.edu/bitstream/handle/2117/182388/2205-Relative-localization-for-aerial-manipulation-with-PL-SLAM.pdf)[M]//Aerial Robotic Manipulation. Springer, Cham, **2019**: 239-248.
+ ğŸ“œ **å­¦ä¹ å‹çº¿æ®µ**ï¼šIEEE Access 2019 [**Learnable line segment descriptor for visual SLAM**](https://ieeexplore.ieee.org/iel7/6287639/6514899/08651490.pdf)ï¼ˆ**ä»£ç **ï¼šhttps://github.com/alexandervakhitov/lld-slam ï¼‰

#### 27. æ¾³å¤§åˆ©äºšæ˜†å£«å…°ç§‘æŠ€å¤§å­¦æœºå™¨äººæŠ€æœ¯ä¸­å¿ƒ

+ **ç ”ç©¶æ–¹å‘**ï¼šè„‘å¯å‘å¼æœºå™¨äººï¼Œé‡‡çŸ¿æœºå™¨äººï¼Œæœºå™¨äººè§†è§‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.qut.edu.au/research/centre-for-robotics
+ **å¼€æºä»£ç **ï¼šhttps://research.qut.edu.au/qcr/open-source-code/
+ ğŸ‘¦ **Niko SÃ¼nderhauf**ï¼š[ä¸ªäººä¸»é¡µ](https://nikosuenderhauf.github.io/) ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=WnKjfFEAAAAJ&hl=zh-CN&oi=ao)
    + ğŸ“œ RA-L 2018 **äºŒæ¬¡æ›²é¢ SLAM**ï¼š[**QuadricSLAM: Dual quadrics from object detections as landmarks in object-oriented SLAM**](https://ieeexplore.ieee.org/abstract/document/8440105/)
    + ğŸ“œ Nicholson L, Milford M, Sunderhauf N. [**QuadricSLAM: Dual quadrics as SLAM landmarks**](http://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w9/Nicholson_QuadricSLAM_Dual_Quadrics_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. **2018**: 313-314.
    + ğŸ“œ **Semantic SLAM é¡¹ç›®ä¸»é¡µ**ï¼šhttp://www.semanticslam.ai/
    + ğŸ“œ **IROS 2017**ï¼š[**Meaningful maps with object-oriented semantic mapping**](https://arxiv.org/pdf/1609.07849)
+ ğŸ‘¦ **Michael Milford**ï¼šè°·æ­Œå­¦æœ¯ https://scholar.google.com/citations?user=TDSmCKgAAAAJ&hl=zh-CN&oi=ao
    + ğŸ“œ **ICRA 2012**ï¼š[**SeqSLAM: Visual route-based navigation for sunny summer days and stormy winter nights**](http://www.cim.mcgill.ca/~dudek/417/Resources/seqslam-milford.pdf) ï¼ˆä»£ç ï¼šhttps://michaelmilford.com/seqslam/ï¼‰
    + ğŸ“œ Ball D, Heath S, Wiles J, et al. [**OpenRatSLAM: an open source brain-based SLAM system**](https://static.springer.com/sgw/documents/1388513/application/pdf/10-3.pdf)[J]. Autonomous Robots, **2013**, 34(3): 149-176.ï¼ˆä»£ç ï¼šhttps://openslam-org.github.io/openratslam.html ï¼‰
    + ğŸ“œ Yu F, Shang J, Hu Y, et al. [**NeuroSLAM: a brain-inspired SLAM system for 3D environments**](https://link.springer.com/article/10.1007/s00422-019-00806-9)[J]. Biological Cybernetics, **2019**, 113(5-6): 515-545. ï¼ˆ**ä»£ç **ï¼šhttps://github.com/cognav/NeuroSLAM ï¼‰

#### 28. æ¾³å¤§åˆ©äºšæœºå™¨äººè§†è§‰ä¸­å¿ƒ

+ **ç ”ç©¶æ–¹å‘**ï¼šæœºå™¨äººæ„ŸçŸ¥ã€ç†è§£ä¸å­¦ä¹  ï¼ˆé›†åˆäº†æ˜†å£«å…°ç§‘æŠ€å¤§å­¦ï¼Œæ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦ï¼Œé˜¿å¾·è±å¾·å¤§å­¦ï¼Œæ˜†å£«å…°å¤§å­¦ç­‰å­¦æ ¡æœºå™¨äººé¢†åŸŸçš„ç ”ç©¶è€…ï¼‰
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.roboticvision.org/
+ **äººç‰©**ï¼šhttps://www.roboticvision.org/rv_person_category/researchers/
+ **å‘è¡¨è®ºæ–‡æ±‡æ€»**ï¼šhttps://www.roboticvision.org/publications/scientific-publications/
+ ğŸ‘¦ **Yasir Latif**ï¼š[ä¸ªäººä¸»é¡µ](http://ylatif.github.io/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=pGsO6EkAAAAJ&hl=zh-CN)
    + ğŸ“œ Latif Y, Cadena C, Neira J. [**Robust loop closing over time for pose graph SLAM**](http://webdiis.unizar.es/~ylatif/papers/IJRR.pdf)[J]. The International Journal of Robotics Research, **2013**, 32(14): 1611-1626.
    + ğŸ“œ Latif Y, Cadena C, Neira J. [**Robust loop closing over time**](https://pdfs.semanticscholar.org/62fb/619f7fc036c4dfb4c55a7c53907a112fe001.pdf)[C]//Proc. Robotics: Science Systems. **2013**: 233-240.ï¼ˆä»£ç ï¼šhttps://github.com/ylatif/rrr ï¼‰
+ ğŸ‘¦ **Ian D Reid**ï¼šè°·æ­Œå­¦æœ¯ï¼šhttps://scholar.google.com/citations?user=ATkNLcQAAAAJ&hl=zh-CN&oi=sra
    + ğŸ“œ **ICRA 2019**ï¼š[**Real-time monocular object-model aware sparse SLAM**](https://arxiv.org/pdf/1809.09149)
    + ğŸ“œ Reid I. [**Towards semantic visual SLAM**](https://ieeexplore.ieee.org/abstract/document/7064267/)[C]//2014 13th International Conference on Control Automation Robotics & Vision (ICARCV). IEEE, **2014**: 1-1.
    
#### 29. æ—¥æœ¬å›½ç«‹å…ˆè¿›å·¥ä¸šç§‘å­¦æŠ€æœ¯ç ”ç©¶æ‰€

+ **äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒ**ï¼šhttps://www.airc.aist.go.jp/en/intro/
+ ğŸ‘¦ **Ken Sakurada**ï¼š[ä¸ªäººä¸»é¡µ](https://kensakurada.github.io/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=Q4JO-ncAAAAJ&hl=zh-CN&oi=sra)
    + ğŸ“œ Sumikura S, Shibuya M, Sakurada K. [**OpenVSLAM: A Versatile Visual SLAM Framework**](https://dl.acm.org/doi/pdf/10.1145/3343031.3350539)[C]//Proceedings of the 27th ACM International Conference on Multimedia. **2019**: 2292-2295.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/xdspacelab/openvslam ï¼‰
+ ğŸ‘¦ **Shuji Oishi**ï¼š[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=wlPYSDgAAAAJ&hl=zh-CN&oi=sra)
    + ğŸ“œ æç¨ å¯†ç‰¹å¾ç‚¹å»ºå›¾ï¼šYokozuka M, Oishi S, Thompson S, et al. [**VITAMIN-E: visual tracking and MappINg with extremely dense feature points**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Yokozuka_VITAMIN-E_VIsual_Tracking_and_MappINg_With_Extremely_Dense_Feature_Points_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. **2019**: 9641-9650.
    + ğŸ“œ Oishi S, Inoue Y, Miura J, et al. [**SeqSLAM++: View-based robot localization and navigation**](https://staff.aist.go.jp/shuji.oishi/assets/papers/published/SeqSLAM++_RAS2019.pdf)[J]. Robotics and Autonomous Systems, **2019**, 112: 13-21.
    
#### 30. Pyojin Kimï¼ˆéŸ©å›½é¦–å°”å¤§å­¦è‡ªä¸»æœºå™¨äººå®éªŒå®¤ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šè§†è§‰é‡Œç¨‹è®¡ï¼Œå®šä½ï¼ŒAR/VR
+ ğŸ‘¦ [ä¸ªäººä¸»é¡µ](http://pyojinkim.com/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=NHpe_8IAAAAJ&hl=en)
+ ğŸ“œ **å¹³é¢ SLAM**ï¼šECCV 2018ï¼š[**Linear RGB-D SLAM for planar environments**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Pyojin_Kim_Linear_RGB-D_SLAM_ECCV_2018_paper.pdf)
+ ğŸ“œ **å…‰ç…§å˜åŒ–ä¸‹çš„é²æ£’ SLAM**ï¼šICRA 2017ï¼š[**Robust visual localization in changing lighting conditions**](https://www.nasa.gov/sites/default/files/atoms/files/kim2017robust.pdf)
+ ğŸ“œ **çº¿é¢ SLAM**ï¼šCVPR 2018ï¼š[**Indoor RGB-D Compass from a Single Line and Plane**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Kim_Indoor_RGB-D_Compass_CVPR_2018_paper.pdf)
+ ğŸ“œ **åšå£«å­¦ä½è®ºæ–‡**ï¼š[**Low-Drift Visual Odometry for Indoor Robotics**](http://pyojinkim.com/download/papers/2019_pjinkim_PhDthesis_low.pdf)

#### 31. é¦™æ¸¯ç§‘æŠ€å¤§å­¦ç©ºä¸­æœºå™¨äººå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šç©ºä¸­æœºå™¨äººåœ¨å¤æ‚ç¯å¢ƒä¸‹çš„è‡ªä¸»è¿è¡Œï¼ŒåŒ…æ‹¬çŠ¶æ€ä¼°è®¡ã€å»ºå›¾ã€è¿åŠ¨è§„åˆ’ã€å¤šæœºå™¨äººååŒä»¥åŠä½æˆæœ¬ä¼ æ„Ÿå™¨å’Œè®¡ç®—ç»„ä»¶çš„å®éªŒå¹³å°å¼€å‘ã€‚
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://uav.ust.hk/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttp://uav.ust.hk/publications/ 
+ ğŸ‘¦ æ²ˆé‚µåŠ¼æ•™æˆ[**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=u8Q0_xsAAAAJ&hl=zh-CN)
+ **ä»£ç å…¬å¼€åœ°å€**ï¼šhttps://github.com/HKUST-Aerial-Robotics
+ ğŸ“œ Qin T, Li P, Shen S. [**Vins-mono: A robust and versatile monocular visual-inertial state estimator**](https://arxiv.org/pdf/1708.03852.pdf)[J]. IEEE Transactions on Robotics, **2018**, 34(4): 1004-1020.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/HKUST-Aerial-Robotics/VINS-Mono ï¼‰
+ ğŸ“œ Wang K, Gao F, Shen S. [**Real-time scalable dense surfel mapping**](https://arxiv.org/pdf/1909.04250)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 6919-6925.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping ï¼‰

#### 32. é¦™æ¸¯ç§‘æŠ€å¤§å­¦æœºå™¨äººä¸å¤šæ„ŸçŸ¥å®éªŒå®¤ RAM-LAB

+ **ç ”ç©¶æ–¹å‘**ï¼šæ— äººè½¦ï¼›æ— äººèˆ¹ï¼›å®¤å†…å®šä½ï¼›æœºå™¨å­¦ä¹ ã€‚
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.ram-lab.com/
+ **å‘è¡¨è®ºæ–‡**ï¼šhttps://www.ram-lab.com/publication/ 
+ ğŸ‘¦ åˆ˜æ˜æ•™æˆ[**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=CdV5LfQAAAAJ&hl=zh-CN&oi=sra)
+ ğŸ“œ Ye H, Chen Y, Liu M. [**Tightly coupled 3d lidar inertial odometry and mapping**](https://arxiv.org/pdf/1904.06993.pdf)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 3144-3150.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/hyye/lio-mapping ï¼‰
+ ğŸ“œ Zhang J, Tai L, Boedecker J, et al. [**Neural slam: Learning to explore with external memory**]()[J]. arXiv preprint arXiv:1706.09520, **2017**.

#### 33. é¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤©çŸ³æœºå™¨äººå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šå·¥ä¸šã€ç‰©æµã€æ‰‹æœ¯æœºå™¨äººï¼Œä¸‰ç»´å½±åƒï¼Œæœºå™¨å­¦ä¹ 
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://ri.cuhk.edu.hk/
+ ğŸ‘¦ **åˆ˜äº‘è¾‰æ•™æˆ**ï¼šhttp://ri.cuhk.edu.hk/yhliu
+ ğŸ‘¦ **ææµ©æ˜‚**ï¼š[ä¸ªäººä¸»é¡µ](https://sites.google.com/view/haoangli/homepage)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=KnnPc0YAAAAJ&hl=zh-CN&oi=sra)
    + ğŸ“œ Li H, Yao J, Bazin J C, et al. [**A monocular SLAM system leveraging structural regularity in Manhattan world**](http://cvrs.whu.edu.cn/projects/Struct-PL-SLAM/source/file/Struct_PL_SLAM.pdf)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 2518-2525.
    + ğŸ“œ Li H, Yao J, Lu X, et al. [**Combining points and lines for camera pose estimation and optimization in monocular visual odometry**](https://ieeexplore.ieee.org/abstract/document/8202304/)[C]//2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2017**: 1289-1296.
    + ğŸ“œ æ¶ˆå¤±ç‚¹æ£€æµ‹ï¼šLu X, Yaoy J, Li H, et al. [**2-Line Exhaustive Searching for Real-Time Vanishing Point Estimation in Manhattan World**](https://www.computer.org/csdl/proceedings/wacv/2017/4822/00/07926628.pdf)[C]//Applications of Computer Vision (WACV), 2017 IEEE Winter Conference on. IEEE, **2017**: 345-353.ï¼ˆä»£ç ï¼šhttps://github.com/xiaohulugo/VanishingPointDetection ï¼‰
+ ğŸ‘¦ **éƒ‘å¸†**ï¼š[ä¸ªäººä¸»é¡µ](https://fzheng.me/cnabout/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=PZOTyfIAAAAJ&hl=zh-CN&oi=sra)
    + ğŸ“œ Zheng F, Tang H, Liu Y H. [**Odometry-vision-based ground vehicle motion estimation with se (2)-constrained se (3) poses**](https://ieeexplore.ieee.org/abstract/document/8357438/)[J]. IEEE transactions on cybernetics, **2018**, 49(7): 2652-2663.ï¼ˆä»£ç ï¼šhttps://github.com/izhengfan/se2clam ï¼‰
    + ğŸ“œ Zheng F, Liu Y H. [**Visual-Odometric Localization and Mapping for Ground Vehicles Using SE (2)-XYZ Constraints**](https://ieeexplore.ieee.org/abstract/document/8793928/)[C]//2019 International Conference on Robotics and Automation (ICRA). IEEE, **2019**: 3556-3562.ï¼ˆä»£ç ï¼šhttps://github.com/izhengfan/se2lam ï¼‰

#### 34. æµ™æ±Ÿå¤§å­¦ CAD&CG å›½å®¶é‡ç‚¹å®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šSFM/SLAMï¼Œä¸‰ç»´é‡å»ºï¼Œå¢å¼ºç°å®
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://www.zjucvg.net/
+ **Github ä»£ç åœ°å€**ï¼šhttps://github.com/zju3dv
+ ğŸ‘¦ **ç« å›½å³°æ•™æˆ**ï¼š[ä¸ªäººä¸»é¡µ](http://www.cad.zju.edu.cn/home/gfzhang/)ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=F0xfpXAAAAAJ&hl=zh-CN&oi=sra)
+ ğŸ“œ **ICE-BA**ï¼šLiu H, Chen M, Zhang G, et al. [**Ice-ba: Incremental, consistent and efficient bundle adjustment for visual-inertial slam**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_ICE-BA_Incremental_Consistent_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2018**: 1974-1982.ï¼ˆä»£ç ï¼šhttps://github.com/zju3dv/EIBA ï¼‰
+ ğŸ“œ **RK-SLAM**ï¼šLiu H, Zhang G, Bao H. [**Robust keyframe-based monocular SLAM for augmented reality**](https://ieeexplore.ieee.org/abstract/document/7781760/)[C]//2016 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, **2016**: 1-10.ï¼ˆé¡¹ç›®ä¸»é¡µï¼šhttp://www.zjucvg.net/rkslam/rkslam.html ï¼‰
+ ğŸ“œ **RD-SLAM**ï¼šTan W, Liu H, Dong Z, et al. [**Robust monocular SLAM in dynamic environments**](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.431.8137&rep=rep1&type=pdf)[C]//2013 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, **2013**: 209-218.

#### 35. é‚¹ä¸¹å¹³ï¼ˆä¸Šæµ·äº¤é€šå¤§å­¦ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šè§†è§‰ SLAMï¼ŒSFMï¼Œå¤šæºå¯¼èˆªï¼Œå¾®å‹æ— äººæœº
+ ğŸ‘¦ **ä¸ªäººä¸»é¡µ**ï¼šhttp://drone.sjtu.edu.cn/dpzou/index.php ï¼Œ [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=y6FsLDQAAAAJ&hl=en&oi=ao)
+ ğŸ“œ **Co-SLAM**ï¼šZou D, Tan P. [**Coslam: Collaborative visual slam in dynamic environments**](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.463.8135&rep=rep1&type=pdf)[J]. IEEE transactions on pattern analysis and machine intelligence, **2012**, 35(2): 354-366.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/danping/CoSLAM ï¼‰
+ ğŸ“œ **StructSLAM**ï¼šZhou H, Zou D, Pei L, et al. [**StructSLAM: Visual SLAM with building structure lines**]()[J]. IEEE Transactions on Vehicular Technology, **2015**, 64(4): 1364-1375.ï¼ˆ**é¡¹ç›®ä¸»é¡µ**ï¼šhttp://drone.sjtu.edu.cn/dpzou/project/structslam.php ï¼‰
+ ğŸ“œ **StructVIO**ï¼šZou D, Wu Y, Pei L, et al. [**StructVIO: visual-inertial odometry with structural regularity of man-made environments**](https://arxiv.org/pdf/1810.06796)[J]. IEEE Transactions on Robotics, **2019**, 35(4): 999-1013.

#### 36. å¸ƒæ ‘è¾‰æ•™æˆï¼ˆè¥¿åŒ—å·¥ä¸šå¤§å­¦æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šè¯­ä¹‰å®šä½ä¸å»ºå›¾ã€SLAMã€åœ¨çº¿å­¦ä¹ ä¸å¢é‡å­¦ä¹ 
+ ğŸ‘¦ **ä¸ªäººä¸»é¡µ**ï¼šhttp://www.adv-ci.com/blog/ &emsp; [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=spwZ6b4AAAAJ&hl=zh-CN&oi=ao)
+ **å¸ƒè€å¸ˆçš„è¯¾ä»¶**ï¼šhttp://www.adv-ci.com/blog/course/
+ å®éªŒå®¤ 2018 å¹´æš‘æœŸåŸ¹è®­èµ„æ–™ï¼šhttps://github.com/zdzhaoyong/SummerCamp2018
+ ğŸ“œ **å¼€æºçš„é€šç”¨ SLAM æ¡†æ¶**ï¼šZhao Y, Xu S, Bu S, et al. [**GSLAM: A general SLAM framework and benchmark**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Zhao_GSLAM_A_General_SLAM_Framework_and_Benchmark_ICCV_2019_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2019**: 1110-1120.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/zdzhaoyong/GSLAM ï¼‰
+ ğŸ“œ Bu S, Zhao Y, Wan G, et al. [**Map2DFusion: Real-time incremental UAV image mosaicing based on monocular slam**](http://www.adv-ci.com/publications/2016_IROS.pdf)[C]//2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2016**: 4564-4571.ï¼ˆ**ä»£ç **ï¼šhttps://github.com/zdzhaoyong/Map2DFusion ï¼‰
+ ğŸ“œ Wang W, Zhao Y, Han P, et al. [**TerrainFusion: Real-time Digital Surface Model Reconstruction based on Monocular SLAM**](https://ieeexplore.ieee.org/abstract/document/8967663/)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 7895-7902.

#### +1 Cyrill Stachnissï¼ˆå¾·å›½æ³¢æ©å¤§å­¦æ‘„å½±æµ‹é‡ä¸æœºå™¨äººå®éªŒå®¤ï¼‰

+ **ç ”ç©¶æ–¹å‘**ï¼šæ¦‚ç‡æœºå™¨äººã€SLAMã€è‡ªä¸»å¯¼èˆªã€è§†è§‰æ¿€å…‰æ„ŸçŸ¥ã€åœºæ™¯åˆ†æä¸åˆ†é…ã€æ— äººé£è¡Œå™¨
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://www.ipb.uni-bonn.de/
+ ğŸ‘¦ **ä¸ªäººä¸»é¡µ**ï¼šhttps://www.ipb.uni-bonn.de/people/cyrill-stachniss/ [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=8vib2lAAAAAJ&hl=zh-CN&authuser=1&oi=ao)
+ å‘è¡¨è®ºæ–‡ï¼šhttps://www.ipb.uni-bonn.de/publications/
+ å¼€æºä»£ç ï¼šhttps://github.com/PRBonn
+ ğŸ“œ IROS 2019 æ¿€å…‰è¯­ä¹‰ SLAMï¼šChen X, Milioto A, Palazzolo E, et al. [**SuMa++: Efficient LiDAR-based semantic SLAM**](https://ieeexplore.ieee.org/abstract/document/8967704/)[C]//2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2019**: 4530-4537.ï¼ˆä»£ç ï¼šhttps://github.com/PRBonn/semantic_suma/ ï¼‰
+ Cyrill Stachniss æ•™æˆ SLAM å…¬å¼€è¯¾ï¼š[youtube](https://www.youtube.com/watch?v=4QG0y0pIOBE&list=PLgnQpQtFTOGQh_J16IMwDlji18SWQ2PZ6) ï¼› [bilibili](https://space.bilibili.com/16886998/channel/detail?cid=118821)
+ æ³¢æ©å¤§å­¦å¦å¤–ä¸€ä¸ª**æ™ºèƒ½è‡ªä¸»ç³»ç»Ÿå®éªŒå®¤**ï¼šhttp://www.ais.uni-bonn.de/research.html

#### +1 ä¸Šæµ·ç§‘æŠ€å¤§å­¦

+ **Mobile Perception Lab**ï¼šhttp://mpl.sist.shanghaitech.edu.cn/
+ ğŸ‘¦ Laurent Kneipï¼š[ä¸ªäººä¸»é¡µ](https://www.laurentkneip.com/)ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com.au/citations?user=lTmh1e0AAAAJ&hl=en)
+ ğŸ“œ Zhou Y, Li H, Kneip L. [**Canny-vo: Visual odometry with rgb-d cameras based on geometric 3-dâ€“2-d edge alignment**](https://ieeexplore.ieee.org/abstract/document/8510917/)[J]. IEEE Transactions on Robotics, **2018**, 35(1): 184-199.
+ **è‡ªä¸»ç§»åŠ¨æœºå™¨äººå®éªŒå®¤**ï¼šhttps://robotics.shanghaitech.edu.cn/zh
+ ğŸ‘¦ SÃ¶ren Schwertfegerï¼š[ä¸ªäººä¸»é¡µ](https://robotics.shanghaitech.edu.cn/zh/people/soeren)ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com.au/citations?user=Y2olJ9kAAAAJ&hl=en&oi=ao)
+ ğŸ“œ Shan Z, Li R, Schwertfeger S. [**RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots**](https://link.zhihu.com/?target=https%3A//www.mdpi.com/1424-8220/19/10/2251)[J]. Sensors, **2019**, 19(10): 2251.ï¼ˆä»£ç ï¼šhttps://github.com/STAR-Center/VINS-RGBD ï¼‰

#### +1 ç¾å›½å¯†æ­‡æ ¹å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€

+ **å­¦é™¢å®˜ç½‘**ï¼šhttps://robotics.umich.edu/
+ **ç ”ç©¶æ–¹å‘**ï¼šhttps://robotics.umich.edu/research/focus-areas/
+ **æ„ŸçŸ¥æœºå™¨äººå®éªŒå®¤ï¼ˆPeRLï¼‰**
    + å®éªŒå®¤ä¸»é¡µï¼šhttp://robots.engin.umich.edu/About/
    + ğŸ‘¦ **Ryan M. Eustice** [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=WroYmiAAAAAJ&hl=en&oi=ao)
    + ğŸ“œ æ¿€å…‰é›·è¾¾æ•°æ®é›† Pandey G, McBride J R, Eustice R M. [**Ford campus vision and lidar data set**](https://journals.sagepub.com/doi/abs/10.1177/0278364911400640)[J]. The International Journal of Robotics Research, **2011**, 30(13): 1543-1552. | [æ•°æ®é›†](http://robots.engin.umich.edu/SoftwareData/Ford)
+ **APRIL robotics lab**
    + å®éªŒå®¤ä¸»é¡µï¼šhttps://april.eecs.umich.edu/
    + ğŸ‘¦ **Edwin Olson** [ä¸ªäººä¸»é¡µ](https://april.eecs.umich.edu/people/ebolson/) | [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=GwtVjKYAAAAJ&hl=en&oi=ao)
    + ğŸ“œ Olson E. [**AprilTag: A robust and flexible visual fiducial system**](https://april.eecs.umich.edu/pdfs/olson2010tags.pdf)[C]//2011 IEEE International Conference on Robotics and Automation. IEEE, **2011**: 3400-3407. | [**ä»£ç **](https://github.com/AprilRobotics/apriltag)
    + ğŸ“œ Wang X, Marcotte R, Ferrer G, et al. [**ApriISAM: Real-time smoothing and mapping**](https://april.eecs.umich.edu/papers/details.php?name=wang2018aprilsam)[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2018**: 2486-2493. | [**ä»£ç **](https://github.com/xipengwang/AprilSAM)
    
#### +1 ç‘å£«è‹é»ä¸–è”é‚¦ç†å·¥è‡ªä¸»ç³»ç»Ÿå®éªŒå®¤

+ **ç ”ç©¶æ–¹å‘**ï¼šå¤æ‚å¤šæ ·ç¯å¢ƒä¸­è‡ªä¸»è¿è¡Œçš„æœºå™¨äººå’Œæ™ºèƒ½ç³»ç»Ÿ
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://asl.ethz.ch/
+ å‘è¡¨è®ºæ–‡ï¼šhttps://asl.ethz.ch/publications-and-sources/publications.html
+ [youtube](https://www.youtube.com/channel/UCgqwlRBPdDL2k4OWvH6Oppg) | [Github](https://github.com/ethz-asl)
+ ğŸ‘¦ **Cesar Cadena** [ä¸ªäººä¸»é¡µ](http://n.ethz.ch/~cesarc/)
+ ğŸ“œ Schneider T, Dymczyk M, Fehr M, et al. [**maplab: An open framework for research in visual-inertial mapping and localization**](https://arxiv.org/pdf/1711.10250)[J]. IEEE Robotics and Automation Letters, **2018**, 3(3): 1418-1425. | [**ä»£ç **](https://github.com/ethz-asl/maplab)
+ ğŸ“œ DubÃ© R, Cramariuc A, Dugas D, et al. [**SegMap: 3d segment mapping using data-driven descriptors**](https://arxiv.org/pdf/1804.09557)[J]. arXiv preprint arXiv:1804.09557, **2018**. | [**ä»£ç **](https://github.com/ethz-asl/segmap)
+ ğŸ“œ Millane A, Taylor Z, Oleynikova H, et al. [**C-blox: A scalable and consistent tsdf-based dense mapping approach**](https://arxiv.org/pdf/1710.07242)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018: 995-1002. | [**ä»£ç **](https://github.com/ethz-asl/cblox)

#### +1 ç¾å›½éº»çœç†å·¥å­¦é™¢ Robust Robotics Group

+ **ç ”ç©¶æ–¹å‘**ï¼šMAV å¯¼èˆªä¸æ§åˆ¶ï¼›äººæœºäº¤äº’çš„è‡ªç„¶è¯­è¨€ç†è§£ï¼›è‡ªä¸»æµ·æ´‹æœºå™¨äººçš„è¯­ä¹‰ç†è§£
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttp://groups.csail.mit.edu/rrg/index.php
+ ğŸ‘¦ Nicholas Royï¼š[Google Scholar](https://scholar.google.com/citations?user=aM3i_9oAAAAJ&hl=zh-CN&oi=ao)
+ ğŸ“œ Greene W N, Ok K, Lommel P, et al. [**Multi-level mapping: Real-time dense monocular SLAM**](https://ieeexplore.ieee.org/abstract/document/7487213/)[C]//2016 IEEE International Conference on Robotics and Automation (**ICRA**). IEEE, **2016**: 833-840. [video](https://www.youtube.com/watch?v=qk2ViPVxmq0&feature=youtu.be)
+ ğŸ“œ ICRA 2020 [Metrically-Scaled Monocular SLAM using Learned Scale Factors." International Conference on Robotics and Automation](http://groups.csail.mit.edu/rrg/papers/greene_icra20.pdf) | [video](https://www.youtube.com/watch?v=qk2ViPVxmq0&feature=youtu.be)       
+ ğŸ“œ ICRA 2019 [Robust Object-based SLAM for High-speed Autonomous Navigation](http://groups.csail.mit.edu/rrg/papers/OkLiu19icra.pdf)

#### +1 ç‘å£«è‹é»ä¸–è”é‚¦ç†å·¥ Vision for Robotics Lab

+ **ç ”ç©¶æ–¹å‘**ï¼šæœºå™¨äººè§†è§‰ï¼Œæ— äººæœºï¼Œè‡ªä¸»å¯¼èˆªï¼Œå¤šæœºå™¨äººååŒ
+ **å®éªŒå®¤ä¸»é¡µ**ï¼šhttps://v4rl.ethz.ch/the-group.html
+ ğŸ‘¦ **Margarita Chli**ï¼š[ä¸ªäººä¸»é¡µ](http://www.margaritachli.com/)  | [Google Scholar](https://scholar.google.com/citations?user=C0UhwEIAAAAJ&hl=zh-CN&oi=ao)
+ ğŸ“œ Schmuck P, Chli M. [**CCMâ€SLAM: Robust and efficient centralized collaborative monocular simultaneous localization and mapping for robotic teams**](https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21854)[J]. Journal of Field Robotics, **2019**, 36(4): 763-781. [**code**](https://github.com/VIS4ROB-lab/ccm_slam) | [video](https://www.youtube.com/watch?v=P3b7UiTlmbQ&feature=youtu.be)       
+ ğŸ“œ Bartolomei L, Karrer M, Chli M. [**Multi-robot Coordination with Agent-Server Architecture for Autonomous Navigation in Partially Unknown Environments**](https://www.research-collection.ethz.ch/handle/20.500.11850/441280)[C]//IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2020)(virtual). **2020**. [**code**](https://github.com/VIS4ROB-lab/multi_robot_coordination) | [video](https://www.youtube.com/watch?v=ATQiTsbaSOw)       
+ ğŸ“œ Schmuck P, Chli M. [**Multi-uav collaborative monocular slam**](https://ieeexplore.ieee.org/abstract/document/7989445/)[C]//2017 IEEE International Conference on Robotics and Automation (ICRA). IEEE, **2017**: 3863-3870.

---

## 3. SLAM å­¦ä¹ èµ„æ–™
> è¿™ä¸€éƒ¨åˆ†çš„å†…å®¹ä¸å¤ªå®Œæ•´ï¼Œé™†ç»­ä¸°å¯Œï¼Œæ¬¢è¿è¡¥å……

### 3.1 å›½å†…èµ„æ–™

+ **1)** SLAMcnï¼šhttp://www.slamcn.org/index.php/
+ **2)** SLAM æœ€æ–°ç ”ç©¶æ›´æ–° Recent_SLAM_Research ï¼šhttps://github.com/YiChenCityU/Recent_SLAM_Research
+ **3)** **è¥¿åŒ—å·¥å¤§æ™ºèƒ½ç³»ç»Ÿå®éªŒå®¤ SLAM åŸ¹è®­**ï¼šhttps://github.com/zdzhaoyong/SummerCamp2018
  + å¸ƒæ ‘è¾‰è€å¸ˆè¯¾ä»¶ï¼šhttp://www.adv-ci.com/blog/course/
+ **4)** IROS 2019 **è§†è§‰æƒ¯å¯¼å¯¼èˆª**çš„æŒ‘æˆ˜ä¸åº”ç”¨ç ”è®¨ä¼šï¼šhttp://udel.edu/~ghuang/iros19-vins-workshop/index.html
+ **5)** æ³¡æ³¡æœºå™¨äºº **VIO** ç›¸å…³èµ„æ–™ï¼šhttps://github.com/PaoPaoRobot/Awesome-VIO
+ **6)** å´”åå¤ï¼šä¸»æµ **VIO è®ºæ–‡æ¨å¯¼åŠä»£ç è§£æ**ï¼šhttps://github.com/StevenCui/VIO-Doc
+ **7)** æè¨€ï¼š[SLAM ä¸­çš„å‡ ä½•ä¸å­¦ä¹ æ–¹æ³•](https://github.com/yanyan-li/SLAM-BOOK)
+ **8)** é»„å±±è€å¸ˆçŠ¶æ€ä¼°è®¡è§†é¢‘ï¼š[bilibili](https://www.bilibili.com/video/av66258275)
+ **9)** è°­å¹³è€å¸ˆ-SLAM 6å°æ—¶è¯¾ç¨‹ï¼š[bilibili](https://www.bilibili.com/video/BV1v4411p735)
+ **10)** 2020 å¹´ SLAM æŠ€æœ¯åŠåº”ç”¨æš‘æœŸå­¦æ ¡ï¼š[è§†é¢‘-bilibili](https://www.bilibili.com/video/BV1Hf4y1X7P5/) | [è¯¾ä»¶](http://www.cad.zju.edu.cn/home/gfzhang/download/2020-SLAM-Summer-School-slides.zip)

### 3.2 å›½å¤–èµ„æ–™

+ **1)** **äº‹ä»¶ç›¸æœº**ç›¸å…³ç ”ç©¶ä¸å‘å±•ï¼šhttps://github.com/uzh-rpg/event-based_vision_resources
+ **2)** åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡**è¯­å¢ƒæœºå™¨äººç ”ç©¶æ‰€** Nikolay Atanasov æ•™æˆ**æœºå™¨äººçŠ¶æ€ä¼°è®¡ä¸æ„ŸçŸ¥è¯¾ç¨‹** pptï¼šhttps://natanaso.github.io/ece276a2019/schedule.html
+ **3)** æ³¢æ©å¤§å­¦ **Mobile Sensing and Robotics Course** å…¬å¼€è¯¾ ï¼š[youtube](https://www.youtube.com/playlist?list=PLgnQpQtFTOGQJXx-x0t23RmRbjp_yMb4v) ï¼Œ[bilibili](https://space.bilibili.com/16886998/channel/detail?cid=118821)

### 3.3 å…¬ä¼—å·

+ **æ³¡æ³¡æœºå™¨äºº SLAM**ï¼špaopaorobot_slam

### 3.4 ä»£ç æ³¨é‡Š

> ä»Šå¤©ï¼ˆ2020.04.25ï¼‰åˆšæƒ³åˆ°çš„ä¸€ä¸ªç‚¹ï¼Œå°±ç®—å‰é¢æ•´ç†äº†å¤§é‡çš„å¼€æºå·¥ä½œï¼Œä½†æ˜¯çœ‹åŸç‰ˆçš„ä»£ç è¿˜æ˜¯ä¼šæœ‰å¾ˆå¤§çš„å›°éš¾ï¼Œ**æ„Ÿè°¢å›½å†… SLAM çˆ±å¥½è€…çš„å°†è‡ªå·±çš„ä»£ç æ³¨é‡Šåˆ†äº«å‡ºæ¥ï¼Œä¿ƒè¿›äº¤æµï¼Œå…±åŒè¿›æ­¥**ã€‚è¿™ä¸€å°èŠ‚çš„å†…å®¹é™†ç»­å‘æ˜ï¼ŒæœŸå¾…å¤§å®¶çš„æ¨èä»£ç æ³¨é‡Šï¼ˆå¯ä»¥åœ¨ issue ä¸­ç•™è¨€ï¼‰ã€‚

### 3.5 æ•°æ®é›†

+ [æ³¡æ³¡æœºå™¨äºº - SLAM æ•°æ®é›†åˆé›†](https://mp.weixin.qq.com/s/zRjwus68Kf4unIqPIubraw)
+ [è®¡ç®—æœºè§†è§‰life - SLAMã€é‡å»ºã€è¯­ä¹‰ç›¸å…³æ•°æ®é›†å¤§å…¨](https://zhuanlan.zhihu.com/p/68294012)
+ [æ°´ä¸‹ SLAM ç›¸å…³ç ”ç©¶ - ä»£ç ã€æ•°æ®é›†](http://note.youdao.com/s/GjCXvWFR)        

---

## 4. è¿‘æœŸè®ºæ–‡ï¼ˆæŒç»­æ›´æ–°ï¼‰

### 2020 å¹´ 10 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰
 
> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 11 æœˆ 09 æ—¥       
å…± 22 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 7 é¡¹ï¼ˆå¾…ï¼‰å¼€æºå·¥ä½œ       
9,10,11ï¼šSLAM ä¸­åŠ¨æ€ç‰©ä½“è·Ÿè¸ªï¼ŒåŠ¨æ€ç‰©ä½“çº§ SLAM ä»Šå¹´å¾ˆç«        
3,7,8,14,18ï¼šçº¿æ®µç›¸å…³**

#### 1. Geometric SLAM

+ [ ] **[1]** Bhutta M, Kuse M, Fan R, et al. [**Loop-box: Multi-Agent Direct SLAM Triggered by Single Loop Closure for Large-Scale Mapping**](https://arxiv.org/abs/2009.13851)[J]. arXiv preprint arXiv:2009.13851, **2020**. IEEE Transactions on Cybernetics, 2020
    + <font color = blue>ç”¨äºå¤§è§„æ¨¡å»ºå›¾çš„ç”±å•é—­ç¯è§¦å‘çš„å¤šæ™ºèƒ½ä½“ç›´æ¥ SLAM</font>
    + é¦™æ¸¯ç§‘æŠ€å¤§å­¦ï¼›[é¡¹ç›®ä¸»é¡µ](https://usmanmaqbool.github.io/loop-box)ï¼›[video](https://www.youtube.com/watch?v=AatjVz5ysV8)
    + æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 11.079
+ [ ] **[2]** Zhou B, He Y, Qian K, et al. [**S4-SLAM: A real-time 3D LIDAR SLAM system for ground/watersurface multi-scene outdoor applications**](https://link.springer.com/article/10.1007/s10514-020-09948-3)[J]. Autonomous Robots, **2020**: 1-22.
    + <font color = blue>S4-SLAMï¼šç”¨äºåœ°é¢/æ°´é¢å¤šåœºæ™¯æˆ·å¤–åº”ç”¨çš„å®æ—¶ 3D LIDAR SLAM ç³»ç»Ÿ</font>
    + ä¸œå—å¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q1ï¼ŒIF 3.6
+ [x] **[3]** Li Y, Yunus R, Brasch N, et al. [**RGB-D SLAM with Structural Regularities**](https://arxiv.org/abs/2010.07997)[J]. arXiv preprint arXiv:2010.07997, **2020**.
    + <font color = blue>**å…·æœ‰ç»“æ„è§„å¾‹çš„ RGB-D SLAM**</font>
    + TUM
+ [ ] **[4]** RodrÃ­guez J J G, Lamarca J, Morlana J, et al. [**SD-DefSLAM: Semi-Direct Monocular SLAM for Deformable and Intracorporeal Scenes**](https://arxiv.org/abs/2010.09409)[J]. arXiv preprint arXiv:2010.09409, **2020**.
    + <font color = blue>SD-DefSLAMï¼šé€‚ç”¨äºå¯å˜å½¢å’Œä½“å†…åœºæ™¯çš„åŠç›´æ¥æ³•å•ç›® SLAM</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦ï¼›ICRA 2021 æŠ•ç¨¿è®ºæ–‡ï¼›[Video](https://www.youtube.com/watch?v=gkcC0IR3X6A&feature=youtu.be)
+ [x] **[5]** Millane A, Oleynikova H, Lanegger C, et al. [**Freetures: Localization in Signed Distance Function Maps**](https://arxiv.org/abs/2010.09378)[J]. arXiv preprint arXiv:2010.09378, **2020**.
    + <font color = blue>**Freeturesï¼šåœ¨ SDF åœ°å›¾ä¸­è¿›è¡Œå®šä½**</font>
    + ETHï¼›[ä»£ç å¼€æº](https://github.com/alexmillane/freetures)ï¼ˆå¾…å¼€æºï¼‰ï¼›[Video](https://www.youtube.com/watch?v=O7ztVZDtDb0&feature=youtu.be)
+ [ ] **[6]** Long R, Rauch C, Zhang T, et al. [**RigidFusion: Robot Localisation and Mapping in Environments with Large Dynamic Rigid Objects**](https://arxiv.org/abs/2010.10841)[J]. arXiv preprint arXiv:2010.10841, **2020**.
    + <font color = blue>RigidFusion: åœ¨å…·æœ‰åŠ¨æ€åˆšä½“ç‰©ä½“çš„å¤§å‹åœºæ™¯ä¸­è¿›è¡Œæœºå™¨äººå®šä½ä¸å»ºå›¾</font>
    + çˆ±ä¸å ¡å¤§å­¦æœºå™¨äººä¸­å¿ƒ
+ [x] **[7]** Garcia-Fidalgo E, Ortiz A, Islands B. [**LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure Detection**](https://arxiv.org/abs/2009.09897)[J]. arXiv e-prints, **2020**: arXiv: 2009.09897.
    + <font color = blue>**LiPo-LCDï¼šç‚¹çº¿å‡ ä½•çš„åŸºäºå¤–è§‚çš„é—­ç¯æ£€æµ‹**</font>
    + å·´åˆ©é˜¿é‡Œç¾¤å²›å¤§å­¦
+ [ ] **[8]** Han J, Dong R, Kan J. [**A novel loop closure detection method with the combination of points and lines based on information entropy**](https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21992)[J]. Journal of Field Robotics. **2020**
    + <font color = blue>ä¸€ç§æ–°çš„åŸºäºä¿¡æ¯ç†µçš„ç‚¹çº¿é—­ç¯æ£€æµ‹æ–¹æ³•</font>
    + åŒ—äº¬æ—ä¸šå¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 3.58

---

#### 2. Semantic/Deep SLAM

+ [x] **[9]** Bescos B, Campos C, TardÃ³s J D, et al. [**DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM**](https://arxiv.org/abs/2010.07820)[J]. arXiv preprint arXiv:2010.07820, **2020**.
    + <font color = blue>**DynaSLAM II: å¤šç›®æ ‡è·Ÿè¸ªä¸ SLAM ç´§è€¦åˆ**</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦ï¼›ä¸€ä½œæ˜¯ DynaSLAM çš„ä½œè€…ï¼ŒäºŒä½œæ˜¯ ORB-SLAM3 çš„ä½œè€…
+ [x] **[10]** Bescos B, Cadena C, Neira J. [**Empty Cities: a Dynamic-Object-Invariant Space for Visual SLAM**](https://arxiv.org/abs/2010.07646)[J]. arXiv preprint arXiv:2010.07646, **2020**.
    + <font color = blue>**è§†è§‰ SLAM åŠ¨æ€çš„ç‰©ä½“ä¸å˜ç©ºé—´**</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦ã€ETHï¼›[ä»£ç å¼€æº](https://github.com/bertabescos/EmptyCities_SLAM)
    + [ä¸ªäººä¸»é¡µ](https://bertabescos.github.io/)ï¼Œç›¸å…³è®ºæ–‡ï¼š
        + ICRA 2018 Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space
+ [x] **[11]** Ballester I, Fontan A, Civera J, et al. [**DOT: Dynamic Object Tracking for Visual SLAM**](https://arxiv.org/abs/2010.00052)[J]. arXiv preprint arXiv:2010.00052, **2020**.
    + <font color = blue>**è§†è§‰ SLAM çš„åŠ¨æ€ç‰©ä½“è·Ÿè¸ª**</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦
+ [ ] **[12]** Wu S C, Tateno K, Navab N, et al. [**SCFusion: Real-time Incremental Scene Reconstruction with Semantic Completion**](https://arxiv.org/abs/2010.13662)[J]. arXiv preprint arXiv:2010.13662, **2020**.
    + <font color = blue>SCFusionï¼šå…·æœ‰å®Œæ•´è¯­ä¹‰çš„å®æ—¶å¢é‡åœºæ™¯é‡å»º</font>
    + TUM
+ [ ] **[13]** Mallick A, StÃ¼ckler J, Lensch H. [**Learning to Adapt Multi-View Stereo by Self-Supervision**](https://arxiv.org/abs/2009.13278)[J]. arXiv preprint arXiv:2009.13278, **2020**.
    + <font color = blue>**é€šè¿‡è‡ªç›‘ç£å­¦ä¹ çš„è‡ªé€‚åº”å¤šè§†å›¾ç«‹ä½“åŒ¹é…**</font>
    + å›¾å®¾æ ¹å¤§å­¦ï¼Œé©¬æ™®æ‰€ JÃ¶rg StÃ¼cklerï¼Œ**BMVC 2020**
    + åŸºäº ECCV 2018 [MVSNet: Depth Inference for Unstructured Multi-view Stereo](https://arxiv.org/abs/1804.02505)ï¼Œ[ä»£ç ](https://github.com/xy-guo/MVSNet_pytorch)

---

#### 3. Sensor Fusion

+ [x] **[14]** Li X, Li Y, Ornek E P, et al. [**Co-Planar Parametrization for Stereo-SLAM and Visual-Inertial Odometry**](https://ieeexplore.ieee.org/abstract/document/9207814)[J]. IEEE Robotics and Automation Letters, **2020**.
    + <font color = blue>**åŒç›® SLAM å’Œ VIO çš„å…±é¢å‚æ•°åŒ–**</font>
    + åŒ—äº¬å¤§å­¦ï¼Œ[ä»£ç å¼€æº](https://github.com/LiXin97/Co-Planar-Parametrization)ï¼ˆæš‚æœªæ”¾å‡ºï¼‰
+ [ ] **[15]** Liu Z, Zhang F. [**BALM: Bundle Adjustment for Lidar Mapping**](https://arxiv.org/abs/2010.08215)[J]. arXiv preprint arXiv:2010.08215, **2020**.
    + <font color = blue>BALMï¼šæ¿€å…‰é›·è¾¾å»ºå›¾ä¸­çš„ BA ä¼˜åŒ–</font>
    + é¦™æ¸¯å¤§å­¦ï¼Œ[ä»£ç å¼€æº](https://github.com/hku-mars/BALM)
+ [x] **[16]** Nguyen T M, Yuan S, Cao M, et al. [**VIRAL-Fusion: A Visual-Inertial-Ranging-Lidar Sensor Fusion Approach**](https://arxiv.org/abs/2010.12274)[J]. arXiv preprint arXiv:2010.12274, **2020**.
    + <font color = blue>**VIRAL-Fusion: è§†è§‰-æƒ¯æ€§-æµ‹è·-æ¿€å…‰é›·è¾¾ä¼ æ„Ÿå™¨èåˆæ–¹æ³•**</font>
    + å—æ´‹ç†å·¥
+ [ ] **[17]** Liu J, Gao W, Hu Z. [**Optimization-Based Visual-Inertial SLAM Tightly Coupled with Raw GNSS Measurements**](https://arxiv.org/abs/2010.11675)[J]. arXiv preprint arXiv:2010.11675, **2020**.
    + <font color = blue>åŸºäºä¼˜åŒ–çš„è§†è§‰æƒ¯æ€§ SLAM ä¸åŸå§‹ GNSS æµ‹é‡ç´§è€¦åˆ</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ï¼›ICRA 2021 æŠ•ç¨¿è®ºæ–‡

---

#### 4. Others

+ [ ] **[18]** Taubner F, Tschopp F, Novkovic T, et al. [**LCD--Line Clustering and Description for Place Recognition**](https://arxiv.org/abs/2010.10867)[J]. arXiv preprint arXiv:2010.10867, 2020. (**3DV 2020**)
    + <font color = blue>LCD: ç”¨äºä½ç½®è¯†åˆ«çš„çº¿æ®µèšç±»å’Œæè¿°</font>
    + ETHï¼›[ä»£ç å¼€æº](https://github.com/ethz-asl/lcd)
+ [x] **[19]** Triebel R. [**3D Scene Reconstruction from a Single Viewport**](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670052.pdf). ECCV 2020
    + <font color = blue>**å•è§†è§’è¿›è¡Œä¸‰ç»´åœºæ™¯é‡å»º**</font>
    + TUMï¼›[ä»£ç å¼€æº](https://github.com/DLR-RM/SingleViewReconstruction)
+ [ ] **[20]** Hidalgo-CarriÃ³ J, Gehrig D, Scaramuzza D. [**Learning Monocular Dense Depth from Events**](https://arxiv.org/pdf/2010.08350.pdf)[J]. arXiv preprint arXiv:2010.08350, 2020.(**3DV 2020**)
    + <font color = blue>ä»äº‹ä»¶ä¸­å­¦ä¹ å•ç›®ç¨ å¯†æ·±åº¦</font>
    + è‹é»ä¸–å¤§å­¦ï¼›[ä»£ç å¼€æº](https://github.com/uzh-rpg/rpg_e2depth)ï¼›[é¡¹ç›®ä¸»é¡µ](http://rpg.ifi.uzh.ch/E2DEPTH.html)
+ [x] **[21]** Yang B. [**Learning to reconstruct and segment 3D objects**](https://arxiv.org/abs/2010.09582)[J]. arXiv preprint arXiv:2010.09582, **2020**.
    + <font color = blue>**å­¦ä¹ é‡å»ºå’Œåˆ†å‰² 3D ç‰©ä½“**</font>
    + ç‰›æ´¥å¤§å­¦ [BoYang](https://yang7879.github.io/) åšå£«å­¦ä½è®ºæ–‡
+ [ ] **[22]** von Stumberg L, Wenzel P, Yang N, et al. [**LM-Reloc: Levenberg-Marquardt Based Direct Visual Relocalization**](**ç²—ä½“æ–‡æœ¬**)[J]. arXiv preprint arXiv:2010.06323, **2020**.
    + <font color = blue>LM-Relocï¼šåŸºäºLevenberg-Marquardt çš„ç›´æ¥è§†è§‰é‡å®šä½</font>
    + TUM
    + ç›¸å…³è®ºæ–‡ï¼švon Stumberg L, Wenzel P, Khan Q, et al. [**Gn-net: The gauss-newton loss for multi-weather relocalization**](https://arxiv.org/abs/1904.11932)[J]. IEEE Robotics and Automation Letters, 2020, 5(2): 890-897.

---

### 2020 å¹´ 9 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰
 
> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 9 æœˆ 28 æ—¥       
å…± 20 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 6 é¡¹ï¼ˆå¾…ï¼‰å¼€æºå·¥ä½œ       
4-5ï¼šæœºå™¨äººè‡ªä¸»æ¢ç´¢        
8-11ï¼šå¤šè·¯æ ‡ SLAM        
13: Jan Czarnowski åšå£«å­¦ä½è®ºæ–‡        
17-20ï¼šå¢å¼ºç°å®ç›¸å…³çš„å‡ é¡¹å¾ˆå¥½ç©çš„å·¥ä½œ**

#### 1. Geometric SLAM

+ [ ] **[1]** FZhao Y, Smith J S, Vela P A. [**Good graph to optimize: Cost-effective, budget-aware bundle adjustment in visual SLAM**](https://arxiv.org/abs/2008.10123)[J]. arXiv preprint arXiv:2008.10123, **2020**.
    + <font color = blue>Good Graph to Optimize: è§†è§‰ SLAM ä¸­å…·æœ‰æˆæœ¬æ•ˆç›Šã€å¯æ„ŸçŸ¥é¢„ç®—çš„ BA</font>
    + ä½æ²»äºšç†å·¥å­¦é™¢ [Yipu Zhao](https://sites.google.com/site/zhaoyipu/)
    + ä½œè€…æœ‰å¾ˆå¤š Good ç³»åˆ—çš„æ–‡ç« 
        + IROS 2018 **Good feature selection for least squares pose optimization in VO/VSLAM**
        + ECCV 2018 **Good line cutting: Towards accurate pose tracking of line-assisted VO/VSLAM**
        + T-RO 2020 **Good Feature Matching: Towards Accurate, Robust VO/VSLAM with Low Latency**
+ [ ] **[2]** Fu Q, Yu H, Wang X, et al. [**FastORB-SLAM: a Fast ORB-SLAM Method with Coarse-to-Fine Descriptor Independent Keypoint Matching**](https://arxiv.org/pdf/2008.09870.pdf)[J]. arXiv preprint arXiv:2008.09870, **2020**.
    + <font color = blue>FastORB-SLAM: ä¸€ç§ Coarse-to-Fine æè¿°ç¬¦ç‹¬ç«‹å…³é”®ç‚¹åŒ¹é…çš„å¿«é€Ÿ ORB-SLAM æ–¹æ³•</font>
    + æ¹–å—å¤§å­¦
+ [x] **[3]** Wenzel P, Wang R, Yang N, et al. [**4Seasons: A Cross-Season Dataset for Multi-Weather SLAM in Autonomous Driving**](https://arxiv.org/abs/2009.06364)[J]. arXiv preprint arXiv:2009.06364, **2020**.
    + <font color = blue>**4Seasonsï¼šè‡ªåŠ¨é©¾é©¶ä¸­å¤šå¤©æ°”SLAMçš„è·¨å­£èŠ‚æ•°æ®é›†**</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ [Nan Yang](https://vision.in.tum.de/members/yangn)
    + æ•°æ®é›†ç½‘é¡µï¼šhttp://www.4seasons-dataset.com/
+ [x] **[4]** Duong T, Yip M, Atanasov N. [**Autonomous Navigation in Unknown Environments with Sparse Bayesian Kernel-based Occupancy Mapping**](https://arxiv.org/abs/2009.07207)[J]. arXiv preprint arXiv:2009.07207, **2020**.
    + <font color = blue>**åŸºäºç¨€ç–è´å¶æ–¯æ ¸å æœ‰åœ°å›¾çš„æœªçŸ¥ç¯å¢ƒè‡ªä¸»å¯¼èˆª**</font>
    + UCSD [Nikolay A. Atanasov](https://natanaso.github.io/)
    + [é¡¹ç›®ä¸»é¡µ](https://thaipduong.github.io/sbkm/) | [**ä»£ç å¼€æº**](https://github.com/thaipduong/sbkm)
+ [ ] **[5]** Bartolomei L, Karrer M, Chli M. [**Multi-robot Coordination with Agent-Server Architecture for Autonomous Navigation in Partially Unknown Environments**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/441280/IROS_2020_Multi_Robot_compressed_v4.pdf?sequence=1)[C]//IEEE/RSJ International Conference on Intelligent Robots and Systems (**IROS** 2020)(virtual). **2020**.
    + <font color = blue>åœ¨éƒ¨åˆ†æœªçŸ¥ç¯å¢ƒä¸­å®ç°è‡ªä¸»å¯¼èˆªçš„å¤šæœºå™¨äººååŒä»£ç†æœåŠ¡å™¨æ¶æ„</font>
    + ETH | [ä»£ç å¼€æº](https://github.com/VIS4ROB-lab/voxblox_multi_agent) | [video](https://www.youtube.com/watch?v=BlFbiuV-d10&feature=youtu.be)
+ [ ] **[6]** Kern A, Bobbe M, Khedar Y, et al. [**OpenREALM: Real-time Mapping for Unmanned Aerial Vehicles**](https://arxiv.org/abs/2009.10492)[J]. arXiv preprint arXiv:2009.10492, **2020**.
    + <font color = blue>OpenREALMï¼šæ— äººæœºå®æ—¶å»ºå›¾æ¡†æ¶</font>
    + [ä»£ç å¼€æº](https://github.com/laxnpander/OpenREALM) | [video](https://www.youtube.com/watch?v=9MvPTHP0r0c)
+ [ ] **[7]** Du Z J, Huang S S, Mu T J, et al. [**Accurate RGB-D SLAM in Dynamic Environment using Observationally Consistent Conditional Random Fields**](https://cg.cs.tsinghua.edu.cn/people/~shisheng/Papers/OC-CRF/paper6.pdf). 2020
    + <font color = blue>åŠ¨æ€ç¯å¢ƒä¸­ä½¿ç”¨è§‚å¯Ÿä¸€è‡´ CRF çš„ç²¾ç¡® RGB-D SLAM</font>
    + æ¸…åå¤§å­¦
+ [ ] **[8]** Holynski A, Geraghty D, Frahm J M, et al. [**Reducing Drift in Structure from Motion using Extended Features**](https://arxiv.org/abs/2008.12295)[J]. arXiv preprint arXiv:2008.12295, **2020**.
    + <font color = blue>ä½¿ç”¨æ‹“å±•ç‰¹å¾å‡å° SFM ä¸­çš„æ¼‚ç§»</font>
    + åç››é¡¿å¤§å­¦ï¼ŒFacebook
+ [ ] **[9]** Fu Q, Wang J, Yu H, et al. [**PL-VINS: Real-Time Monocular Visual-Inertial SLAM with Point and Line**](https://arxiv.org/abs/2009.07462)[J]. arXiv preprint arXiv:2009.07462, **2020**.
    + <font color = blue>PL-VINS: å®æ—¶ç‚¹çº¿å•ç›®è§†è§‰æƒ¯æ€§ SLAM</font>
    + æ¹–å—å¤§å­¦ | [ä»£ç å¼€æº](https://github.com/cnqiangfu/PL-VINS)
+ [ ] **[10]** Company-Corcoles J P, Garcia-Fidalgo E, Ortiz A. [**LiPo-LCD: Combining Lines and Points for Appearance-based Loop Closure Detection**](https://arxiv.org/abs/2009.09897)[J]. arXiv preprint arXiv:2009.09897, 2020.(**BMVC 2020**)
    + <font color = blue>å‡ ä½•ç‚¹çº¿çš„åŸºäºå¤–è§‚çš„é—­ç¯æ£€æµ‹</font>
    + å·´åˆ©é˜¿é‡Œç¾¤å²›å¤§å­¦
+ [x] **[11]** Wang Q, Yan Z, Wang J, et al. [**Line Flow based SLAM**]()[J]. arXiv preprint arXiv:2009.09972, **2020**.
    + <font color = blue>**åŸºäº SLAM çš„çº¿æµ**</font>
    + åŒ—å¤§
+ [ ] **[12]** Badias A, Alfaro I, Gonzalez D, et al. [**MORPH-DSLAM: Model Order Reduction for PHysics-based Deformable SLAM**](https://arxiv.org/abs/2009.00576)[J]. arXiv preprint arXiv:2009.00576, **2020**.
    + <font color = blue>åŸºäºç‰©ç†å¯å˜å½¢ SLAM é™ä½æ¨¡å‹é˜¶æ•°</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦

---

#### 2. Semantic/Deep SLAM

+ [x] **[13]** Czarnowski J. [**Learned representations for real-time monocular SLAM**](https://spiral.imperial.ac.uk/handle/10044/1/82178)[J]. 2020.
    + <font color = blue>**å®æ—¶å•ç›® SLAM çš„å­¦ä¹ è¡¨ç¤º**</font>
    + å¸å›½ç†å·¥å­¦é™¢ [Jan Czarnowski](https://scholar.google.com/citations?user=t-eq4skAAAAJ&hl=zh-CN&oi=sra) åšå£«å­¦ä½è®ºæ–‡ | å¯¼å¸ˆ Andrew Davison
    + ä»£è¡¨ä½œ
        + CVPR 2018 [CodeSLAMâ€”learning a compact, optimisable representation for dense visual SLAM](https://openaccess.thecvf.com/content_cvpr_2018/papers/Bloesch_CodeSLAM_--_Learning_CVPR_2018_paper.pdf)
        + RAL 2020 [Deepfactors: Real-time probabilistic dense monocular slam](https://arxiv.org/pdf/2001.05049.pdf)
        + icra 2018 [DeepFusion: real-time dense 3D reconstruction for monocular SLAM using single-view depth and gradient predictions](https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dyson-robotics-lab/tlaidlow_etal_icra2019.pdf)
+ [ ] **[14]** Li J, Pei L, Zou D, et al. [**Attention-SLAM: A Visual Monocular SLAM Learning from Human Gaze**](https://arxiv.org/abs/2009.06886)[J]. arXiv preprint arXiv:2009.06886, **2020**.
    + <font color = blue>Attention-SLAMï¼šä»äººç±»è§†çº¿ä¸­å­¦ä¹ çš„å•ç›®è§†è§‰ SLAM</font>
    + ä¸Šæµ·äº¤é€šå¤§å­¦
+ [ ] **[15]** Cremona J, Uzal L, Pire T. [**WGANVO: Monocular Visual Odometry based on Generative Adversarial Networks**](https://arxiv.org/abs/2007.13704)[J]. arXiv preprint arXiv:2007.13704, **2020**.
    + <font color = blue>WGANVO: åŸºäºç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„å•ç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + é˜¿æ ¹å»· CIFASIS | [ä»£ç å¼€æº](https://github.com/CIFASIS/wganvo) | [video](https://www.youtube.com/watch?v=zg5BlvUQhWE)
+ [x] **[16]** LabbÃ© Y, Carpentier J, Aubry M, et al. [**CosyPose: Consistent multi-view multi-object 6D pose estimation**](https://arxiv.org/abs/2008.08465)[J]. arXiv preprint arXiv:2008.08465, **2020**.ï¼ˆECCV 2020ï¼‰
    + <font color = blue>**CosyPoseï¼šä¸€è‡´çš„å¤šè§†å›¾å¤šç‰©ä½“ 6D ä½å§¿ä¼°è®¡**</font>
    + Object SLAM @ ç‰©ä½“ä½å§¿ä¼°è®¡

---

#### 3. AR/VR/MR

+ [x] **[17]** Yang X, Zhou L, Jiang H, et al. [**Mobile3DRecon: Real-time Monocular 3D Reconstruction on a Mobile Phone**](https://ieeexplore.ieee.org/document/9201064/authors#authors)[J]. IEEE Annals of the History of Computing, **2020** (01): 1-1.
    + <font color = blue>**Mobile3DReconæ‰‹æœºä¸Šçš„å®æ—¶å•ç›®é‡å»º**</font>
    + å•†æ±¤ã€æµ™å¤§
+ [ ] **[18]** Ungureanu D, Bogo F, Galliani S, et al. HoloLens 2 Research Mode as a Tool for Computer Vision Research[J]. arXiv preprint arXiv:2008.11239, 2020.
    + <font color = blue>HoloLens 2 ç ”ç©¶æ¨¡å¼ä½œä¸ºè®¡ç®—æœºè§†è§‰ç ”ç©¶çš„å·¥å…·</font>
    + ä¸‰æ˜Ÿ AI ä¸­å¿ƒï¼Œå¾®è½¯
+ [ ] **[19]** Mori S, Erat O, Broll W, et al. [**InpaintFusion: Incremental RGB-D Inpainting for 3D Scenes**](https://ieeexplore.ieee.org/abstract/document/9184389)[J]. IEEE Transactions on Visualization and Computer Graphics, **2020**, 26(10): 2994-3007.
    + <font color = blue>InpaintFusionï¼š3Dåœºæ™¯çš„å¢é‡RGB-Dä¿®å¤</font>
    + æ ¼æ‹‰èŒ¨å·¥ä¸šå¤§å­¦ æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒº,JCR Q1, IF 4.558
+ [ ] **[20]** [**AAR: Augmenting a Wearable Augmented Reality Display with an Actuated Head-Mounted Projector**](https://jjhartmann.github.io/AugmentedAugmentedReality/assets/Hartmann_AugmentedAugmentedReality_UIST2020_LowRes.pdf). 2020
    + <font color = blue>ä½¿ç”¨å¯é©±åŠ¨çš„å¤´æˆ´å¼æŠ•å½±ä»ªå¢å¼ºå¯ç©¿æˆ´çš„å¢å¼ºç°å®æ˜¾ç¤º</font>
    + æ»‘é“å¢å¤§å­¦
    + åœ¨ AR çœ¼é•œä¸Šå†è£…ä¸ªæŠ•å½±ä»ªã€‚ã€‚ã€‚ã€‚ä¼šç©

---

### 2020 å¹´ 8 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ30 ç¯‡ï¼‰

> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 8 æœˆ 27 æ—¥       
å…± 30 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 11 é¡¹ï¼ˆå¾…ï¼‰å¼€æºå·¥ä½œ       
è¿™ä¸ªæœˆå…¬å¼€çš„è®ºæ–‡æ¯”è¾ƒå¤šï¼Œä¸”æœ‰æ„æ€ã€é«˜è´¨é‡çš„å·¥ä½œä¹Ÿä¸å°‘ï¼Œå¤šæ¥è‡ªäº IROSã€RAL(å¤§éƒ¨åˆ†ä¹ŸåŒæ­¥å‘è¡¨äº IROS)ï¼Œæ¯”å¦‚èåˆè§†è§‰ã€æƒ¯å¯¼ã€LiDAR çš„ LIC-Fusion 2.0 å’Œ èåˆç‰©ä½“è¯­ä¹‰çš„è§†æƒ¯é‡Œç¨‹è®¡ OrcVIOï¼Œå…¶ä»–ï¼š         
4-6ã€15ï¼šå¤šæœº/å¤šåœ°å›¾        
8-13ï¼šç»“æ„åŒ–/å®¤å†… SLAM**

#### 1. Geometric SLAM

+ [ ] **[1]** Geppert M, Larsson V, Speciale P, et al. [**Privacy Preserving Structure-from-Motion**](https://cvg.ethz.ch/research/privacy-preserving-sfm/paper/Geppert2020ECCV.pdf)[J]. **2020**.
    + <font color = blue>å…·æœ‰éšç§ä¿æŠ¤çš„ SFM</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥
    + ç›¸å…³è®ºæ–‡ï¼š
        + CVPR 2019 [Privacy Preserving Image-Based Localization](http://openaccess.thecvf.com/content_CVPR_2019/papers/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.pdf)
        + ECCV 2020 [Privacy Preserving Visual SLAM](https://arxiv.org/abs/2007.10361)
+ [x] **[2]** Zhang Z, Scaramuzza D. [**Fisher Information Field: an Efficient and Differentiable Map for Perception-aware Planning**](https://arxiv.org/abs/2008.03324)[J]. arXiv preprint arXiv:2008.03324, **2020**.
    + <font color = blue>Fisher ä¿¡æ¯åœºï¼šä¸€ç§ç”¨äºæ„ŸçŸ¥è§„åˆ’çš„é«˜æ•ˆä¸”å¯å¾®åˆ†çš„åœ°å›¾</font>
    + è‹é»ä¸–å¤§å­¦[å¼ å­æ½®](http://www.ifi.uzh.ch/en/rpg/people/zichao.html) | [ä»£ç å¼€æº](https://github.com/uzh-rpg/rpg_information_field)
    + ç›¸å…³å·¥ä½œï¼šICRA 2019 [Beyond Point Clouds: Fisher Information Field for Active Visual Localization](http://rpg.ifi.uzh.ch/docs/ICRA19_Zhang.pdf)
+ [ ] **[3]** Zhou Y, Gallego G, Shen S. [**Event-based Stereo Visual Odometry**](https://arxiv.org/abs/2007.15548)[J]. arXiv preprint arXiv:2007.15548, **2020**.
    + <font color = blue>åŸºäºäº‹ä»¶ç›¸æœºçš„åŒç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + æ¸¯ç§‘æ²ˆé‚µåŠ¼ç»„ | [ä»£ç å¼€æº](https://github.com/HKUST-Aerial-Robotics/ESVO)ï¼ˆæš‚æœªå…¬å¼€ï¼‰| [video](https://www.youtube.com/watch?v=3CPPs1gz04k&feature=youtu.be)
+ [ ] **[4]** Yue Y, Zhao C, Wu Z, et al. [**Collaborative Semantic Understanding and Mapping Framework for Autonomous Systems**](https://www.researchgate.net/profile/Yufeng_Yue/publication/343519028_Collaborative_Semantic_Understanding_and_Mapping_Framework_for_Autonomous_Systems/links/5f329ae4458515b72915ab09/Collaborative-Semantic-Understanding-and-Mapping-Framework-for-Autonomous-Systems.pdf)[J]. IEEE/ASME Transactions on Mechatronics, **2020**.
    + <font color = blue>è‡ªæ²»ç³»ç»Ÿåä½œå¼è¯­ä¹‰ç†è§£å’Œå»ºå›¾æ¡†æ¶</font>
    + å—æ´‹ç†å·¥å¤§å­¦ | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 5.6
+ [ ] **[5]** Do H, Hong S, Kim J. [**Robust Loop Closure Method for Multi-Robot Map Fusion by Integration of Consistency and Data Similarity**](https://ieeexplore.ieee.org/abstract/document/9145697)[J]. IEEE Robotics and Automation Letters, **2020**, 5(4): 5701-5708.
    + <font color = blue>èåˆä¸€è‡´æ€§å’Œæ•°æ®ç›¸ä¼¼æ€§çš„å¤šæœºå™¨äººåœ°å›¾èåˆçš„é—­ç¯æ–¹æ³•</font>
    + éŸ©å›½å¯æ˜å¤§å­¦ | [Google Scholar](https://scholar.google.com/citations?user=rGX5_gEAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[6]** Zhan Z, Jian W, Li Y, et al. [**A SLAM Map Restoration Algorithm Based on Submaps and an Undirected Connected Graph**](https://arxiv.org/abs/2007.14592)[J]. arXiv preprint arXiv:2007.14592, **2020**.
    + <font color = blue>åŸºäºå­å›¾å’Œæ— å‘è¿é€šå›¾çš„ SLAM åœ°å›¾æ¢å¤ç®—æ³•</font>
    + æ­¦æ±‰å¤§å­¦
+ [ ] **[7]** Chen H, Zhang G, Ye Y. [**Semantic Loop Closure Detection with Instance-Level Inconsistency Removal in Dynamic Industrial Scenes**](https://ieeexplore.ieee.org/abstract/document/9145862)[J]. IEEE Transactions on Industrial Informatics, **2020**.
    + <font color = blue>åŠ¨æ€å·¥ä¸šåœºæ™¯ä¸­å…·æœ‰å®ä¾‹çº§ä¸ä¸€è‡´æ¶ˆé™¤åŠŸèƒ½çš„è¯­ä¹‰é—­ç¯æ£€æµ‹</font>
    + å¦é—¨å¤§å­¦ | æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 9.1 
+ [ ] **[8]** Li Y, Brasch N, Wang Y, et al. [**Structure-SLAM: Low-Drift Monocular SLAM in Indoor Environments**](https://ieeexplore.ieee.org/abstract/document/9165014)[J]. IEEE Robotics and Automation Letters, **2020**, 5(4): 6583-6590.
    + <font color = blue>Structure-SLAMï¼šå®¤å†…ç¯å¢ƒä¸­çš„ä½æ¼‚ç§»å•ç›® SLAM</font>
    + TUM | [ä»£ç å¼€æº](https://github.com/yanyan-li/Structure-SLAM-PointLine)
+ [ ] **[9]** Liu J, Meng Z. [**Visual SLAM with Drift-Free Rotation Estimation in Manhattan World**](https://ieeexplore.ieee.org/abstract/document/9161354/)[J]. IEEE Robotics and Automation Letters, **2020**.
    + <font color = blue>äºšç‰¹å…°å¤§ä¸–ç•Œçš„å…¨å±€æœ€ä¼˜å’Œæœ‰æ•ˆæ¶ˆå¤±ç‚¹ä¼°è®¡</font>
    + æ¸¯ä¸­æ–‡åˆ˜äº‘è¾‰æ•™æˆç»„ [Haoang Li](https://scholar.google.com/citations?user=KnnPc0YAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[10]** Hou J, Yu L, Fei S. [**A highly robust automatic 3D reconstruction system based on integrated optimization by point line features**](https://www.sciencedirect.com/science/article/pii/S0952197620302256)[J]. Engineering Applications of Artificial Intelligence, **2020**, 95: 103879.
    + <font color = blue>åŸºäºç‚¹çº¿è”åˆä¼˜åŒ–çš„è‡ªåŠ¨ä¸‰ç»´é‡å»º</font>
    + è‹å·å¤§å­¦ã€ä¸œå—å¤§å­¦ | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 4.2
+ [ ] **[11]** Li H, Kim P, Zhao J, et al. [**Globally Optimal and Efficient Vanishing Point Estimation in Atlanta World**](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123670154.pdf)[J]. **2020**.
    + <font color = blue>æ›¼å“ˆé¡¿ä¸–ç•Œä¸­æ— æ¼‚ç§»æ—‹è½¬ä¼°è®¡çš„è§†è§‰ SLAM</font>
    + æ¸¯ä¸­æ–‡ï¼Œè¥¿è’™å¼—é›·æ³½å¤§å­¦
+ [ ] **[12]** Wang X, Christie M, Marchand E. [**Relative Pose Estimation and Planar Reconstruction via Superpixel-Driven Multiple Homographies**](https://hal.inria.fr/hal-02915045/document)[C]//IEEE/RSJ Int. Conf. on Intelligent Robots and Systems, **IROS'20**. 2020.
    + <font color = blue>é€šè¿‡è¶…åƒç´ é©±åŠ¨çš„å¤šä¸ªå•åº”æ€§çš„ç›¸å¯¹å§¿åŠ¿ä¼°è®¡å’Œå¹³é¢é‡å»º</font>
    + é›·æ©å¤§å­¦
+ [ ] **[13]** ZuÃ±iga-NoÃ«l D, Jaenal A, Gomez-Ojeda R, et al. [**The UMA-VI dataset: Visualâ€“inertial odometry in low-textured and dynamic illumination environments**](https://journals.sagepub.com/doi/abs/10.1177/0278364920938439)[J]. The International Journal of Robotics Research, **2020**: 0278364920938439.
    + <font color = blue>UMA-VI æ•°æ®é›†ï¼šåœ¨ä½çº¹ç†å’ŒåŠ¨æ€ç…§æ˜ç¯å¢ƒä¸­çš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡</font>
    + é©¬æ‹‰åŠ å¤§å­¦ | PL-SLAM çš„[ä½œè€…](http://mapir.uma.es/mapirwebsite/index.php/people/270) | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 4.0 | [æ•°æ®é›†åœ°å€](http://mapir.isa.uma.es/mapirwebsite/index.php/mapir-downloads/291-uma-visual-inertial-dataset.html)

---

#### 2. Sensor Fusion

+ [x] **[14]** Zuo X, Yang Y, Geneva P, et al. [**LIC-Fusion 2.0: LiDAR-Inertial-Camera Odometry with Sliding-Window Plane-Feature Tracking**](https://arxiv.org/abs/2008.07196)[J]. arXiv preprint arXiv:2008.07196, **2020**.
    + <font color = blue>**LIC-Fusion 2.0ï¼šæ»‘åŠ¨çª—å£å¹³é¢ç‰¹å¾è·Ÿè¸ªçš„ LiDAR-æƒ¯æ€§-è§†è§‰é‡Œç¨‹è®¡**</font>
    + æµ™å¤§ã€ETH [Google Scholar](https://scholar.google.com/citations?user=CePv8agAAAAJ&hl=zh-CN&oi=sra)
    + ç›¸å…³è®ºæ–‡ï¼š[LIC-Fusion: LiDAR-Inertial-Camera Odometry](https://arxiv.org/abs/1909.04102)
+ [ ] **[15]** Alliez P, Bonardi F, Bouchafa S, et al. [**Real-Time Multi-SLAM System for Agent Localization and 3D Mapping in Dynamic Scenarios**](https://hal.inria.fr/hal-02913098/)[C]//International Confererence on Intelligent Robots and Systems (IROS 2020). **2020**.
    + <font color = blue>å®æ—¶ Multi-SLAM ç³»ç»Ÿï¼Œç”¨äºåŠ¨æ€åœºæ™¯ä¸­æ™ºèƒ½ä½“å®šä½å’Œ 3D å»ºå›¾</font>
    + æ³•å›½å›½å®¶ä¿¡æ¯ä¸è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€
+ [ ] **[16]** Shao X, Zhang L, Zhang T, et al. [**A Tightly-coupled Semantic SLAM System with Visual, Inertial and Surround-view Sensors for Autonomous Indoor Parking**](https://sse.tongji.edu.cn/linzhang/files/VISSLAM.pdf)[J].**2020**.
    + <font color = blue>å…·æœ‰è§†è§‰ã€æƒ¯æ€§å’Œå…¨æ™¯ä¼ æ„Ÿå™¨çš„ç´§å¯†è€¦åˆè¯­ä¹‰ SLAM ç³»ç»Ÿï¼Œç”¨äºè‡ªä¸»å®¤å†…åœè½¦</font>
    + åŒæµå¤§å­¦
+ [x] **[17]** Shan M, Feng Q, Atanasov N. [**OrcVIO: Object residual constrained Visual-Inertial Odometry**](https://arxiv.org/abs/2007.15107)[J]. arXiv preprint arXiv:2007.15107, 2020.(**IROS 2020**)
    + <font color = blue>**ç‰©ä½“æ®‹å·®çº¦æŸçš„ VIO**</font>
    + åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡ | [Nikolay A. Atanasov](https://scholar.google.com/citations?user=RTkSatQAAAAJ&hl=zh-CN&oi=sra) | [Mo Shan](http://me-llamo-sean.cf/)
+ [ ] **[18]** Seok H, Lim J. [**ROVINS: Robust Omnidirectional Visual Inertial Navigation System**](https://ieeexplore.ieee.org/abstract/document/9144432/)[J]. IEEE Robotics and Automation Letters, **2020**.
    + <font color = blue>é²æ£’çš„å…¨æ–¹ä½è§†è§‰æƒ¯æ€§å¯¼èˆªç³»ç»Ÿ</font>
    + éŸ©å›½æ±‰é˜³å¤§å­¦
+ [ ] **[19]** Liu W, Caruso D, Ilg E, et al. [**TLIO: Tight Learned Inertial Odometry**](https://ieeexplore.ieee.org/abstract/document/9134860)[J]. IEEE Robotics and Automation Letters, **2020**, 5(4): 5653-5660.
    + <font color = blue>åŸºäºå­¦ä¹ çš„ç´§è€¦åˆçš„æƒ¯æ€§é‡Œç¨‹è®¡</font>
    + å®¾å¤•æ³•å°¼äºšå¤§å­¦ | [ä»£ç å¼€æº](https://github.com/CathIAS/TLIO) | [é¡¹ç›®ä¸»é¡µ](https://cathias.github.io/TLIO/)
+ [ ] **[20]** Sartipi K, Do T, Ke T, et al. [**Deep Depth Estimation from Visual-Inertial SLAM**](https://arxiv.org/abs/2008.00092)[J]. arXiv preprint arXiv:2008.00092, **2020**.
    + <font color = blue>è§†è§‰æƒ¯æ€§ SLAM æ·±åº¦ä¼°è®¡</font>
    + æ˜å°¼è‹è¾¾å¤§å­¦ | [ä»£ç å¼€æº](https://github.com/MARSLab-UMN/vi_depth_completion)

---

#### 3. Semantic/Deep SLAM

+ [x] **[21]** Gomez C, Hernandez A C, Derner E, et al. [**Object-Based Pose Graph for Dynamic Indoor Environments**](https://ieeexplore.ieee.org/abstract/document/9134440)[J]. IEEE Robotics and Automation Letters, **2020**, 5(4): 5401-5408.
    + <font color = blue>**åŠ¨æ€å®¤å†…ç¯å¢ƒä¸­åŸºäºç‰©ä½“çš„ä½å§¿å›¾**</font>
    + é©¬å¾·é‡Œå¡æ´›æ–¯ä¸‰ä¸–å¤§å­¦ | [Google Scholar](https://scholar.google.com/citations?user=2HNQTKwAAAAJ&hl=zh-CN&oi=sra)ï¼ˆæœ‰å¾ˆå¤šåŸºäºç‰©ä½“çš„æœºå™¨äººå¯¼èˆªçš„å·¥ä½œï¼‰
+ [ ] **[22]** Wang H, Wang C, Xie L. [**Online Visual Place Recognition via Saliency Re-identification**](https://arxiv.org/abs/2007.14549)[J]. arXiv preprint arXiv:2007.14549, 2020.ï¼ˆ**IROS 2020**ï¼‰
    + <font color = blue>é€šè¿‡æ˜¾è‘—æ€§é‡è¯†åˆ«è¿›è¡Œè§†è§‰åœºæ™¯é‡è¯†åˆ«</font>
    + CMU | [ä»£ç å¼€æº](https://github.com/wh200720041/SRLCD)
+ [ ] **[23]** Jau Y Y, Zhu R, Su H, et al. [**Deep Keypoint-Based Camera Pose Estimation with Geometric Constraints**](https://arxiv.org/abs/2007.15122)[J]. arXiv preprint arXiv:2007.15122, 2020.ï¼ˆ**IROS 2020**ï¼‰
    + <font color = blue>å…·æœ‰å‡ ä½•çº¦æŸçš„åŸºäºæ·±åº¦å…³é”®ç‚¹çš„ç›¸æœºå§¿åŠ¿ä¼°è®¡</font>
    + åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡ | [ä»£ç å¼€æº](https://github.com/eric-yyjau/pytorch-deepFEPE)
+ [ ] **[24]** Gong X, Liu Y, Wu Q, et al. [**An Accurate, Robust Visual Odometry and Detail-preserving Reconstruction System**](http://www.3dgp.net/paper/2020/An%20Accurate,%20Robust%20Visual%20Odometry%20and%20Detail-preserving%20Reconstruction%20System.pdf)[J]. **2020**.
    + <font color = blue>å‡†ç¡®ã€é²æ£’çš„è§†è§‰é‡Œç¨‹è®¡å’Œä¿ç•™ç»†èŠ‚çš„é‡å»ºç³»ç»Ÿ</font>
    + å—äº¬èˆªç©ºèˆªå¤©å¤§å­¦ | [ä»£ç å¼€æº](https://github.com/NUAA-XiaoxiG/EDI-VO)ï¼ˆæš‚æœªå…¬å¼€ï¼‰
+ [ ] **[25]** Wei P, Hua G, Huang W, et al. [**Unsupervised Monocular Visual-inertial Odometry Network**](https://www.ijcai.org/Proceedings/2020/0325.pdf)[J].IJCAI 2020
    + <font color = blue>æ— ç›‘ç£å•ç›®è§†æƒ¯é‡Œç¨‹è®¡ç½‘ç»œ</font>
    + åŒ—å¤§ | [ä»£ç å¼€æº](https://github.com/Ironbrotherstyle/UnVIO) ï¼ˆæš‚æœªå…¬å¼€ï¼‰     
+ [ ] **[26]** Li D, Shi X, Long Q, et al. [**DXSLAM: A Robust and Efficient Visual SLAM System with Deep Features**](https://arxiv.org/abs/2008.05416)[J]. arXiv preprint arXiv:2008.05416, 2020.ï¼ˆ**IROS 2020**ï¼‰
    + <font color = blue>åŸºäºæ·±åº¦ä¿¡æ¯çš„é²æ£’é«˜æ•ˆè§†è§‰ SLAM ç³»ç»Ÿ</font>
    + æ¸…åå¤§å­¦ | [ä»£ç å¼€æº](https://github.com/ivipsourcecode/dxslam)

---

#### 4. AR/VR/MR

+ [x] **[27]** Tahara T, Seno T, Narita G, et al. [**Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based on 3D Scene Graph**](https://arxiv.org/abs/2008.07817)[J]. arXiv preprint arXiv:2008.07817, **2020**.
    + <font color = blue>**å¯é‡å®šä½çš„ ARï¼šåŸºäºå®¤å†… 3D åœºæ™¯å›¾ä¸­çš„æƒ…æ™¯æ„ŸçŸ¥å¢å¼ºç°å®**</font>
    + ç´¢å°¼
+ [ ] **[28]** Li X, Tian Y, Zhang F, et al. [**Object Detection in the Context of Mobile Augmented Reality**](https://arxiv.org/abs/2008.06655)[J]. arXiv preprint arXiv:2008.06655, 2020.ï¼ˆ**ISMAR 2020**ï¼‰
    + <font color = blue>ç§»åŠ¨å¢å¼ºç°å®ç¯å¢ƒä¸‹çš„ç›®æ ‡æ£€æµ‹</font>
    + oppo
+ [x] **[29]** Liu C, Shen S. [**An Augmented Reality Interaction Interface for Autonomous Drone**](https://arxiv.org/abs/2008.02234)[J]. arXiv preprint arXiv:2008.02234, 2020.ï¼ˆ**IROS 2020**ï¼‰
    + <font color = blue>**è‡ªä¸»æ— äººæœºå¢å¼ºç°å®äº¤äº’ç•Œé¢**</font>
    + æ¸¯ç§‘æ²ˆé‚µåŠ¼ç»„
+ [ ] **[30]** Du R, Turner E L, Dzitsiuk M, et al. [DepthLab: Real-time 3D Interaction with Depth Maps for Mobile Augmented Reality](https://research.google/pubs/pub49275/)[J]. 2020.
    + <font color = blue>ä½¿ç”¨æ·±åº¦åœ°å›¾å®æ—¶ 3D äº¤äº’çš„ç§»åŠ¨å¢å¼ºç°å®</font>
    + Google

---

### 2020 å¹´ 7 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰

> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 7 æœˆ 27 æ—¥       
å…± 20 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 8 é¡¹ï¼ˆå¾…ï¼‰å¼€æºå·¥ä½œ       
æœ¬æœˆæœˆåˆ ECCVï¼ŒIROS æ”¾æ¦œï¼Œä¸å°‘æ–°è®ºæ–‡å‡ºç°        
2 éšç§ä¿æŠ¤çš„è§†è§‰ SLAMï¼Œ11 ç§¦é€šå¤§ä½¬çš„ AVP-SLAM       
æœˆåº• ORB-SLAM3 åˆåˆ¶é€ äº†å¤§æ–°é—»ï¼Œè°·æ­Œå­¦æœ¯éƒ½æ²¡æ¥å¾—åŠæ”¶å½•ï¼Œå›½å†…å…¬ä¼—å·éƒ½å‡ºè§£æäº†**

#### 1. Geometric SLAM

+ [x] **[1]** Carlos Campos, Richard Elvira, et al.[**ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual-Inertial and Multi-Map SLAM**](https://arxiv.org/abs/2007.11898)[J]. arXiv preprint arXiv:2007.11898, **2020**.
    + <font color = blue>**ORB-SLAM3: é›†è§†è§‰ã€è§†æƒ¯ã€å¤šåœ°å›¾ SLAM äºä¸€ä½“**</font>
    + è¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ | [**ä»£ç å¼€æº**](https://github.com/UZ-SLAMLab/ORB_SLAM3) | [Video](https://www.youtube.com/channel/UCXVt-kXG6T95Z4tVaYlU80Q)
+ [x] **[2]** Shibuya M, Sumikura S, Sakurada K. [**Privacy Preserving Visual SLAM**](https://arxiv.org/abs/2007.10361)[J]. arXiv preprint arXiv:2007.10361, 2020.(**ECCV 2020**)
    + <font color = blue>**ä¿æŠ¤éšç§çš„è§†è§‰ SLAM**</font>
    + æ—¥æœ¬å›½ç«‹å…ˆè¿›å·¥ä¸šç§‘å­¦æŠ€æœ¯ç ”ç©¶é™¢ï¼›ä¸œäº¬å·¥ä¸šå¤§å­¦
    + ä½œè€… Shinya Sumikura [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=BNC4JasAAAAJ&hl=zh-CN&oi=sra)ï¼Œ**OpenVSLAM** çš„ä½œè€… | [é¡¹ç›®ä¸»é¡µ](https://xdspacelab.github.io/lcvslam/) | [Video](https://www.youtube.com/watch?v=gEtUqnHx83w&feature=youtu.be)
    + ç›¸å…³è®ºæ–‡ï¼šCVPR 2019 [**Privacy preserving image-based localization**](http://openaccess.thecvf.com/content_CVPR_2019/html/Speciale_Privacy_Preserving_Image-Based_Localization_CVPR_2019_paper.html)
+ [x] **[3]** Tompkins A, Senanayake R, Ramos F. [**Online Domain Adaptation for Occupancy Mapping**](https://arxiv.org/abs/2007.00164)[J]. arXiv preprint arXiv:2007.00164, 2020.ï¼ˆ**RSS 2020**ï¼‰
    + <font color = blue>**åœ¨çº¿åŒºåŸŸè°ƒæ•´ä»¥è¿›è¡Œå æœ‰åœ°å›¾æ„å»º**</font>
    + æ‚‰å°¼å¤§å­¦ | [**ä»£ç å¼€æº**](https://github.com/MushroomHunting/RSS2020-online-domain-adaptation-pot) | [Video](https://www.youtube.com/watch?v=qLv0mM9Le8E&feature=youtu.be)
+ [x] **[4]** Li Y, Zhang T, Nakamura Y, et al. [**SplitFusion: Simultaneous Tracking and Mapping for Non-Rigid Scenes**](https://arxiv.org/abs/2007.02108)[J]. arXiv preprint arXiv:2007.02108, 2020.(**IROS 2020**)
    + <font color = blue>**SplitFusionï¼šéåˆšæ€§åœºæ™¯çš„ SLAM**</font>
    + ä¸œäº¬å¤§å­¦
+ [ ] **[5]** WeiChen Dai, Yu Zhang, Ping Li, and Zheng Fang, Sebastian Scherer. [**RGB-D SLAM in Dynamic Environments Using Points Correlations**](https://arxiv.org/abs/1811.03217). IEEE Transactions on Pattern Analysis and Machine Intelligence (**TPAMI**), **2020**.
    + <font color = blue>**åŠ¨æ€ç¯å¢ƒä¸­ä½¿ç”¨ç‚¹å…³è”çš„ RGB-D SLAM**</font>
    + æµ™æ±Ÿå¤§å­¦ï¼›æœŸåˆŠï¼šPAMI ä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 17.86
+ [x] **[6]** Huang H, Ye H, Sun Y, et al. [**GMMLoc: Structure Consistent Visual Localization with Gaussian Mixture Models**](https://ieeexplore.ieee.org/abstract/document/9126150)[J]. IEEE Robotics and Automation Letters, **2020**.
    + <font color = blue>**é«˜æ–¯æ··åˆæ¨¡å‹çš„ç»“æ„ä¸€è‡´è§†è§‰å®šä½**</font>
    + é¦™æ¸¯ç§‘æŠ€å¤§å­¦ | [ä»£ç å¼€æº](https://github.com/HyHuang1995/gmmloc)

---

#### 2. Sensor Fusion

+ [ ] **[7]** Zuo X, Ye W, Yang Y, et al. [**Multimodal localization: Stereo over LiDAR map**](https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21936)[J]. Journal of Field Robotics, **2020**.
    + <font color = blue>å¤šæ¨¡å¼å®šä½ï¼šåœ¨ LiDAR å…ˆéªŒåœ°å›¾ä¸­ä½¿ç”¨åŒç›®ç›¸æœºå®šä½</font>
    + æµ™æ±Ÿå¤§å­¦ã€ç‰¹æ‹‰åå¤§å­¦ | ä½œè€…[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=CePv8agAAAAJ&hl=zh-CN&oi=sra) | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q2ï¼ŒIF 4.19
+ [ ] **[8]** Shan T, Englot B, Meyers D, et al. [**LIO-SAM: Tightly-coupled Lidar Inertial Odometry via Smoothing and Mapping**](https://arxiv.org/abs/2007.00258)[J]. arXiv preprint arXiv:2007.00258, 2020.ï¼ˆ**IROS 2020**ï¼‰
    + <font color = blue>å¹³æ»‘ç´§è€¦åˆçš„æ¿€å…‰é›·è¾¾é‡Œç¨‹è®¡å’Œå»ºå›¾</font>
    + MIT | [**ä»£ç å¼€æº**](https://github.com/TixiaoShan/LIO-SAM) | [Video](https://www.youtube.com/watch?v=A0H8CoORZJU&feature=youtu.be)
+ [ ] **[9]** Rozenberszki D, Majdik A. [**LOL: Lidar-Only Odometry and Localization in 3D Point Cloud Maps**](https://arxiv.org/abs/2007.01595)[J]. arXiv preprint arXiv:2007.01595, 2020.ï¼ˆ**ICRA 2020**ï¼‰
    + <font color = blue>3D ç‚¹äº‘åœ°å›¾ä¸­ä»…æ¿€å…‰é›·è¾¾çš„é‡Œç¨‹è®¡å’Œå®šä½</font>
    + åŒˆç‰™åˆ©ç§‘å­¦é™¢æœºå™¨æ„ŸçŸ¥å®éªŒå®¤ | [**ä»£ç å¼€æº**](https://github.com/RozDavid/LOL) | [Video](https://www.youtube.com/watch?v=ektGb5SQGRM&feature=youtu.be)
+ [ ] **[10]** You R, Hou H. [**Real-Time Pose Estimation by Fusing Visual and Inertial Sensors for Autonomous Driving**]()[J]. **2020**.
    + <font color = blue>é€šè¿‡èåˆè§†è§‰å’Œæƒ¯æ€§ä¼ æ„Ÿå™¨è¿›è¡Œè‡ªåŠ¨é©¾é©¶çš„å®æ—¶ä½å§¿ä¼°è®¡</font>
    + ç‘å…¸æŸ¥å°”é»˜æ–¯ç†å·¥å¤§å­¦ ç¡•å£«å­¦ä½è®ºæ–‡ | [ä»£ç å¼€æº](https://github.com/chalmersfsd/cfsd-perception-slam)

---

#### 3. Semantic/Deep SLAM

+ [x] **[11]** Qin T, Chen T, Chen Y, et al. [**AVP-SLAM: Semantic Visual Mapping and Localization for Autonomous Vehicles in the Parking Lot**](https://arxiv.org/abs/2007.01813)[J]. arXiv preprint arXiv:2007.01813, **2020**.(**IROS 2020**)
    + <font color = blue>**AVP-SLAMï¼šåœè½¦åœºä¸­è‡ªåŠ¨é©¾é©¶è½¦è¾†çš„è¯­ä¹‰ SLAM**</font>
    + åä¸ºç§¦é€š | [çŸ¥ä¹æ–‡ç« ](https://zhuanlan.zhihu.com/p/157340737)
+ [x] **[12]** Gomez C, Silva A C H, Derner E, et al. [**Object-Based Pose Graph for Dynamic Indoor Environments**](https://ieeexplore.ieee.org/abstract/document/9134440)[J]. IEEE Robotics and Automation Letters, **2020**.
    + <font color = blue>**åŠ¨æ€å®¤å†…ç¯å¢ƒä¸­åŸºäºç‰©ä½“çš„ä½å§¿å›¾**</font>
    + è¥¿ç­ç‰™é©¬å¾·é‡Œå¡æ´›æ–¯ä¸‰ä¸–å¤§å­¦
+ [ ] **[13]** Costante G, Mancini M. [**Uncertainty Estimation for Data-Driven Visual Odometry**](https://ieeexplore.ieee.org/abstract/document/9126117/)[J]. IEEE Transactions on Robotics, **2020**.
    + <font color = blue>æ•°æ®é©±åŠ¨çš„è§†è§‰æµ‹ç¨‹çš„ä¸ç¡®å®šåº¦ä¼°è®¡</font>
    + æ„å¤§åˆ©ä½©é²è´¾å¤§å­¦ | [**ä»£ç å¼€æº**](https://github.com/isarlab-department-engineering/UA-VO)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰ | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 7.0
+ [ ] **[14]** Min Z, Yang Y, Dunn E. [**VOLDOR: Visual Odometry From Log-Logistic Dense Optical Flow Residuals**](http://openaccess.thecvf.com/content_CVPR_2020/html/Min_VOLDOR_Visual_Odometry_From_Log-Logistic_Dense_Optical_Flow_Residuals_CVPR_2020_paper.html)[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. **CVPR 2020**: 4898-4909.
    + <font color = blue>å¯¹æ•°é€»è¾‘å¯†é›†å…‰æµæ®‹å·®çš„è§†è§‰é‡Œç¨‹è®¡</font>
    + å²è’‚æ–‡æ–¯æŠ€æœ¯å­¦é™¢ | [**ä»£ç å¼€æº**](https://github.com/htkseason/VOLDOR)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [ ] **[15]** Zou Y, Ji P, Tran Q H, et al. [**Learning Monocular Visual Odometry via Self-Supervised Long-Term Modeling**](https://arxiv.org/abs/2007.10983)[J]. arXiv preprint arXiv:2007.10983, 2020.ï¼ˆ**ECCV 2020**ï¼‰
    + <font color = blue>è‡ªç›‘ç£é•¿æœŸå»ºæ¨¡å­¦ä¹ å•ç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + å¼—å‰å°¼äºšç†å·¥å¤§å­¦ | [é¡¹ç›®ä¸»é¡µ](https://yuliang.vision/LTMVO/) | ä»£ç  Coming soon
+ [ ] **[16]** Wei P, Hua G, Huang W, et al. Unsupervised Monocular Visual-inertial Odometry Network[J].2020
    + <font color = blue>æ— ç›‘ç£å•ç›®è§†æƒ¯é‡Œç¨‹è®¡</font>
    + åŒ—äº¬å¤§å­¦ | [**ä»£ç å¼€æº**](https://github.com/Ironbrotherstyle/UnVIO)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰

---

#### 4. AR/VR/MR

+ [ ] **[17]** Chen Y, Zhang B, Zhou J, et al. [**Real-time 3D unstructured environment reconstruction utilizing VR and Kinect-based immersive teleoperation for agricultural field robots**](https://www.sciencedirect.com/science/article/pii/S0168169920311479)[J]. Computers and Electronics in Agriculture, **2020**, 175: 105579.
    + <font color = blue>åˆ©ç”¨ VR å’ŒåŸºäº Kinect çš„æ²‰æµ¸å¼è¿œç¨‹æ“ä½œæŠ€æœ¯å¯¹å†œä¸šæœºå™¨äººè¿›è¡Œå®æ—¶ 3D éç»“æ„åŒ–ç¯å¢ƒé‡æ„</font>
    + å—äº¬å†œä¸šå¤§å­¦ | æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 4.0
+ [ ] **[18]** Choi J, Son M G, Lee Y Y, et al. [**Position-based augmented reality platform for aiding construction and inspection of offshore plants**](https://link.springer.com/article/10.1007/s00371-020-01902-9)[J]. The Visual Computer, **2020**: 1-11.
    + <font color = blue>åŸºäºä½ç½®çš„å¢å¼ºç°å®å¹³å°ï¼Œç”¨äºè¾…åŠ©æµ·ä¸Šå·¥å‚çš„å»ºè®¾å’Œæ£€æŸ¥</font>
    + éŸ©å›½å…‰å·ç§‘å­¦æŠ€æœ¯é™¢ | æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼ŒJCR Q3

---

#### 5. Others

+ [ ] **[19]** Brazil G, Pons-Moll G, Liu X, et al. [**Kinematic 3D Object Detection in Monocular Video**](https://arxiv.org/abs/2007.09548)[J]. arXiv preprint arXiv:2007.09548, 2020.(**ECCV 2020**)
    + <font color = blue>**å•ç›®è§†é¢‘ä¸­è¿åŠ¨ 3D ç›®æ ‡æ£€æµ‹**</font>
    + å¯†æ­‡æ ¹å·ç«‹å¤§å­¦ | [å®éªŒå®¤ä¸»é¡µ](http://cvlab.cse.msu.edu/) | [é¡¹ç›®ä¸»é¡µ](http://cvlab.cse.msu.edu/project-kinematic.html)ï¼ˆè®ºæ–‡ä¸­è¯´ä¼šå¼€æºï¼‰
+ [x] **[20]** Yu Z, Jin L, Gao S. [**P2Net: Patch-match and Plane-regularization for Unsupervised Indoor Depth Estimation**](https://arxiv.org/abs/2007.07696)[J]. arXiv preprint arXiv:2007.07696, 2020.ï¼ˆ**ECCV 2020**ï¼‰
    + <font color = blue>**æ— ç›‘ç£å®¤å†…æ·±åº¦ä¼°è®¡çš„å—åŒ¹é…å’Œå¹³é¢æ­£åˆ™åŒ–**</font>
    + ä¸Šæµ·ç§‘æŠ€å¤§å­¦ | [**ä»£ç å¼€æº**](https://github.com/svip-lab/Indoor-SfMLearner)

---

### 2020 å¹´ 6 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰

> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 6 æœˆ 27 æ—¥       
å…± 20 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 3 é¡¹å¼€æºå·¥ä½œ       
4,5,6,12,13 çº¿ã€è¾¹ã€å¹³é¢ã€ç‰©ä½“å¤šè·¯æ ‡ SLAM       
2,3 å¤šæœºå™¨äºº SLAM       
7,16 æ‹“æ‰‘ç›¸å…³       
11ï¼šæ·±åº¦å­¦ä¹ ç”¨äºå®šä½å’Œå»ºå›¾çš„è°ƒç ”**

#### 1. Geometric SLAM

+ [ ] **[1]** Zhang T, Zhang H, Li Y, et al. [**FlowFusion: Dynamic Dense RGB-D SLAM Based on Optical Flow**](https://arxiv.org/abs/2003.05102)[C]. **ICRA 2020**.
    + <font color = blue>FlowFusionï¼šåŸºäºå…‰æµçš„åŠ¨æ€ç¨ å¯† RGB-D SLAM</font>
    + ä¸œäº¬å¤§å­¦ï¼›[ä½œè€…è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=pKo_rrMAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[2]** Lajoie P Y, Ramtoula B, Chang Y, et al. [**DOOR-SLAM: Distributed, online, and outlier resilient SLAM for robotic teams**](https://ieeexplore.ieee.org/abstract/document/8962967/)[J]. IEEE Robotics and Automation Letters, **2020**, 5(2): 1656-1663.
    + <font color = blue>**é€‚ç”¨äºå¤šæœºå™¨äººçš„åˆ†å¸ƒå¼ï¼Œåœ¨çº¿å’Œå¼‚å¸¸çµæ´»çš„ SLAM**</font>
    + åŠ æ‹¿å¤§è’™ç‰¹åˆ©å°”ç†å·¥å­¦é™¢ï¼›[**ä»£ç å¼€æº**](https://github.com/MISTLab/DOOR-SLAM)
+ [ ] **[3]** Chakraborty K, Deegan M, Kulkarni P, et al. [**JORB-SLAM: A Jointly optimized Multi-Robot Visual SLAM**](https://um-mobrob-t12-w19.github.io/docs/report.pdf)[J].
    + <font color = blue>å¤šæœºå™¨äºº SLAM è”åˆä¼˜åŒ–</font>
    + å¯†æ­‡æ ¹å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€
+ [x] **[4]** Zhang H, Ye C. [**Plane-Aided Visual-Inertial Odometry for 6-DOF Pose Estimation of a Robotic Navigation Aid**](https://ieeexplore.ieee.org/abstract/document/9091872/)[J]. IEEE Access, **2020**, 8: 90042-90051.
    + <font color = blue>ç”¨äºæœºå™¨äººå¯¼èˆª 6 è‡ªç”±åº¦ä½å§¿ä¼°è®¡çš„å¹³é¢è¾…åŠ© VIO</font>
    + å¼—å‰å°¼äºšè”é‚¦å¤§å­¦ï¼›å¼€æºæœŸåˆŠï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=u7uY5DMAAAAJ&hl=zh-CN&oi=sra)
+ [x] **[5]** Ali A J B, Hashemifar Z S, Dantu K. [**Edge-SLAM: edge-assisted visual simultaneous localization and mapping**](https://dl.acm.org/doi/pdf/10.1145/3386901.3389033)[C]//Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services. **2020**: 325-337.
    + <font color = blue>**Edge-SLAM: è¾¹è¾…åŠ©çš„è§†è§‰ SLAM**</font>
    + å¸ƒæ³•ç½—å¤§å­¦
+ [ ] **[6]** Mateus A, Ramalingam S, Miraldo P. [**Minimal Solvers for 3D Scan Alignment With Pairs of Intersecting Lines**](http://openaccess.thecvf.com/content_CVPR_2020/papers/Mateus_Minimal_Solvers_for_3D_Scan_Alignment_With_Pairs_of_Intersecting_CVPR_2020_paper.pdf)[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. **2020**: 7234-7244.
    + <font color = blue>æˆå¯¹çš„ç›¸äº¤çº¿ 3D æ‰«æå¯¹é½çš„æœ€å°‘æ±‚è§£å™¨</font>
    + è‘¡è„ç‰™é‡Œæ–¯æœ¬å¤§å­¦ï¼Œè°·æ­Œ
+ [ ] **[7]** Xue W, Ying R, Gong Z, et al. [**SLAM Based Topological Mapping and Navigation**](https://ieeexplore.ieee.org/abstract/document/9110190/)[C]//2020 IEEE/ION Position, Location and Navigation Symposium (PLANS). IEEE, **2020**: 1336-1341.
    + <font color = blue>åŸºäº SLAM çš„æ‹“æ‰‘å»ºå›¾ä¸å¯¼èˆª</font>
    + ä¸Šäº¤
    
---

#### 2. Sensor Fusion

+ [ ] **[8]** Lee W, Eckenhoff K, Geneva P, et al. [**Intermittent GPS-aided VIO: Online Initialization and Calibration**](https://www.researchgate.net/profile/Patrick_Geneva/publication/341713067_Intermittent_GPS-aided_VIO_Online_Initialization_and_Calibration/links/5ed0063b299bf1c67d26cf5d/Intermittent-GPS-aided-VIO-Online-Initialization-and-Calibration.pdf)[J].**2020**
    + <font color = blue>é—´æ­‡æ€§ GPS è¾…åŠ© VIOï¼šåœ¨çº¿åˆå§‹åŒ–å’Œæ ¡å‡†</font>
    + ç‰¹æ‹‰åå¤§å­¦ï¼Œé»„å›½æƒæ•™æˆ
+ [ ] **[9]** Alliez P, Bonardi F, Bouchafa S, et al. [**Indoor Localization and Mapping: Towards Tracking Resilience Through a Multi-SLAM Approach**](https://hal.inria.fr/hal-02611679/document)[C]//28th Mediterranean Conference on Control and Automation (MED 2020). **2020**.
    + <font color = blue>å®¤å†…å®šä½å’Œåˆ¶å›¾ï¼šé€šè¿‡å¤šä¼ æ„Ÿå™¨ SLAM æ–¹æ³•å®ç°å¼¹æ€§è·Ÿè¸ª</font>
+ [ ] **[10]** Nam D V, Gon-Woo K. [**Robust Stereo Visual Inertial Navigation System Based on Multi-Stage Outlier Removal in Dynamic Environments**](https://www.mdpi.com/1424-8220/20/10/2922)[J]. Sensors, **2020**, 20(10): 2922.
    + <font color = blue>åŠ¨æ€ç¯å¢ƒä¸­åŸºäºå¤šé˜¶æ®µç¦»ç¾¤å€¼å‰”é™¤çš„é²æ£’åŒç›®è§†è§‰æƒ¯æ€§å¯¼èˆªç³»ç»Ÿ</font>
    + éŸ©å›½å¿ åŒ—å›½ç«‹å¤§å­¦ï¼Œå¼€æºæœŸåˆŠï¼Œ[ä½œè€…ä¸»é¡µ](https://sites.google.com/view/dinhnam/)

---

#### 3. Semantic/Deep SLAM

+ [x] **[11]** Chen C, Wang B, Lu C X, et al. [**A Survey on Deep Learning for Localization and Mapping: Towards the Age of Spatial Machine Intelligence**](https://arxiv.org/abs/2006.12567)[J]. arXiv preprint arXiv:2006.12567, **2020**.
    + <font color = blue>**æ·±åº¦å­¦ä¹ ç”¨äºå®šä½å’Œå»ºå›¾çš„è°ƒç ”ï¼šèµ°å‘ç©ºé—´æœºå™¨æ™ºèƒ½æ—¶ä»£**</font>
    + ç‰›æ´¥å¤§å­¦ï¼›æ‰€æœ‰æ¶‰åŠåˆ°çš„**è®ºæ–‡çš„åˆ—è¡¨**ï¼š[Github](https://github.com/changhao-chen/deep-learning-localization-mapping)
+ [x] **[12]** Li J, Koreitem K, Meger D, et al. [**View-Invariant Loop Closure with Oriented Semantic Landmarks**](http://www.cim.mcgill.ca/~jimmyli/pubs/li2020icra.pdf)[C]. ICRA 2020.
    + <font color = blue>**é¢å‘è¯­ä¹‰è·¯æ ‡çš„è§†å›¾ä¸å˜é—­ç¯**</font>
    + éº¦å‰å°”å¤§å­¦ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=DVuM2KsAAAAJ&hl=zh-CN&oi=sra)
+ [x] **[13]** Bavle H, De La Puente P, How J, et al. [**VPS-SLAM: Visual Planar Semantic SLAM for Aerial Robotic Systems**](https://ieeexplore.ieee.org/iel7/6287639/8948470/09045978.pdf)[J]. IEEE Access, **2020**.
    + <font color = blue>**VPS-SLAMï¼šèˆªç©ºæœºå™¨äººçš„è§†è§‰å¹³é¢è¯­ä¹‰ SLAM**</font>
    + é©¬å¾·é‡Œç†å·¥å¤§å­¦è‡ªåŠ¨åŒ–ä¸æœºå™¨äººç ”ç©¶ä¸­å¿ƒï¼ŒMIT èˆªç©ºèˆªå¤©æ§åˆ¶å®éªŒå®¤
    + [ä»£ç å¼€æº](https://bitbucket.org/hridaybavle/semantic_slam/src/master/)
+ [ ] **[14]** Shi T, Cui H, Song Z, et al. [**Dense Semantic 3D Map Based Long-Term Visual Localization with Hybrid Features**](https://arxiv.org/abs/2005.10766)[J]. arXiv preprint arXiv:2005.10766, **2020**.
    + <font color = blue>ä½¿ç”¨æ··åˆç‰¹å¾çš„åŸºäºå¯†é›† 3D è¯­ä¹‰åœ°å›¾çš„é•¿è·ç¦»è§†è§‰å®šä½</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€
+ [ ] **[15]** [**Metrically-Scaled Monocular SLAM using Learned Scale Factors**](https://wngreene.github.io/data/papers/greene_icra20.pdf) [C]. **ICRA 2020** Best Paper Award in Robot Vision
    + <font color = blue>é€šè¿‡å­¦ä¹ å°ºåº¦å› å­çš„å•ç›®åº¦é‡ SLAM</font>
    + MITï¼›[ä½œè€…ä¸»é¡µ](https://wngreene.github.io/)
+ [ ] **[16]** Chaplot D S, Salakhutdinov R, Gupta A, et al. [**Neural Topological SLAM for Visual Navigation**](http://openaccess.thecvf.com/content_CVPR_2020/papers/Chaplot_Neural_Topological_SLAM_for_Visual_Navigation_CVPR_2020_paper.pdf)[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. **CVPR 2020**: 12875-12884.
    + <font color = blue>ç”¨äºè§†è§‰å¯¼èˆªçš„ç¥ç»æ‹“æ‰‘SLAM</font>
    + CMUï¼›[é¡¹ç›®ä¸»é¡µ](https://www.cs.cmu.edu/~dchaplot/projects/neural-topological-slam.html)
+ [ ] **[17]** Min Z, Yang Y, Dunn E. [**VOLDOR: Visual Odometry From Log-Logistic Dense Optical Flow Residuals**](http://openaccess.thecvf.com/content_CVPR_2020/papers/Min_VOLDOR_Visual_Odometry_From_Log-Logistic_Dense_Optical_Flow_Residuals_CVPR_2020_paper.pdf)[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. **CVPR 2020**: 4898-4909.
    + <font color = blue>åŸºäºå¯¹æ•°é€»è¾‘ç¨ å¯†å…‰æµæ®‹å·®çš„è§†è§‰é‡Œç¨‹è®¡</font>
    + å²è’‚æ–‡æ–¯ç†å·¥å­¦é™¢ï¼›[ä»£ç å¼€æº](https://github.com/htkseason/VOLDOR)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [ ] **[18]** Loo S Y, Mashohor S, Tang S H, et al. [**DeepRelativeFusion: Dense Monocular SLAM using Single-Image Relative Depth Prediction**](https://arxiv.org/abs/2006.04047)[J]. arXiv preprint arXiv:2006.04047, **2020**.
    + <font color = blue>ä½¿ç”¨å•è§†å›¾æ·±åº¦é¢„æµ‹çš„å•ç›®ç¨ å¯† SLAM</font>
    + å“¥ä¼¦æ¯”äºšå¤§å­¦
    
---

#### 4. AR/VR/MR

+ [x] **[19]** Choudhary S, Sekhar N, Mahendran S, et al. [**Multi-user, Scalable 3D Object Detection in AR Cloud**](http://itzsid.github.io/scalable_or/CV4ARVR_Paper.pdf)[C]. CVPR Workshop on Computer Vision for Augmented and Virtual Reality, Seattle, WA, 2020.
    + <font color = blue>**AR äº‘è¿›è¡Œå¤šç”¨æˆ·å¯æ‰©å±•çš„ 3D ç›®æ ‡æ£€æµ‹**</font>
    + Magic Leap ï¼›[é¡¹ç›®ä¸»é¡µ](http://itzsid.github.io/scalable_or/index.html)
+ [ ] **[20]** Tang F, Wu Y, Hou X, et al. [**3D Mapping and 6D Pose Computation for Real Time Augmented Reality on Cylindrical Objects**](https://ieeexplore.ieee.org/abstract/document/8887213/)[J]. IEEE Transactions on Circuits and Systems for Video Technology, **2019**.
    + <font color = blue>åœ†æŸ±ç‰©ä½“ä¸Šå®æ—¶å¢å¼ºç°å®çš„ 3D å»ºå›¾å’Œ 6D ä½å§¿è®¡ç®—</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€

---

### 2020 å¹´ 5 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ20 ç¯‡ï¼‰

> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 5 æœˆ 23 æ—¥       
å…± 20 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 5 é¡¹å¼€æºå·¥ä½œ       
æœ€è¿‘ä¸çŸ¥é“æ˜¯ä¸æ˜¯å—ç–«æƒ…å½±å“ï¼Œè®ºæ–‡å¥½åƒæœ‰ç‚¹å°‘äº†        
Voxgraphï¼šè‹é»ä¸–ç†å·¥å¼€æºçš„å®æ—¶ä½“ç´ å»ºå›¾        
Neural-SLAMï¼šCMU å¼€æºçš„ä¸»åŠ¨ç¥ç»ç½‘ç»œ**

#### 1. Geometric SLAM

+ [x] **[1]** Wang W, Zhu D, Wang X, et al. [**TartanAir: A Dataset to Push the Limits of Visual SLAM**](https://arxiv.org/pdf/2003.14338)[J]. arXiv preprint arXiv:2003.14338, **2020**.
    + <font color = blue>**TartanAirï¼šçªç ´è§†è§‰ SLAM æé™çš„æ•°æ®é›†**</font>
    + CMUï¼Œæ¸¯ä¸­æ–‡ï¼›æ•°æ®é›†å…¬å¼€ï¼šhttp://theairlab.org/tartanair-dataset/
    + æœ±å¾·é¾™å¸ˆå…„å‚ä¸çš„ä¸€é¡¹å·¥ä½œï¼Œä¸Šä¸ªæœˆæ¨èè¿‡äº†ï¼Œè¿™ä¸ªæœˆåˆšå®Œå–„ç½‘ç«™å†æ¨èä¸€éï¼Œå¹¶åœ¨ CVPR 2020 ç»„ç»‡äº† [workshop](https://sites.google.com/view/vislocslamcvpr2020/slam-challenge)
+ [x] **[2]** Reijgwart V, Millane A, Oleynikova H, et al. [**Voxgraph: Globally Consistent, Volumetric Mapping Using Signed Distance Function Submaps**](https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/385682/1/Voxgraph-ETHpreprintversion.pdf)[J]. IEEE Robotics and Automation Letters, **2019**, 5(1): 227-234.
    + <font color = blue>**ä½¿ç”¨ SDF å­å›¾çš„å…¨å±€ä¸€è‡´ä½“ç´ å»ºå›¾**</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥ï¼›[**ä»£ç å¼€æº**](https://github.com/ethz-asl/voxgraph)
+ [x] **[3]** FontÃ¡n A, Civera J, Triebel R. [**Information-Driven Direct RGB-D Odometry**](https://vision.in.tum.de/_media/spezial/bib/fontan20information.pdf)[J].2020.
    + <font color = blue>ä¿¡æ¯é©±åŠ¨çš„ç›´æ¥æ³• RGB-D SLAM</font>
    + è¨æ‹‰æˆˆè¨å¤§å­¦ï¼Œ TUM
+ [x] **[4]** Murai R, Saeedi S, Kelly P H J. [**BIT-VO: Visual Odometry at 300 FPS using Binary Features from the Focal Plane**](https://arxiv.org/abs/2004.11186)[J]. arXiv preprint arXiv:2004.11186, **2020**.
    + <font color = blue>**BIT-VOï¼šä½¿ç”¨ç„¦å¹³é¢çš„äºŒè¿›åˆ¶ç‰¹å¾ä»¥ 300 FPS è¿è¡Œçš„è§†è§‰é‡Œç¨‹è®¡**</font>
    + å¸å›½ç†å·¥ [é¡¹ç›®ä¸»é¡µã€æ¼”ç¤ºè§†é¢‘](https://rmurai0610.github.io/BIT-VO/)
+ [x] **[5]** Du S, Guo H, Chen Y, et al. [**GPO: Global Plane Optimization for Fast and Accurate Monocular SLAM Initialization**](https://arxiv.org/abs/2004.12051)[J]. ICRA **2020**.
    + <font color = blue>å‡†ç¡®å¿«é€Ÿå•ç›® SLAM åˆå§‹åŒ–çš„å…¨å±€å¹³é¢ä¼˜åŒ–</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ï¼Œå­—èŠ‚è·³åŠ¨
+ [ ] **[6]** Li F, Fu C, Gostar A K, et al. [**Advanced Mapping Using Planar Features Segmented from 3D Point Clouds**](https://ieeexplore.ieee.org/abstract/document/9074570/)[C]//2019 International Conference on Control, Automation and Information Sciences (ICCAIS). IEEE, **2019**: 1-6.
    + <font color = blue>åˆ©ç”¨ 3D ç‚¹äº‘åˆ†å‰²çš„å¹³é¢è¿›è¡Œå»ºå›¾</font>
    + é‡åº†å¤§å­¦
+ [ ] **[7]** Zou Y, Chen L, Jiang J. [**Lightweight Indoor Modeling Based on Vertical Planes and Lines**](https://ieeexplore.ieee.org/abstract/document/9078949/)[C]//2020 11th International Conference on Information and Communication Systems (ICICS). IEEE, **2020**: 136-142.
    + <font color = blue>åŸºäºå‚ç›´å¹³é¢å’Œçº¿æ®µçš„å®¤å†…è½»é‡åŒ–å»ºå›¾</font>
    + å›½é˜²ç§‘å¤§ï¼›ICICSï¼šCCF C ç±»ä¼šè®®
+ [ ] **[8]** Nobis F, Papanikolaou O, Betz J, et al. [**Persistent Map Saving for Visual Localization for Autonomous Vehicles: An ORB-SLAM Extension**](https://arxiv.org/abs/2005.07429)[J]. arXiv preprint arXiv:2005.07429, **2020**.
    + <font color = blue>ORB-SLAM2 çš„æ‹“å±•åº”ç”¨ï¼šæ°¸ä¹…ä¿å­˜è½¦è¾†è§†è§‰å®šä½çš„åœ°å›¾</font>
    + TUM æ±½è½¦æŠ€æœ¯ç ”ç©¶æ‰€ï¼›[**ä»£ç å¼€æº**](https://github.com/TUMFTM/orbslam-map-saving-extension)
    
---

#### 2. Sensor Fusion

+ [x] **[9]** Li X, He Y, Lin J, et al. [**Leveraging Planar Regularities for Point Line Visual-Inertial Odometry**](https://arxiv.org/abs/2004.11969)[J]. arXiv preprint arXiv:2004.11969, **2020**.
    + <font color = blue>åˆ©ç”¨å¹³é¢è§„å¾‹çš„ç‚¹çº¿ VIO</font>
    + åŒ—äº¬å¤§å­¦ï¼›IROS 2020 æŠ•ç¨¿è®ºæ–‡
+ [ ] **[10]** Liu J, Gao W, Hu Z. [**Bidirectional Trajectory Computation for Odometer-Aided Visual-Inertial SLAM**](https://arxiv.org/abs/2002.00195)[J]. arXiv preprint arXiv:2002.00195, **2020**.
    + <font color = blue>é‡Œç¨‹è®¡è¾…åŠ©è§†æƒ¯ SLAM çš„åŒå‘è½¨è¿¹è®¡ç®—</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ï¼›**è§£å†³ SLAM åœ¨è½¬å¼¯ä¹‹åå®¹æ˜“é€€åŒ–çš„é—®é¢˜**
+ [ ] **[11]** Liu R, Marakkalage S H, Padmal M, et al. [**Collaborative SLAM based on Wifi Fingerprint Similarity and Motion Information**](https://ieeexplore.ieee.org/abstract/document/8920098/)[J]. IEEE Internet of Things Journal, **2019**.
    + <font color = blue>åŸºäº Wifi æŒ‡çº¹ç›¸ä¼¼åº¦å’Œè¿åŠ¨ä¿¡æ¯çš„åä½œå¼ SLAM</font>
    + æ–°åŠ å¡ç§‘æŠ€è®¾è®¡å¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 11.2
+ [ ] **[12]** Jung J H, Heo S, Park C G. [**Observability Analysis of IMU Intrinsic Parameters in Stereo Visual-Inertial Odometry**](https://ieeexplore.ieee.org/abstract/document/9056527/)[J]. IEEE Transactions on Instrumentation and Measurement, **2020**.
    + <font color = blue>ç«‹ä½“è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡ä¸­IMUå†…éƒ¨å‚æ•°çš„å¯è§‚å¯Ÿæ€§åˆ†æ</font>
    + éŸ©å›½é¦–å°”å¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2ï¼ŒIF 3.0

---

#### 3. Semantic/Deep SLAM

+ [x] **[13]** Wu Y, Zhang Y, Zhu D, et al. [**EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association**](https://arxiv.org/abs/2004.12730)[J]. arXiv preprint arXiv:2004.12730, **2020**.
    + <font color = blue>**åŸºäºé›†åˆæ•°æ®å…³è”çš„å•ç›®åŠç¨ å¯†ç‰©ä½“çº§ SLAM**</font>
    + ä¸œåŒ—å¤§å­¦ï¼Œæ¸¯ä¸­æ–‡ï¼›IROS 2020 æŠ•ç¨¿è®ºæ–‡
    + åšç€è„¸çš®æ¨èä¸€ä¸‹è‡ªå·±çš„å·¥ä½œï¼Œæ¬¢è¿æ‰¹è¯„æŒ‡æ­£
    + [**ä»£ç å¼€æº**]()ï¼ˆè¿˜æœªå…¬å¼€ï¼‰ï¼›æ¼”ç¤ºè§†é¢‘ï¼š[**YouTube**](https://youtu.be/pvwdQoV1KBI) | [**bilibili**](https://www.bilibili.com/video/av94805216)
+ [ ] **[14]** Vasilopoulos V, Pavlakos G, Schmeckpeper K, et al. [**Reactive Navigation in Partially Familiar Planar Environments Using Semantic Perceptual Feedback**](https://arxiv.org/abs/2002.08946)[J]. arXiv preprint arXiv:2002.08946, **2020**.
    + <font color = blue>ä½¿ç”¨è¯­ä¹‰æ„ŸçŸ¥åé¦ˆçš„éƒ¨åˆ†ç†Ÿæ‚‰å¹³é¢ç¯å¢ƒä¸­çš„ååº”æ€§å¯¼èˆª</font>
    + å®¾å¤•æ³•å°¼äºšå¤§å­¦
+ [x] **[15]** Chaplot D S, Gandhi D, Gupta S, et al. [**Learning to explore using active neural slam**](https://arxiv.org/abs/2004.05155)[C]. ICLR **2020**.
    + <font color = blue>**ä½¿ç”¨ä¸»åŠ¨ç¥ç» SLAM è¿›è¡Œå­¦ä¹ æ¢ç´¢**</font>
    + CMUï¼›[**ä»£ç å¼€æº**](https://github.com/devendrachaplot/Neural-SLAM)ï¼›[é¡¹ç›®ä¸»é¡µ](https://www.cs.cmu.edu/~dchaplot/projects/neural-slam.html)
+ [ ] **[16]** Li S, Wang X, Cao Y, et al. [**Self-Supervised Deep Visual Odometry with Online Adaptation**]()[C]. CVPR. 2020.
    + <font color = blue>åœ¨çº¿è‡ªé€‚åº”çš„è‡ªç›‘ç£è§†è§‰é‡Œç¨‹è®¡</font>
    + åŒ—äº¬å¤§å­¦
+ [x] **[17]** Li W, Gu J, Chen B, et al. [**Incremental Instance-Oriented 3D Semantic Mapping via RGB-D Cameras for Unknown Indoor Scene**](https://www.hindawi.com/journals/ddns/2020/2528954/)[J]. Discrete Dynamics in Nature and Society, 2020, **2020**.
    + <font color = blue>RGB-D ç›¸æœºå®¤å†…å¢é‡å¼ä¸‰ç»´å®ä¾‹è¯­ä¹‰å»ºå›¾</font>
    + æ²³åŒ—å·¥ä¸šå¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q3Q4 å¼€æºæœŸåˆŠ
+ [x] **[18]** Tiwari L, Ji P, Tran Q H, et al. [**Pseudo RGB-D for Self-Improving Monocular SLAM and Depth Prediction**](https://arxiv.org/abs/2004.10681)[J]. arXiv preprint arXiv:2004.10681, **2020**.
    + <font color = blue>ä¼ª RGB-D ç”¨äºæ”¹å–„å•ç›® SLAM å’Œæ·±åº¦é¢„æµ‹</font>ï¼ˆå•ç›® SLAM + å•ç›®æ·±åº¦ä¼°è®¡ï¼‰
    + å°åº¦å¾·é‡Œ Indraprastha ä¿¡æ¯æŠ€æœ¯å­¦é™¢(IIIT-Delhi)

---

#### 4. Others

+ [ ] **[19]** Wald J, Dhamo H, Navab N, et al. [**Learning 3D Semantic Scene Graphs from 3D Indoor Reconstructions**](https://arxiv.org/pdf/2004.03967.pdf)[C]. CVPR **2020**.
    + <font color = blue>ä»ä¸‰ç»´å®¤å†…é‡å»ºä¸­å­¦ä¹  3D è¯­ä¹‰åœºæ™¯å›¾</font>
    + TUMï¼›[é¡¹ç›®ä¸»é¡µ](https://3dssg.github.io/)ï¼›[Video](https://www.youtube.com/watch?v=8D3HjYf6cYw)
+ [ ] **[20]** Sommer C, Sun Y, Bylow E, et al. [**PrimiTect: Fast Continuous Hough Voting for Primitive Detection**](https://arxiv.org/abs/2005.07457)[C]. ICRA **2020**.
    + <font color = blue>ç”¨äºåŸºå…ƒæ£€éªŒçš„å¿«é€Ÿè¿ç»­éœå¤«æŠ•ç¥¨</font>
    + TUM

---

### 2020 å¹´ 4 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰
> **æœ¬æœŸæ›´æ–°äº 2020 å¹´ 4 æœˆ 25 æ—¥       
å…± 22 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 7 é¡¹å¼€æºå·¥ä½œï¼Œ 1 é¡¹å…¬å¼€æ•°æ®é›†ï¼›     
2ã€8ã€12 è·Ÿçº¿æ®µæœ‰å…³       
9ã€10 VIO ç›¸å…³        
TartanAir çªç ´è§†è§‰ SLAM æé™çš„æ•°æ®é›†ï¼ŒæŠ•ç¨¿äº IROS 2020        
VPS-SLAM å¹³é¢è¯­ä¹‰ SLAM æ¯”è¾ƒæœ‰æ„æ€ï¼Œä»£ç å¼€æº**

#### 1. Geometric SLAM

+ [x] **[1]** Wang W, Zhu D, Wang X, et al. [**TartanAir: A Dataset to Push the Limits of Visual SLAM**](https://arxiv.org/pdf/2003.14338)[J]. arXiv preprint arXiv:2003.14338, **2020**.
    + <font color = blue>**TartanAirï¼šçªç ´è§†è§‰ SLAM æé™çš„æ•°æ®é›†**</font>
    + CMUï¼Œæ¸¯ä¸­æ–‡ï¼›æ•°æ®é›†å…¬å¼€ï¼šhttp://theairlab.org/tartanair-dataset/
    + **æœ±å¾·é¾™å¸ˆå…„çš„å·¥ä½œï¼Œç½®é¡¶æ¨èä¸€ä¸‹**
+ [x] **[2]** Gomez-Ojeda R. [**Robust Visual SLAM in Challenging Environments with Low-texture and Dynamic Illumination**](https://riuma.uma.es/xmlui/handle/10630/19479)[J]. **2020**.
    + <font color = blue>**ä½çº¹ç†å’ŒåŠ¨æ€å…‰ç…§æŒ‘æˆ˜ç¯å¢ƒä¸‹çš„é²æ£’è§†è§‰ SLAM**</font>
    + è¥¿ç­ç‰™é©¬æ‹‰åŠ å¤§å­¦ï¼Œ**ç‚¹çº¿ SLAM ä½œè€…çš„åšå£«å­¦ä½è®ºæ–‡**
+ [ ] **[3]** Yang S, Li B, Cao Y P, et al. [**Noise-resilient reconstruction of panoramas and 3D scenes using robot-mounted unsynchronized commodity RGB-D cameras**](http://orca.cf.ac.uk/130657/1/DepthPano-TOG2020.pdf)[J]. ACM Transactions on Graphics, **2020**.
    + <font color = blue>ä½¿ç”¨å®‰è£…åœ¨æœºå™¨äººä¸Šçš„éå•†ç”¨ RGB-D ç›¸æœºå¯¹å…¨æ™¯å›¾å’Œä¸‰ç»´åœºæ™¯è¿›è¡ŒæŠ—å™ªå£°é‡å»º</font>
    + æ¸…åå¤§å­¦èƒ¡äº‹æ°‘æ•™æˆï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 7.176
+ [x] **[4]** Huang J, Yang S, Mu T J, et al. [**ClusterVO: Clustering Moving Instances and Estimating Visual Odometry for Self and Surroundings**](https://arxiv.org/pdf/2003.12980)[J]. arXiv preprint arXiv:2003.12980, **2020**.
    + <font color = blue>**ClusterVOï¼šå¯¹ç§»åŠ¨å®ä¾‹è¿›è¡Œèšç±»å¹¶ä¼°ç®—è‡ªèº«å’Œå‘¨å›´ç¯å¢ƒçš„è§†è§‰é‡Œç¨‹è®¡**</font>
    + æ¸…åå¤§å­¦èƒ¡äº‹æ°‘æ•™æˆï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=paK-WCQpX-Y&feature=youtu.be)
    + Huang J, Yang S, Zhao Z, et al. [**ClusterSLAM: A SLAM Backend for Simultaneous Rigid Body Clustering and Motion Estimation**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_ClusterSLAM_A_SLAM_Backend_for_Simultaneous_Rigid_Body_Clustering_and_ICCV_2019_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2019**: 5875-5884.
+ [ ] **[5]** Quenzel J, Rosu R A, LÃ¤be T, et al. [**Beyond Photometric Consistency: Gradient-based Dissimilarity for Improving Visual Odometry and Stereo Matching**](https://arxiv.org/pdf/2004.04090)[C]. International Conference on Robotics and Automation (ICRA), **2020**.
    + <font color = blue>è¶…è¶Šå…‰åº¦ä¸€è‡´æ€§ï¼šç”¨äºæ”¹å–„è§†è§‰é‡Œç¨‹è®¡å’Œç«‹ä½“åŒ¹é…çš„åŸºäºæ¢¯åº¦çš„å·®å¼‚</font>
    + [æ³¢æ©å¤§å­¦æ™ºèƒ½è‡ªä¸»å®éªŒå®¤](http://www.ais.uni-bonn.de/research.html)
+ [ ] **[6]** Yang Y, Tang D, Wang D, et al. [**Multi-camera visual SLAM for off-road navigation**](https://www.sciencedirect.com/science/article/pii/S0921889019308711)[J]. Robotics and Autonomous Systems, **2020**: 103505.
    + <font color = blue>ç”¨äºè¶Šé‡å¯¼èˆªçš„å¤šç›¸æœº SLAM</font>
    + åŒ—ç†å·¥è‡ªåŠ¨åŒ–å­¦é™¢
+ [ ] **[7]** Cheng W. [**Methods for large-scale image-based localization using structure-from-motion point clouds**](https://dr.ntu.edu.sg/bitstream/10356/137803/2/wentao_cheng_PhD_thesis.pdf)[J]. **2020**.
    + <font color = blue>åˆ©ç”¨ SFM ç‚¹äº‘åœ¨å¤§è§„æ¨¡ç¯å¢ƒä¸‹çš„åŸºäºå›¾åƒçš„å®šä½</font>
    + å—æ´‹ç†å·¥å¤§å­¦åšå£«å­¦ä½è®ºæ–‡ï¼›[ç›¸å…³ä»£ç ](https://github.com/wentaocheng-cv/cpf_localization)
+ [ ] **[8]** Sun T, Song D, Yeung D Y, et al. [**Semi-semantic Line-Cluster Assisted Monocular SLAM for Indoor Environments**](https://arxiv.org/pdf/1811.01592)[C]//International Conference on Computer Vision Systems. Springer, Cham, **2019**: 63-74.
    + <font color = blue>å®¤å†…ç¯å¢ƒä¸­åŠè¯­ä¹‰çº¿æ®µç°‡è¾…åŠ©å•ç›® SLAM</font>
    + é¦™æ¸¯ç§‘æŠ€å¤§å­¦æœºå™¨äººä¸å¤šæ„ŸçŸ¥å®éªŒå®¤ RAM-LAB

---

#### 2. Sensor Fusion

+ [x] **[9]** Nagy B, Foehn P, Scaramuzza D. [**Faster than FAST: GPU-Accelerated Frontend for High-Speed VIO**](https://arxiv.org/pdf/2003.13493)[J]. arXiv preprint arXiv:2003.13493, **2020**.
    + <font color = blue>**ç”¨äºé«˜é€Ÿ VIO çš„å‰ç«¯ GPU åŠ é€Ÿ**</font>
    + è‹é»ä¸–å¤§å­¦ã€è‹é»ä¸–è”é‚¦ç†å·¥ï¼›[**ä»£ç å¼€æº**](https://github.com/uzh-rpg/vilib)
+ [x] **[10]** Li J, Yang B, Huang K, et al. [**Robust and Efficient Visual-Inertial Odometry with Multi-plane Priors**](http://www.cad.zju.edu.cn/home/gfzhang/projects/prcv2019-planeVIO.pdf)[C]//Chinese Conference on Pattern Recognition and Computer Vision (PRCV). Springer, Cham, **2019**: 283-295.
    + <font color = blue>å…·æœ‰å¤šå¹³é¢å…ˆéªŒçš„ç¨³å¥é«˜æ•ˆçš„è§†è§‰æƒ¯æ€§é‡Œç¨‹è®¡</font>
    + æµ™å¤§ CAD&CG å®éªŒå®¤ï¼Œç« å›½å³°ï¼›[ç« è€å¸ˆä¸»é¡µ](http://www.cad.zju.edu.cn/home/gfzhang/)ä¸Šæ˜¯æ˜¾ç¤º**å°†ä¼šå¼€æº**
+ [ ] **[11]** Debeunne C, Vivet D. [**A Review of Visual-LiDAR Fusion based Simultaneous Localization and Mapping**](https://www.mdpi.com/1424-8220/20/7/2068)[J]. Sensors, **2020**, 20(7): 2068.
    + <font color = blue>è§†è§‰-æ¿€å…‰ SLAM ç»¼è¿°</font>
    + å›¾å¢å…¹å¤§å­¦ï¼›å¼€æºæœŸåˆŠï¼Œä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2Q3
+ [ ] **[12]** Yu H, Zhen W, Yang W, et al. [**Monocular Camera Localization in Prior LiDAR Maps with 2D-3D Line Correspondences**](https://arxiv.org/pdf/2004.00740)[J]. arXiv preprint arXiv:2004.00740, **2020**.
    + <font color = blue>åœ¨å…ˆéªŒé›·è¾¾åœ°å›¾ä¸­é€šè¿‡ 2D-3D çº¿æ®µå…³è”å®ç°å•ç›®è§†è§‰å®šä½</font>
    + CMUï¼Œæ­¦æ±‰å¤§å­¦ï¼›[**ä»£ç å¼€æº**](https://github.com/levenberg/2D-3D-pose-tracking)

---

#### 3. Semantic/Deep SLAM

+ [x] **[13]** Bavle H, De La Puente P, How J, et al. [**VPS-SLAM: Visual Planar Semantic SLAM for Aerial Robotic Systems**](https://ieeexplore.ieee.org/iel7/6287639/8948470/09045978.pdf)[J]. IEEE Access, **2020**.
    + <font color = blue>**VPS-SLAMï¼šèˆªç©ºæœºå™¨äººçš„è§†è§‰å¹³é¢è¯­ä¹‰ SLAM**</font>
    + é©¬å¾·é‡Œç†å·¥å¤§å­¦è‡ªåŠ¨åŒ–ä¸æœºå™¨äººç ”ç©¶ä¸­å¿ƒï¼ŒMIT èˆªç©ºèˆªå¤©æ§åˆ¶å®éªŒå®¤
    + [ä»£ç å¼€æº](https://bitbucket.org/hridaybavle/semantic_slam/src/master/)ï¼Œ[è§†é¢‘](https://vimeo.com/368217703)
+ [x] **[14]** Liao Z, Wang W, Qi X, et al. [**Object-oriented SLAM using Quadrics and Symmetry Properties for Indoor Environments**](https://arxiv.org/ftp/arxiv/papers/2004/2004.05303.pdf)[J]. arXiv preprint arXiv:2004.05303, **2020**.
    + <font color = blue>**å®¤å†…ç¯å¢ƒä¸­ä½¿ç”¨äºŒæ¬¡æ›²é¢å’Œå¯¹å¶å±æ€§çš„é¢å‘ç‰©ä½“çš„ SLAM**</font>
    + åŒ—èˆªï¼›[ä»£ç å¼€æº](https://github.com/XunshanMan/Object-oriented-SLAM)ï¼Œ[è§†é¢‘](https://www.youtube.com/watch?v=u9zRBp4TPIs)
+ [x] **[15]** Ma Q M, Jiang G, Lai D Z. [**Robust Line Segments Matching via Graph Convolution Networks**](https://arxiv.org/pdf/2004.04993.pdf)[J]. arXiv preprint arXiv:2004.04993, **2020**.
    + <font color = blue>**å›¾å·ç§¯ç¥ç»ç½‘ç»œçº¿æ®µåŒ¹é…**</font>
    + è¥¿å®‰ç”µå­ç§‘å¤§ï¼›[ä»£ç å¼€æº](https://github.com/mameng1/GraphLineMatching)
+ [ ] **[16]** Li R, Wang S, Gu D. [**DeepSLAM: A Robust Monocular SLAM System with Unsupervised Deep Learning**](https://ieeexplore.ieee.org/abstract/document/9047170/)[J]. IEEE Transactions on Industrial Electronics, **2020**.
    + <font color = blue>DeepSLAMï¼šæ— ç›‘ç£æ·±åº¦å­¦ä¹ çš„å•ç›® SLAM</font>
    + ä¸­å›½å›½å®¶å›½é˜²ç§‘æŠ€åˆ›æ–°ç ”ç©¶é™¢ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 8.4
    + ICRA 2018 æ— ç›‘ç£å•ç›® VOï¼š[Undeepvo: Monocular visual odometry through unsupervised deep learning](https://arxiv.org/pdf/1709.06841)
    + Cognitive Computation 2018 SLAM ä»å‡ ä½•åˆ°æ·±åº¦å­¦ä¹ ï¼šæŒ‘æˆ˜ä¸æœºé‡ï¼š[Ongoing evolution of visual SLAM from geometry to deep learning: challenges and opportunities](https://www.researchgate.net/profile/Ruihao_Li4/publication/327531951_Ongoing_Evolution_of_Visual_SLAM_from_Geometry_to_Deep_Learning_Challenges_and_Opportunities/links/5c8650db92851c1d5e156d7f/Ongoing-Evolution-of-Visual-SLAM-from-Geometry-to-Deep-Learning-Challenges-and-Opportunities.pdf)

---

#### 4. AR/VR/MR

+ [ ] **[17]** Baker L, Ventura J, Zollmann S, et al. [**SPLAT: Spherical Localization and Tracking in Large Spaces**](http://www.jventura.net/sites/default/files/lbaker_vr20b.pdf)[J].**2020**.
    + <font color = blue>SPLATï¼šå¤§åœºæ™¯ä¸­çƒå½¢å®šä½ä¸è·Ÿè¸ª</font>
    + æ–°è¥¿å…°å¥¥å¡”å“¥å¤§å­¦
+ [ ] **[18]** Valentini I, Ballestin G, Bassano C, et al. [**Improving Obstacle Awareness to Enhance Interaction in Virtual Reality**](https://conferences.computer.org/vr-tvcg/2020/pdfs/VR2020-2f8MzUJjtCXG6Ue9RYFSN2/560800a044/560800a044.pdf)[C]. IEEE Conference on Virtual Reality and 3D User Interfaces (VR). **2020**.
    + <font color = blue>å¢å¼ºéšœç¢æ„è¯†ä»¥æå‡è™šæ‹Ÿç°å®ä¸­çš„äº’åŠ¨</font>
    + æ„å¤§åˆ©çƒ­é‚£äºšå¤§å­¦ï¼›[video](https://www.youtube.com/watch?v=dPJ9rSK_p3I&feature=youtu.be)
+ [ ] **[19]** Stylianidis E, Valari E, Pagani A, et al. [**Augmented Reality Geovisualisation for Underground Utilities**](https://link.springer.com/content/pdf/10.1007/s41064-020-00108-x.pdf)[J]. **2020**.
    + <font color = blue>å¢å¼ºç°å®åœ°ç†å¯è§†åŒ–</font>
    + å¸Œè…Šäºšé‡Œå£«å¤šå¾·å¤§å­¦

---

#### 5. Others

+ [ ] **[20]** Sengupta S, Jayaram V, Curless B, et al. [**Background Matting: The World is Your Green Screen**](https://arxiv.org/pdf/2004.00626.pdf)[J]. arXiv preprint arXiv:2004.00626, **2020**.
    + <font color = blue>èƒŒæ™¯æŠ å›¾</font>
    + åç››é¡¿å¤§å­¦ï¼›[**ä»£ç å¼€æº**](https://github.com/senguptaumd/Background-Matting)
+ [ ] **[21]** Wang L, Wei H. [**Avoiding non-Manhattan obstacles based on projection of spatial corners in indoor environment**](https://ieeexplore.ieee.org/abstract/document/9049452/)[J]. IEEE/CAA Journal of Automatica Sinica, **2020**.
    + <font color = blue>å®¤å†…ç¯å¢ƒä¸­åŸºäºç©ºé—´è§’æŠ•å½±é¿å…éæ›¼å“ˆé¡¿éšœç¢ç‰©</font>
    + åŒ—å¤§ã€ä¸Šæµ·ç†å·¥ã€å¤æ—¦ï¼›æœŸåˆŠï¼šè‡ªåŠ¨åŒ–å­¦æŠ¥è‹±æ–‡ç‰ˆ
+ [ ] **[22]** Spencer J, Bowden R, Hadfield S. [**Same Features, Different Day: Weakly Supervised Feature Learning for Seasonal Invariance**](https://arxiv.org/pdf/2003.13431)[J]. arXiv preprint arXiv:2003.13431, **2020**.
    + <font color = blue>ä¸åŒæ—¶é—´çš„ç›¸åŒç‰¹å¾ï¼šå­£èŠ‚æ€§ä¸å˜çš„å¼±ç›‘ç£ç‰¹å¾å­¦ä¹ </font>
    + è‹±å›½è¨é‡Œå¤§å­¦ï¼›[**ä»£ç å¼€æº**](https://github.com/jspenmar/DejaVu_Features)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰

--- 

### 2020 å¹´ 3 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ23 ç¯‡ï¼‰
> **æœ¬æœŸ 23 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 7 é¡¹å¼€æºå·¥ä½œï¼›     
1ã€2 å¤šç›¸æœº SLAM ç³»ç»Ÿ       
9ã€10 VIO        
21ã€22 3D ç›®æ ‡æ£€æµ‹        
12-19 å…«ç¯‡è·Ÿ semantic/deep learning æœ‰å…³ï¼Œè¶‹åŠ¿ï¼Ÿ        
æ³¨ï¼šæ²¡æœ‰ç‰¹æ„æ•´ç† CVPRï¼ŒICRA æ–°çš„è®ºæ–‡ï¼Œå¤§éƒ¨åˆ†éƒ½åŠå¹´å‰å°±æœ‰é¢„å°ç‰ˆäº†ï¼Œåœ¨è¿™ä¸ªä»“åº“é‡ŒåŸºæœ¬ä¸Šä¹Ÿæ—©æ”¶å½•äº†        
2020 å¹´ 3 æœˆ 29 æ—¥æ›´æ–°**
        
#### 1. Geometric SLAM

+ [ ] **[1]** Kuo J, Muglikar M, Zhang Z, et al. [**Redesigning SLAM for Arbitrary Multi-Camera Systems**](https://arxiv.org/pdf/2003.02014.pdf)[C]. ICRA **2020**.
    + <font color = blue>ä»»æ„å¤šç›¸æœº SLAM ç³»ç»Ÿ</font>
    + è‹é»ä¸–å¤§å­¦ã€è‹é»ä¸–è”é‚¦ç†å·¥ [æœºå™¨äººä¸æ„ŸçŸ¥è¯¾é¢˜ç»„](http://rpg.ifi.uzh.ch/research_vo.html)ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=JGL4H93BiNw&feature=youtu.be)
+ [ ] **[2]** Won C, Seok H, Cui Z, et al. [**OmniSLAM: Omnidirectional Localization and Dense Mapping for Wide-baseline Multi-camera Systems**](https://arxiv.org/pdf/2003.08056.pdf)[J]. arXiv preprint arXiv:2003.08056, **2020**.
    + <font color = blue>**OmniSLAMï¼šå®½åŸºçº¿å’Œå¤šç›¸æœºçš„å…¨å‘å®šä½å’Œå»ºå›¾**</font>
    + éŸ©å›½æ±‰é˜³å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»
+ [ ] **[3]** Colosi M, Aloise I, Guadagnino T, et al. [**Plug-and-Play SLAM: A Unified SLAM Architecture for Modularity and Ease of Use**](https://arxiv.org/pdf/2003.00754.pdf)[J]. arXiv preprint arXiv:2003.00754, **2020**.
    + <font color = blue>**å³æ’å³ç”¨å‹ SLAMï¼šæ¨¡å—åŒ–ä¸”æ˜“ç”¨çš„ SLAM ç»Ÿä¸€æ¡†æ¶**</font>
    + æ„å¤§åˆ©ç½—é©¬è¨çš®æ©æ‰å¤§å­¦ï¼›[**ä»£ç å¼€æº**](https://github.com/srrg-sapienza/srrg2_slam_interfaces)
    + ä½œè€…ä¹‹å‰ä¸€ç¯‡ç±»ä¼¼çš„æ–‡ç« ï¼Œæ•™ä½ æ€ä¹ˆæ¨¡å—åŒ–ä¸€ä¸ª SLAM ç³»ç»Ÿï¼š
        + Schlegel D, Colosi M, Grisetti G. [**Proslam: Graph SLAM from a programmer's perspective**](https://arxiv.org/pdf/1709.04377.pdf)[C]//2018 IEEE International Conference on Robotics and Automation (**ICRA**). IEEE, **2018**: 1-9.
        + [ä»£ç å¼€æº](https://gitlab.com/srrg-software/srrg_proslam)
+ [ ] **[4]** Wu X, Vela P, Pradalier C. [**Robust Monocular Edge Visual Odometry through Coarse-to-Fine Data Association**](https://www.researchgate.net/profile/Xiaolong_Wu5/publication/336056167_Robust_Monocular_Edge_Visual_Odometry_through_Coarse-to-Fine_Data_Association/links/5e700cc4458515eb5aba76fd/Robust-Monocular-Edge-Visual-Odometry-through-Coarse-to-Fine-Data-Association.pdf)[J].**2020**.
    + <font color = blue>é€šè¿‡ä»ç²—åˆ°ç»†çš„æ•°æ®å…³è”å®ç°é²æ£’çš„å•ç›®åŸºäºè¾¹çš„è§†è§‰é‡Œç¨‹è®¡</font>
    + ä½æ²»äºšç†å·¥å­¦é™¢
+ [ ] **[5]** Rosinol A, Gupta A, Abate M, et al. [**3D Dynamic Scene Graphs: Actionable Spatial Perception with Places, Objects, and Humans**]()[J]. arXiv preprint arXiv:2002.06289, **2020**.
    + <font color = blue>3D åŠ¨æ€åœºæ™¯å›¾ï¼šå…·æœ‰ä½ç½®ï¼Œç‰©ä½“å’Œäººçš„å¯æ“ä½œç©ºé—´æ„ŸçŸ¥</font>
    + MITï¼›[Kimera](https://github.com/MIT-SPARK/Kimera-Semantics) çš„ä½œè€…ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=SWbofjhyPzI&feature=youtu.be)ï¼›[Google Scholar](https://scholar.google.com/citations?user=aFGYNYgAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[6]** Zeng T, Li X, Si B. [**StereoNeuroBayesSLAM: A Neurobiologically Inspired Stereo Visual SLAM System Based on Direct Sparse Method**]()[J]. arXiv preprint arXiv:2003.03091, **2020**.
    + <font color = blue>ç±»è„‘åŒç›®ç›´æ¥ç¨€ç– SLAM</font>
    + æ²ˆè‡ªæ‰€æ–¯è€å¸ˆ
+ [x] **[7]** Oleynikova H, Taylor Z, Siegwart R, et al. [**Sparse 3d topological graphs for micro-aerial vehicle planning**](https://arxiv.org/pdf/1803.04345.pdf)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, **2018**: 1-9.
    + <font color = blue>**å¾®å‹é£è¡Œå™¨è·¯å¾„è§„åˆ’çš„ç¨€ç– 3D æ‹“æ‰‘å›¾**</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥ï¼›[ä½œè€…ä¸»é¡µ](http://helenol.github.io/)ï¼›è·¯å¾„è§„åˆ’ä¸å»ºå›¾éƒ¨åˆ†[ä»£ç å¼€æº](https://github.com/ethz-asl/voxblox)ï¼Œç›¸å…³è®ºæ–‡ï¼š
        + Oleynikova H, Taylor Z, Fehr M, et al. [**Voxblox: Incremental 3d euclidean signed distance fields for on-board mav planning**](https://arxiv.org/pdf/1611.03631)[C]//2017 Ieee/rsj International Conference on Intelligent Robots and Systems (iros). IEEE, **2017**: 1366-1373.
+ [ ] **[8]** Ye H, Huang H, Liu M. [**Monocular Direct Sparse Localization in a Prior 3D Surfel Map**](https://arxiv.org/pdf/2002.09923.pdf)[J]. arXiv preprint arXiv:2002.09923, **2020**.
    + <font color = blue>åœ¨ Surfel åœ°å›¾ä¸­çš„å•ç›®ç¨€ç–ç›´æ¥æ³•å®šä½</font>
    + [æ¸¯ç§‘å¤§ RAM å®éªŒå®¤](https://ram-lab.com/)
    + Tipsï¼šæ„é€ ç¨€ç–ç‚¹çš„å…¨å±€å¹³é¢ä¿¡æ¯
    
---

#### 2. ä¼ æ„Ÿå™¨èåˆ

+ [ ] **[9]** Zhao Y, Smith J S, Karumanchi S H, et al. [**Closed-Loop Benchmarking of Stereo Visual-Inertial SLAM Systems: Understanding the Impact of Drift and Latency on Tracking Accuracy**](https://arxiv.org/pdf/2003.01317.pdf)[C].  **ICRA 2020**.
    + <font color = blue>åŒç›®è§†è§‰é‡Œç¨‹è®¡é—­ç¯æ£€æµ‹åŸºå‡†ç³»ç»Ÿï¼šå…³äºæµ‹è¯•æ¼‚ç§»å’Œå»¶è¿Ÿå¯¹è·Ÿè¸ªå‡†ç¡®æ€§çš„å½±å“</font>
    + ä½æ²»äºšç†å·¥å­¦é™¢ IVALab [ROBO SLAM](https://ivalab.github.io/RoboSLAM/public/research/)ï¼›[**ä»£ç å¼€æº**](https://github.com/ivalab/meta_ClosedLoopBench)ï¼›[ä½œè€…ä¸»é¡µ](https://sites.google.com/site/zhaoyipu/)
+ [ ] **[10]** Giubilato R, Chiodini S, Pertile M, et al. [**MiniVO: Minimalistic Range Enhanced Monocular System for Scale Correct Pose Estimation**]()[J]. **2020**.
    + <font color = blue>ç”¨äºæ­£ç¡®å°ºåº¦ä½å§¿ä¼°è®¡çš„æœ€å°èŒƒå›´å¢å¼ºå•ç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + æ„å¤§åˆ©å¸•å¤šç“¦å¤§å­¦
    + Tipsï¼š1D LiDAR çŸ«æ­£å•ç›®å°ºåº¦
+ [ ] **[11]** Huang W, Liu H, Wan W. [**An Online Initialization and Self-Calibration Method for Stereo Visual-Inertial Odometry**](https://www.researchgate.net/profile/Weibo_Huang2/publication/339394318_An_Online_Initialization_and_Self-Calibration_Method_for_Stereo_Visual-Inertial_Odometry/links/5e58b0b2299bf1bdb840bd59/An-Online-Initialization-and-Self-Calibration-Method-for-Stereo-Visual-Inertial-Odometry.pdf)[J]. IEEE Transactions on Robotics, **2020**.
    + <font color = blue>ä¸€ç§åŒç›®è§†æƒ¯é‡Œç¨‹è®¡çš„åœ¨çº¿åˆå§‹åŒ–å’Œè‡ªæ ‡å®šæ–¹æ³•</font>
    + åŒ—äº¬å¤§å­¦ï¼›[Google Scholar](https://scholar.google.com/citations?user=d72ju6AAAAAJ&hl=zh-CN&authuser=1&oi=sra)ï¼›ä½œè€…å¦å¤–ä¸€ç¯‡æ–‡ç« ï¼š
        + Huang W, Liu H. [**Online initialization and automatic camera-IMU extrinsic calibration for monocular visual-inertial SLAM**](http://robotics.pkusz.edu.cn/static/papers/ICRA2018-huangweibo.pdf)[C]//2018 IEEE International Conference on Robotics and Automation (**ICRA**). IEEE, **2018**: 5182-5189.

---

#### 3. Semantic/Deep SLAM

+ [ ] **[12]** Landgraf Z, Falck F, Bloesch M, et al. [**Comparing View-Based and Map-Based Semantic Labelling in Real-Time SLAM**](https://arxiv.org/pdf/2002.10342.pdf)[J]. arXiv preprint arXiv:2002.10342, **2020**.
    + <font color = blue>åœ¨å®æ—¶ SLAM ä¸­æ¯”è¾ƒåŸºäºè§†å›¾å’ŒåŸºäºåœ°å›¾çš„è¯­ä¹‰æ ‡ç­¾</font>
    + å¸å›½ç†å·¥å­¦é™¢è®¡ç®—æœºç³»æˆ´æ£®æœºå™¨äººå®éªŒå®¤
+ [ ] **[13]** Singh G, Wu M, Lam S K. [**Fusing Semantics and Motion State Detection for Robust Visual SLAM**](http://openaccess.thecvf.com/content_WACV_2020/papers/Singh_Fusing_Semantics_and_Motion_State_Detection_for_Robust_Visual_SLAM_WACV_2020_paper.pdf)[C]//The IEEE Winter Conference on Applications of Computer Vision. **2020**: 2764-2773.
    + <font color = blue>èåˆè¯­ä¹‰å’Œè¿åŠ¨çŠ¶æ€æ£€æµ‹ä»¥å®ç°é²æ£’çš„è§†è§‰ SLAM</font>
    + å—æ´‹ç†å·¥å¤§å­¦
+ [ ] **[14]** Gupta A, Iyer G, Kodgule S. [**DeepEvent-VO: Fusing Intensity Images and Event Streams for End-to-End Visual Odometry**](https://epiception.github.io/assets/Projects/deep_event_vo.pdf)[J].
    + <font color = blue>DeepEvent-VOï¼šèåˆå¼ºåº¦å›¾åƒå’Œäº‹ä»¶æµçš„ç«¯åˆ°ç«¯è§†è§‰æµ‹è·</font>
    + CMUï¼›[**ä»£ç å¼€æº**](https://github.com/SuhitK/DeepEvent-VO)
+ [ ] **[15]** Wagstaff B, Peretroukhin V, Kelly J. [**Self-Supervised Deep Pose Corrections for Robust Visual Odometry**](https://arxiv.org/pdf/2002.12339.pdf)[J]. arXiv preprint arXiv:2002.12339, **2020**.
    + <font color = blue>é²æ£’è§†è§‰é‡Œç¨‹è®¡çš„è‡ªç›‘ç£æ·±åº¦ä½å§¿çŸ«æ­£</font>
    + å¤šä¼¦å¤šå¤§å­¦ STARS å®éªŒå®¤ï¼›[**ä»£ç å¼€æº**](https://github.com/utiasSTARS/ss-dpc-net)
+ [ ] **[16]** Ye X, Ji X, Sun B, et al. [**DRM-SLAM: Towards dense reconstruction of monocular SLAM with scene depth fusion**](https://www.sciencedirect.com/science/article/abs/pii/S0925231220302344)[J]. Neurocomputing, 2020.
    + <font color = blue>DRM-SLAMï¼šé€šè¿‡åœºæ™¯æ·±åº¦èåˆå®ç°å•ç›® SLAM çš„ç¨ å¯†é‡å»º</font>
    + å¤§è¿ç†å·¥å¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 3.824
+ [ ] **[17]** Yang N, von Stumberg L, Wang R, et al. [**D3VO: Deep Depth, Deep Pose and Deep Uncertainty for Monocular Visual Odometry**](https://arxiv.org/pdf/2003.01060.pdf)[C]. CVPR **2020**.
    + <font color = blue>D3VOï¼šå•ç›®è§†è§‰é‡Œç¨‹è®¡ä¸­é’ˆå¯¹æ·±åº¦ã€ä½å§¿å’Œä¸ç¡®å®šæ€§çš„æ·±åº¦ç½‘ç»œ</font>
    + TUM è®¡ç®—æœºè§†è§‰ç»„ï¼›[ä¸ªäººä¸»é¡µ](https://vision.in.tum.de/members/wangr)
+ [ ] **[18]** Chen C, Rosa S, Miao Y, et al. [**Selective sensor fusion for neural visual-inertial odometry**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Selective_Sensor_Fusion_for_Neural_Visual-Inertial_Odometry_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2019**: 10542-10551.
    + <font color = blue>ç¥ç»è§†è§‰ VIO çš„é€‰æ‹©æ€§ä¼ æ„Ÿå™¨èåˆ</font>
    + ç‰›æ´¥å¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»ï¼›[Google Scholar](https://scholar.google.com/citations?user=OqlY-98AAAAJ&hl=zh-CN&authuser=1&oi=sra)
+ [ ] **[19]** Towards the Probabilistic [**Fusion of Learned Priors into Standard Pipelines for 3D Reconstruction**](https://www.doc.ic.ac.uk/~sleutene/publications/Laidlow_ICRA2020_CameraReady.pdf)[C]. ICRA **2020**.
    + <font color = blue>å°†å­¦ä¹ çš„å…ˆéªŒä¿¡æ¯èåˆåˆ°æ ‡å‡†çš„ä¸‰ç»´é‡å»ºä¸­</font>
    + [å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤](https://www.imperial.ac.uk/dyson-robotics-lab/publications/)

---

#### 4. AR/VR/MR

+ [ ] **[20]** Wu L, Wan W, Yu X, et al. [**A novel augmented reality framework based on monocular semiâ€dense simultaneous localization and mapping**](https://onlinelibrary.wiley.com/doi/pdf/10.1002/cav.1922?casa_token=1GWiXGz2GzIAAAAA:1vDJn5Dx97ruaTSTthXoUpfBDRPe4rjvRl5UsTbRSbgvzU3YrXin54Al_jQWLdhwDm-TCm7HF-92cA)[J]. Computer Animation and Virtual Worlds, **2020**: e1922.
    + <font color = blue>**åŸºäºå•ç›®åŠç¨ å¯† SLAM çš„æ–°å‹ AR æ¡†æ¶**</font>
    + ä¸Šæµ·å¤§å­¦ï¼›æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼ŒJCR Q4ï¼ŒIF 0.794
    
---

#### 5. Others

+ [x] **[21]** Shi W. [**Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud**]()[C]. **CVPR 2020**.
    + <font color = blue>**Point-GNN: å›¾ç¥ç»ç½‘ç»œç”¨äºç‚¹äº‘ä¸­çš„ 3D ç›®æ ‡å¯¹è±¡æ£€æµ‹**</font>
    + CMUï¼› [**ä»£ç å¼€æº**](https://github.com/WeijingShi/Point-GNN)ï¼›[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=K7AoP_wAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[22]** Chen Y, Tai L, Sun K, et al. [**MonoPair: Monocular 3D Object Detection Using Pairwise Spatial Relationships**](https://arxiv.org/pdf/2003.00504.pdf)[C]. **CVPR 2020**.
    + <font color = blue>MonoPair: ä½¿ç”¨æˆå¯¹ç©ºé—´å…³ç³»çš„å•ç›® 3D å¯¹è±¡æ£€æµ‹</font>
    + é˜¿é‡Œå·´å·´
+ [ ] **[23]** Chen X, Song J, Hilliges O. [**Monocular neural image based rendering with continuous view control**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Chen_Monocular_Neural_Image_Based_Rendering_With_Continuous_View_Control_ICCV_2019_paper.pdf)[C]//Proceedings of the IEEE International Conference on Computer Vision. **2019**: 4090-4100.
    + <font color = blue>åŸºäºè¿ç»­è§†å›¾æ§åˆ¶çš„å•ç›®ç¥ç»å›¾åƒç»˜åˆ¶</font>
    + [è‹é»ä¸–è”é‚¦ç†å·¥ AIT å®éªŒå®¤](https://ait.ethz.ch/)
    + ç±»ä¼¼äºç™¾åº¦è‡ªåŠ¨é©¾é©¶ä»¿çœŸ AADS é‡‡ç”¨çš„æ–°è§†å›¾åˆæˆï¼Ÿ

---

### 2020 å¹´ 2 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰
> **è¿™ä¸ªæœˆèµ¶è®ºæ–‡ï¼Œçœ‹çš„è®ºæ–‡æ¯”è¾ƒå°‘ï¼Œæœ¬æœŸ 17 ç¯‡ï¼Œå…¶ä¸­ 3 é¡¹å¼€æºå·¥ä½œï¼›     
1ã€2ã€3ã€4 å»ºå›¾ç›¸å…³        
7ã€8ã€9 åŠ¨æ€ç›¸å…³        
10ã€11 è§†æƒ¯èåˆ        
13ã€14ã€15 ARç›¸å…³        
2020 å¹´ 2 æœˆ 25 æ—¥æ›´æ–°**
        
#### 1. Geometric SLAM

+ [x] **[1]** Muglikar M, Zhang Z, Scaramuzza D. [**Voxel map for visual slam**](https://arxiv.org/pdf/2003.02247.pdf)[C]. ICRA **2020**.
    + <font color = blue>**ä½¿ç”¨ä½“ç´ å›¾çš„è§†è§‰ SLAM**</font>
    + è‹é»ä¸–å¤§å­¦ï¼Œå¼ å­æ½®
+ [ ] **[2]** Ye X, Ji X, Sun B, et al. [**DRM-SLAM: Towards Dense Reconstruction of Monocular SLAM with Scene Depth Fusion**](https://www.sciencedirect.com/science/article/pii/S0925231220302344)[J]. Neurocomputing, **2020**.
    + <font color = blue>é€šè¿‡åœºæ™¯æ·±åº¦èåˆå®ç°å•ç›® SLAM çš„ç¨ å¯†é‡å»º</font>
    + å¤§è¿ç†å·¥å¤§å­¦ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼Œ IF 4.0
+ [x] **[3]** Nardi F, Grisetti G, Nardi D. [**High-Level Environment Representations for Mobile Robots**](https://iris.uniroma1.it/retrieve/handle/11573/1222797/982918/Tesi_dottorato_Nardi.pdf). **2019**.
    + <font color = blue>**ç§»åŠ¨æœºå™¨äººé«˜çº§åˆ«çš„ç¯å¢ƒè¡¨ç¤º**</font>
    + ç½—é©¬å¤§å­¦åšå£«å­¦ä½è®ºæ–‡
+ [ ] **[4]** Puligilla S S, Tourani S, Vaidya T, et al. [**Topological Mapping for Manhattan-like Repetitive Environments**](https://arxiv.org/pdf/2002.06575.pdf)[J]. arXiv preprint arXiv:2002.06575, **2020**.
    + <font color = blue>ç±»æ›¼å“ˆé¡¿é‡å¤ç¯å¢ƒä¸­çš„æ‹“æ‰‘å»ºå›¾</font>
    + å°åº¦æµ·å¾—æ‹‰å·´å›½é™…ä¿¡æ¯æŠ€æœ¯ç ”ç©¶æ‰€ï¼›[ä»£ç å¼€æº](https://github.com/Shubodh/ICRA2020)ï¼›[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=swYcwrjprh0)
+ [ ] **[5]** Li X, Ling H. [**Hybrid Camera Pose Estimation with Online Partitioning for SLAM**](https://www3.cs.stonybrook.edu/~hling/publication/slam-icra20.pdf)[J]. IEEE Robotics and Automation Letters, **2020**, 5(2): 1453-1460.
    + <font color = blue>åœ¨çº¿åˆ†å‰² SLAM ä¸­çš„æ··åˆç›¸æœºä½å§¿ä¼°è®¡</font>
    + å¤©æ™®å¤§å­¦ï¼Œæ—æµ·æ»¨æ•™æˆ
+ [ ] **[6]** Karimian A, Yang Z, Tron R. [**Statistical Outlier Identification in Multi-robot Visual SLAM using Expectation Maximization**](https://arxiv.org/pdf/2002.02638.pdf)[J]. arXiv preprint arXiv:2002.02638, **2020**.
    + <font color = blue>ä½¿ç”¨æœ€å¤§æœŸæœ›è¯†åˆ«å¤šæœºå™¨äºº VSLAM ä¸­çš„å¼‚å¸¸å€¼</font>
    + æ³¢å£«é¡¿å¤§å­¦
+ [x] **[7]** Henein M, Zhang J, Mahony R, et al. [**Dynamic SLAM: The Need For Speed**](https://arxiv.org/pdf/2002.08584.pdf)[J]. arXiv preprint arXiv:2002.08584, **2020**.
    + <font color = blue>**æ»¡è¶³é€Ÿåº¦ä¼°è®¡éœ€æ±‚çš„åŠ¨æ€ SLAM**</font>
    + æ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦ï¼Œä½œè€…ä¸»è¦ç ”ç©¶åŠ¨æ€ SLAM [**Google Scholar**](https://scholar.google.com/citations?user=PUBFzCAAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[8]** Nair G B, Daga S, Sajnani R, et al. [**Multi-object Monocular SLAM for Dynamic Environments**](https://arxiv.org/pdf/2002.03528.pdf)[J]. arXiv preprint arXiv:2002.03528, **2020**.
    + <font color = blue>ç”¨äºåŠ¨æ€ç¯å¢ƒçš„å¤šç›®æ ‡å•ç›® SLAM</font>
    + å°åº¦æµ·å¾—æ‹‰å·´ç†å·¥å­¦é™¢
+ [ ] **[9]** Cheng J, Zhang H, Meng M Q H. [**Improving Visual Localization Accuracy in Dynamic Environments Based on Dynamic Region Removal**](https://ieeexplore.ieee.org/abstract/document/8972551/)[J]. IEEE Transactions on Automation Science and Engineering, **2020**.
    + <font color = blue>é€šè¿‡åŠ¨æ€åŒºåŸŸå‰”é™¤æ¥æå‡åŠ¨æ€ç¯å¢ƒä¸­è§†è§‰å®šä½çš„å‡†ç¡®æ€§</font>
    + æ¸¯ä¸­æ–‡ï¼›ä¸­ç§‘é™¢äºŒåŒº JCR Q1

---

#### 2. ä¼ æ„Ÿå™¨èåˆ

+ [ ] **[10]** Patel N, Khorrami F, Krishnamurthy P, et al. [**Tightly Coupled Semantic RGB-D Inertial Odometry for Accurate Long-Term Localization and Mapping**](https://ieeexplore.ieee.org/abstract/document/8981658/)[C]//2019 19th International Conference on Advanced Robotics (**ICAR**). IEEE, **2019**: 523-528.
    + <font color = blue>ç”¨äºç²¾ç¡®ã€é•¿æœŸå®šä½å’Œå»ºå›¾çš„ç´§è€¦åˆè¯­ä¹‰ RGB-D æƒ¯æ€§é‡Œç¨‹è®¡</font>
    + çº½çº¦å¤§å­¦
+ [ ] **[11]** Chiodini S, Giubilato R, Pertile M, et al. [**Retrieving Scale on Monocular Visual Odometry Using Low Resolution Range Sensors**](https://ieeexplore.ieee.org/abstract/document/8950136/)[J]. IEEE Transactions on Instrumentation and Measurement, **2020**.
    + <font color = blue>ä½¿ç”¨ä½åˆ†è¾¨ç‡è·ç¦»ä¼ æ„Ÿå™¨æ¢å¤å•ç›®è§†è§‰é‡Œç¨‹è®¡çš„å°ºåº¦</font>
    + æ„å¤§åˆ©å¸•å¤šç“¦å¤§å­¦ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒº JCR Q1Q2

---

#### 3. Semantic/Deep SLAM

+ [ ] **[12]** Jin S, Chen L, Sun R, et al. [**A novel vSLAM framework with unsupervised semantic segmentation based on adversarial transfer learning**](https://www.sciencedirect.com/science/article/pii/S1568494620300934)[J]. Applied Soft Computing, **2020**: 106153.
    + <font color = blue>åŸºäºå¯¹æŠ—è¿ç§»å­¦ä¹ çš„æ— ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„æ–°å‹ vSLAM æ¡†æ¶</font>
    + è‹å·å¤§å­¦ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒº JCR Q1

---

#### 4. AR/VR/MR

+ [x] **[13]** Liu R, Zhang J, Chen S, et al. [**Towards SLAM-based outdoor localization using poor GPS and 2.5 D building models**]()[C]//2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, **2019**: 1-7.
    + <font color = blue>**ä½¿ç”¨ç²—ç³™çš„ GPS å’Œ 2.5 D å»ºç­‘æ¨¡å‹å®ç°åŸºäº SLAM çš„æˆ·å¤–å®šä½**</font>
    + æµ™æ±Ÿå·¥ä¸šå¤§å­¦ï¼› [ä»£ç å¼€æº](https://github.com/luyujiangmiao/Buiding-GPS-SLAM)
+ [x] **[14]** Miyamoto K, Shiraga T, Okato Y. [**User-Selected Object Data Augmentation for 6DOF CNN Localization**](https://pdfs.semanticscholar.org/1ddc/9ba1668db506717fbe180ae63152917d174f.pdf)[J].
    + <font color = blue>**6 è‡ªç”±åº¦ CNN å®šä½çš„ç”¨æˆ·é€‰æ‹©ç›®æ ‡æ•°æ®å¢å¼º**</font>
+ [x] **[15]** Gui Z W. [**Register Based on Large Scene for Augmented Reality System**](https://jit.ndhu.edu.tw/article/view/2226)[J]. Journal of Internet Technology, **2020**, 21(1): 99-111.
    + <font color = blue>åŸºäºå¤§åœºæ™¯çš„å¢å¼ºç°å®ä¸‰ç»´æ³¨å†Œ</font>
    + åŒ—ç†å·¥ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼Œ IF 0.7

---

#### 5. Others

+ [x] **[16]** Gao G, Lauri M, Wang Y, et al. [**6D Object Pose Regression via Supervised Learning on Point Clouds**](https://arxiv.org/pdf/2001.08942.pdf)[C]. ICRA 2020.
    + <font color = blue>**é€šè¿‡ç‚¹äº‘ä¸Šçš„ç›‘ç£å­¦ä¹ è¿›è¡Œ 6D ç‰©ä½“ä½å§¿å›å½’**</font>
    + å¾·å›½æ±‰å ¡å¤§å­¦ã€æ¸…åå¤§å­¦ï¼› [ä»£ç å¼€æº](https://github.com/GeeeG/CloudPose)ï¼› [æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=OhNMngzxFDQ)
+ [x] **[17]** Habib R, Saii M. [**Object Pose Estimation in Monocular Image Using Modified FDCM**](https://journals.agh.edu.pl/csci/article/view/3426)[J]. Computer Science, **2020**, 21(1).
    + <font color = blue>ä½¿ç”¨æ”¹è¿›çš„ FDCM ä¼°è®¡å•ç›®å›¾åƒä¸­çš„ç‰©ä½“ä½ç½®</font>
    + ç±»ä¼¼äºæ—‹è½¬ç‰©ä½“æ£€éªŒï¼Ÿ
    
---

### 2020 å¹´ 1 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ26 ç¯‡ï¼‰
> **æœ¬æœŸ 26 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 7 é¡¹å¼€æºå·¥ä½œï¼Œ1 é¡¹å¼€æ”¾æ•°æ®é›†ï¼›     
5ã€6ã€10 å…³äºçº¿æ®µçš„ SLAM        
7 åŸºäºäº‹ä»¶ç›¸æœºçš„ SLAM ç»¼è¿°        
8ã€9ã€10 è§†æƒ¯èåˆ        
16ã€17 AR+SLAM        
2020 å¹´ 1 æœˆ 28 æ—¥æ›´æ–°**

#### 1. Geometric SLAM

+ [ ] **[1]** RÃœCKERT, Darius; INNMANN, Matthias; STAMMINGER, Marc. [**FragmentFusion: A Light-Weight SLAM Pipeline for Dense Reconstruction**](https://ieeexplore.ieee.org/abstract/document/8951922/). In: 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**. p. 342-347.
    + <font color = blue>FragmentFusionï¼šä¸€ç§è½»é‡çº§çš„ç”¨äºç¨ å¯†é‡å»ºçš„æ–¹æ¡ˆ</font>
    + å¾·å›½åŸƒæœ—æ ¹-çº½ä¼¦å ¡å¤§å­¦
+ [x] **[2]** Chen Y, Shen S, Chen Y, et al. [**Graph-Based Parallel Large Scale Structure from Motion**](https://arxiv.org/pdf/1912.10659.pdf)[J]. arXiv preprint arXiv:1912.10659, **2019**.
    + <font color = blue>åŸºäºå›¾çš„å¹¶è¡Œå¤§å°ºåº¦çš„ SFM</font>
    + ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ï¼Œ[**ä»£ç å¼€æº**](https://github.com/AIBluefisher/GraphSfM)
+ [x] **[3]** Sommer C, Sun Y, Guibas L, et al. [**From Planes to Corners: Multi-Purpose Primitive Detection in Unorganized 3D Point Clouds**](https://arxiv.org/pdf/2001.07360.pdf)[J]. arXiv preprint arXiv:2001.07360, **2020**.
    + <font color = blue>**ä»å¹³é¢åˆ°è§’ç‚¹ï¼šæ— ç»„ç»‡çš„ç‚¹äº‘ä¸­çš„å¤šç”¨é€”åŸºæœ¬ä½“æ£€æµ‹**</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ï¼Œ[ä»£ç å¼€æº](https://github.com/c-sommer/orthogonal-planes)
+ [x] **[4]** Zhao Y, Vela P A. [**Good feature matching: Towards accurate, robust VO/VSLAM with low latency**](https://arxiv.xilesou.top/pdf/2001.00714.pdf)[J]. IEEE Transactions on Robotics, **2019**, 7: 181800-181811.
    + <font color = blue>è‰¯å¥½çš„ç‰¹å¾åŒ¹é…ï¼šé¢å‘ä½å»¶è¿Ÿã€å‡†ç¡®ä¸”é²æ£’çš„ VO / VSLAM</font>
    + ä½æ²»äºšç†å·¥å­¦é™¢ï¼Œ[ä½œè€…ä¸»é¡µ](https://sites.google.com/site/zhaoyipu/home?authuser=0)ï¼Œ[**ä»£ç å¼€æº**](https://github.com/ivalab/FullResults_GoodFeature)ï¼ŒæœŸåˆŠä¸­ç§‘é™¢äºŒåŒº  JCR Q1
+ [ ] **[5]** Luo X, Tan Z, Ding Y. [**Accurate Line Reconstruction for Point and Line-Based Stereo Visual Odometry**](https://ieeexplore.ieee.org/abstract/document/8936964/)[J]. IEEE Access, **2019**, 7: 185108-185120.
    + <font color = blue>åŸºäºåŒç›®ç‚¹çº¿è§†è§‰é‡Œç¨‹è®¡çš„ç²¾ç¡®çº¿æ®µé‡æ„</font>
    + æµ™æ±Ÿå¤§å­¦è¶…å¤§è§„æ¨¡é›†æˆç”µè·¯è®¾è®¡ç ”ç©¶é™¢ï¼ŒIEEE Access å¼€æºæœŸåˆŠ
+ [ ] **[6]** Ma J, Wang X, He Y, et al. [**Line-Based Stereo SLAM by Junction Matching and Vanishing Point Alignment**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935194)[J]. IEEE Access, **2019**, 7: 181800-181811.
    + <font color = blue>é€šè¿‡èŠ‚ç‚¹åŒ¹é…ä¸æ¶ˆå¤±ç‚¹å¯¹é½çš„åŸºäºçº¿çš„åŒç›® SLAM</font>
    + æ­¦æ±‰å¤§å­¦ã€ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–æ‰€ï¼ŒIEEE Access å¼€æºæœŸåˆŠ
+ [ ] **[7]** é©¬è‰³é˜³, å¶æ¢“è±ª, åˆ˜å¤å, ç­‰. [**åŸºäºäº‹ä»¶ç›¸æœºçš„å®šä½ä¸å»ºå›¾ç®—æ³•: ç»¼è¿°**](http://www.aas.net.cn/cn/article/doi/10.16383/j.aas.c190550)[J]. è‡ªåŠ¨åŒ–å­¦æŠ¥, **2020**, 46: 1-11.
    + ä¸­å±±å¤§å­¦

#### 2. ä¼ æ„Ÿå™¨èåˆ

+ [x] **[8]** WEN, Shuhuan, et al. [**Joint optimization based on direct sparse stereo visual-inertial odometry**](https://link.springer.com/article/10.1007/s10514-019-09897-6). Autonomous Robots, **2020**, 1-19.
    + <font color = blue>åŸºäºç›´æ¥ç¨€ç–åŒç›®è§†è§‰æƒ¯å¯¼é‡Œç¨‹è®¡çš„è”åˆä¼˜åŒ–</font>
+ [x] **[9]** Chen C, Zhu H, Wang L, et al. [**A Stereo Visual-Inertial SLAM Approach for Indoor Mobile Robots in Unknown Environments Without Occlusions**](https://ieeexplore.ieee.org/abstract/document/8937551/)[J]. IEEE Access, **2019**, 7: 185408-185421.
    + <font color = blue>æ— é®æŒ¡æœªçŸ¥ç¯å¢ƒä¸­å®¤å†…ç§»åŠ¨æœºå™¨äººçš„åŒç›®è§†è§‰æƒ¯æ€§ SLAM æ–¹æ³•</font>
    + ä¸­å›½çŸ¿ä¸šå¤§å­¦ï¼Œ[**ä»£ç å¼€æº**](https://github.com/cumtxz/STCM-SLAM)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰ï¼ŒIEEE Access å¼€æºæœŸåˆŠ
+ [x] **[10]** Yan D, Wu C, Wang W, et al. [**Invariant Cubature Kalman Filter for Monocular Visual Inertial Odometry with Line Features**](https://arxiv.org/ftp/arxiv/papers/1912/1912.11749.pdf)[J]. arXiv preprint arXiv:1912.11749, **2019**.
    + <font color = blue>å•ç›®çº¿ç‰¹å¾è§†è§‰æƒ¯æ€§é‡Œç¨‹æ³•çš„ä¸å˜å®¹ç§¯å¡å°”æ›¼æ»¤æ³¢</font>
    + çŸ³å®¶åº„é“é“å¤§å­¦ã€åŒ—äº¬äº¤é€šå¤§å­¦

#### 3. Semantic/Deep SLAM

+ [ ] **[11]** XU, Jingao, et al. [**Edge Assisted Mobile Semantic Visual SLAM**](http://tns.thss.tsinghua.edu.cn/~jingao/papers/infocom20_edgeSLAM.pdf).
    + <font color = blue>è¾¹ç¼˜è¾…åŠ©ç§»åŠ¨è¯­ä¹‰è§†è§‰SLAM</font>
    + æ¸…åã€å¤§å·¥ã€å¾®è½¯
+ [ ] **[12]** ZHAO, Zirui, et al. [**Visual Semantic SLAM with Landmarks for Large-Scale Outdoor Environment**](https://arxiv.org/pdf/2001.01028.pdf). arXiv preprint arXiv:2001.01028, **2020**.
    + <font color = blue>ç”¨äºå¤§è§„æ¨¡å®¤å¤–ç¯å¢ƒçš„å…·æœ‰è·¯æ ‡çš„è§†è§‰è¯­ä¹‰ SLAM</font>
    + è¥¿å®‰äº¤å¤§ã€åŒ—äº¬äº¤å¤§
+ [x] **[13]** WANG, Li, et al. [**Object-Aware Hybrid Map for Indoor Robot Visual Semantic Navigation**](https://ieeexplore.ieee.org/abstract/document/8961495/). In: 2019 IEEE International Conference on Robotics and Biomimetics (**ROBIO**). IEEE, **2019**. p. 1166-1172.
    + <font color = blue>**ç”¨äºå®¤å†…æœºå™¨äººè§†è§‰è¯­ä¹‰å¯¼èˆªçš„å¯¹è±¡æ„ŸçŸ¥æ··åˆåœ°å›¾**</font>
    + ç‡•å±±å¤§å­¦ã€åŠ æ‹¿å¤§é˜¿å°”ä¼¯å¡”å¤§å­¦ã€ä¼¦æ•¦å¤§å­¦ï¼›Autonomous Robots æœŸåˆŠä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q1ï¼Œ IF 2.244
+ [ ] **[14]** Czarnowski, J., Laidlow, T., Clark, R., & Davison, A. J. (**2020**). [**DeepFactors: Real-Time Probabilistic Dense Monocular SLAM**](https://arxiv.org/pdf/2001.05049.pdf). IEEE Robotics and Automation Letters, 5(2), 721â€“728. doi:10.1109/lra.2020.2965415
    + <font color = blue>DeepFactorsï¼šå®æ—¶çš„æ¦‚ç‡å•ç›®ç¨ å¯† SLAM</font>
    + å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤ï¼Œ[ä»£ç å¼€æº](https://github.com/jczarnowski/DeepFactors)
+ [ ] **[15]** TRIPATHI, Nivedita; SISTU, Ganesh; YOGAMANI, Senthil. [**Trained Trajectory based Automated Parking System using Visual SLAM**](https://arxiv.org/pdf/2001.02161.pdf). arXiv preprint arXiv:2001.02161, **2020**.
    + <font color = blue>ä½¿ç”¨è§†è§‰ SLAM åŸºäºè½¨è¿¹è®­ç»ƒçš„è‡ªåŠ¨åœè½¦ç³»ç»Ÿ</font>
    + çˆ±å°”å…°æ³•é›·å¥¥è§†è§‰ç³»ç»Ÿå…¬å¸

#### 4. AR/VR/MR

+ [ ] **[16]** WANG, Cheng, et al. [**NEAR: The NetEase AR Oriented Visual Inertial Dataset**](https://ieeexplore.ieee.org/abstract/document/8951909). In: 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**. p. 366-371.
    + <font color = blue>**é¢å‘è§†è§‰æƒ¯å¯¼æ•°æ®é›†çš„ç½‘æ˜“ AR**</font>
    + ç½‘æ˜“ï¼Œ[**æ•°æ®é›†åœ°å€**](https://github.com/EZXR-Research/NEAR-VI-Dataset)
+ [ ] **[17]** HUANG, Ningsheng; CHEN, Jing; MIAO, Yuandong. [**Optimization for RGB-D SLAM Based on Plane Geometrical Constraint**](https://ieeexplore.ieee.org/abstract/document/8952003/). In: 2019 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**. p. 326-331.
    + <font color = blue>**åŸºäºå¹³é¢å‡ ä½•çº¦æŸä¼˜åŒ–çš„ RGB-D SLAM**</font>
    + åŒ—ç†å·¥
+ [ ] **[18]** WU, Yi-Chin; CHAN, Liwei; LIN, Wen-Chieh. [**Tangible and Visible 3D Object Reconstruction in Augmented Reality**](https://ieeexplore.ieee.org/abstract/document/8943645/). In: 2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR). IEEE, **2019**. p. 26-36.
    + <font color = blue>å¢å¼ºç°å®ä¸­æœ‰å½¢ä¸”å¯è§çš„ä¸‰ç»´ç‰©ä½“é‡å»º</font>
    + å°æ¹¾å›½ç«‹äº¤é€šå¤§å­¦è®¡ç®—æœºç§‘å­¦ç³»
+ [ ] **[19]** FEIGL, Tobias, et al. [**Localization Limitations of ARCore, ARKit, and Hololens in Dynamic Large-Scale Industry Environments**](https://www2.cs.fau.de/publication/download/GRAPP2020.pdf).
    + <font color = blue>å¤§å‹å·¥ä¸šåŠ¨æ€ç¯å¢ƒä¸­ ARCoreï¼ŒARKit å’Œ Hololens çš„å®šä½å±€é™æ€§</font>
    + å¾·å›½å¼—é‡Œå¾·é‡Œå¸Œ-äºšå†å±±å¤§å¤§å­¦
+ [ ] **[20]** Yang X, Yang J, He H, et al. [**A Hybrid 3D Registration Method of Augmented Reality for Intelligent Manufacturing**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8933068)[J]. IEEE Access, **2019**, 7: 181867-181883.
    + <font color = blue>ç”¨äºæ™ºèƒ½åˆ¶é€ çš„å¢å¼ºç°å®æ··åˆä¸‰ç»´æ³¨å†Œæ–¹æ³•</font>
    + å¹¿ä¸œå·¥ä¸šå¤§å­¦ï¼Œå¼€æºæœŸåˆŠ
    
#### 5. Others

+ [x] **[21]** Speciale P. [**Novel Geometric Constraints for 3D Computer Vision Applications**](https://www.research-collection.ethz.ch/handle/20.500.11850/380042)[D]. ETH Zurich, **2019**.
    + <font color = blue>**é€‚ç”¨äº 3D è®¡ç®—æœºè§†è§‰åº”ç”¨çš„æ–°å‹å‡ ä½•çº¦æŸ**</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥åšå£«å­¦ä½è®ºæ–‡ã€å¾®è½¯ï¼Œ[Google Scholar](https://scholar.google.com/citations?user=-LEhIh0AAAAJ&hl=zh-CN&oi=sra)
+ [x] **[22]** Patil V, Van Gansbeke W, Dai D, et al. [**Don't Forget The Past: Recurrent Depth Estimation from Monocular Video**](https://arxiv.org/pdf/2001.02613.pdf)[J]. arXiv preprint arXiv:2001.02613, **2020**.
    + <font color = blue>ä¸è¦å¿˜è®°è¿‡å»çš„ä¿¡æ¯:ä»å•ç›®è§†é¢‘ä¸­çš„é‡å¤æ·±åº¦ä¼°è®¡</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥ï¼Œ[**ä»£ç å¼€æº**](https://github.com/wvangansbeke/Recurrent-Depth-Estimation/branches)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [x] **[23]** CHIU, Hsu-kuang, et al. [**Probabilistic 3D Multi-Object Tracking for Autonomous Driving**](https://arxiv.org/pdf/2001.05673.pdf). arXiv preprint arXiv:2001.05673, **2020**.
    + <font color = blue>ç”¨äºè‡ªåŠ¨é©¾é©¶çš„æ¦‚ç‡ 3D å¤šç›®æ ‡è·Ÿè¸ª</font>
    + æ–¯å¦ç¦å¤§å­¦ã€ä¸°ç”°ç ”ç©¶æ‰€ï¼Œ[**ä»£ç å¼€æº**](https://github.com/eddyhkchiu/mahalanobis_3d_multi_object_tracking)
+ [x] **[24]** ZHOU, Boyu, et al. [**Robust Real-time UAV Replanning Using Guided Gradient-based Optimization and Topological Paths**](https://arxiv.org/pdf/1912.12644.pdf). arXiv preprint arXiv:1912.12644, **2019**.
    + <font color = blue>ä½¿ç”¨åŸºäºæ¢¯åº¦ä¼˜åŒ–å’Œæ‹“æ‰‘è·¯å¾„å¼•å¯¼è¿›è¡Œé²æ£’ä¸”å®æ—¶çš„æ— äººæœºé‡è§„åˆ’</font>
    + æ¸¯ç§‘å¤§ï¼Œä»£ç å¼€æºï¼š[Fast-Planner](https://github.com/HKUST-Aerial-Robotics/Fast-Planner), [
TopoTraj](https://github.com/HKUST-Aerial-Robotics/TopoTraj)
+ [x] **[25]** [**Object-based localization**](https://patents.google.com/patent/US20190392212A1/en)ï¼Œ**2019**.
    + <font color = blue>ä¸“åˆ©ï¼šåŸºäºç‰©ä½“çš„å®šä½</font>
+ [x] **[26]** [**Device pose estimation using 3d line clouds**](https://patents.google.com/patent/US20200005486A1/en)ï¼Œ**2019**.
    + <font color = blue>ä¸“åˆ©ï¼šä½¿ç”¨ 3D çº¿äº‘çš„è®¾å¤‡ä½å§¿ä¼°è®¡</font>

---

### 2019 å¹´ 12 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ23 ç¯‡ï¼‰
> **æœ¬æœŸ 23 ç¯‡è®ºæ–‡ï¼Œå…¶ä¸­ 5 é¡¹å¼€æºå·¥ä½œï¼›     
æ¯”è¾ƒæœ‰æ„æ€çš„æœ‰ TextSLAMã€VersaVIS å’Œå•ç›® 3D ç›®æ ‡æ£€æµ‹ã€‚**

#### 1. Geometric SLAM

+ [x] **[1]** Tanke J, Kwon O H, Stotko P, et al. [**Bonn Activity Maps: Dataset Description**](https://arxiv.org/pdf/1912.06354.pdf)[J]. arXiv preprint arXiv:1912.06354, **2019**.
    + <font color = blue>**åŒ…å«äººä½“è·Ÿè¸ªã€å§¿æ€å’Œç¯å¢ƒè¯­ä¹‰ä¸‰ç»´é‡å»ºçš„æ•°æ®é›†**</font>
    + æ³¢æ©å¤§å­¦ï¼Œ[é¡¹ç›®ã€æ•°æ®é›†ä¸»é¡µ](https://github.com/bonn-activity-maps/bonn_activity_maps)
+ [x] **[2]** An S, Che G, Zhou F, et al. [**Fast and Incremental Loop Closure Detection Using Proximity Graphs**](https://arxiv.org/pdf/1911.10752.pdf)[J]. arXiv preprint arXiv:1911.10752, **2019**.
    + <font color = blue>**ä½¿ç”¨é‚»è¿‘å›¾çš„å¿«é€Ÿå¢é‡å¼é—­ç¯æ£€æµ‹**</font>
    + äº¬ä¸œã€åŒ—èˆªï¼Œ[ä»£ç å¼€æº](https://github.com/AnshanTJU/FILD)
+ [x] **[3]** Li B, Zou D, Sartori D, et al. [**TextSLAM: Visual SLAM with Planar Text Features**](https://arxiv.org/pdf/1912.05002.pdf)[J]. arXiv preprint arXiv:1912.05002, 2019.
    + <font color = blue>**TextSLAMï¼šåŸºäºå¹³é¢æ–‡æœ¬çš„è§†è§‰ SLAM**</font>
    + ä¸Šäº¤é‚¹ä¸¹å¹³è€å¸ˆ
+ [x] **[4]** [Bundle Adjustment Revisited](https://arxiv.org/pdf/1912.03858.pdf)
    + <font color = blue>**å†è°ˆ BA**</font>
    + åŒ—äº¬å¤§å­¦
+ [ ] **[5]** Lange M, Raisch C, Schilling A. [LVO: Line only stereo Visual Odometry](https://ieeexplore.ieee.org/abstract/document/8911808)[C]//2019 International Conference on Indoor Positioning and Indoor Navigation (IPIN). IEEE, 1-8.
    + <font color = blue>åŒç›®çº¿ VO</font>
    + å›¾å®¾æ ¹å¤§å­¦
    + ç›¸å…³è®ºæ–‡ï¼šVakhitov A, Lempitsky V. [**Learnable Line Segment Descriptor for Visual SLAM**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8651490)[J]. IEEE Access, 2019, 7: 39923-39934. [ä»£ç å¼€æº](https://github.com/alexandervakhitov/lld-slam)
+ [ ] **[6]** Liu W, Mo Y, Jiao J. [**An efficient edge-feature constraint visual SLAM**](https://dl.acm.org/citation.cfm?id=3371455)[C]//Proceedings of the International Conference on Artificial Intelligence, Information Processing and Cloud Computing. ACM, **2019**: 13.
    + <font color = blue>ä¸€ç§é«˜æ•ˆçš„åŸºäºè¾¹ç¼˜ç‰¹å¾çº¦æŸçš„è§†è§‰ SLAM</font>
    + åŒ—é‚®
+ [ ] **[7]** Pan L, Wang P, Cao J, et al. [**Dense RGB-D SLAM with Planes Detection and Mapping**](https://ieeexplore.ieee.org/abstract/document/8927657)[C]//IECON 2019-45th Annual Conference of the IEEE Industrial Electronics Society. IEEE, **2019**, 1: 5192-5197.
    + <font color = blue>ä½¿ç”¨å¹³é¢æ£€æµ‹ä¸å»ºå›¾çš„ç¨ å¯† RGB-D SLAM</font>
    + æ–°åŠ å¡å›½ç«‹å¤§å­¦
+ [ ] **[8]** Ji S, Qin Z, Shan J, et al. [**Panoramic SLAM from a multiple fisheye camera rig**](https://www.sciencedirect.com/science/article/pii/S0924271619302758)[J]. ISPRS Journal of Photogrammetry and Remote Sensing, **2020**, 159: 169-183.
    + <font color = blue>å¤šé±¼çœ¼ç›¸æœºçš„å…¨æ™¯ SLAM</font>
    + æ­¦æ±‰å¤§å­¦
+ [ ] **[9]** Lecrosnier L, Boutteau R, Vasseur P, et al. [**Vision based vehicle relocalization in 3D line-feature map using Perspective-n-Line with a known vertical direction**]()[C]//2019 IEEE Intelligent Transportation Systems Conference (ITSC). IEEE, **2019**: 1263-1269.
    + <font color = blue>ä½¿ç”¨å…·æœ‰å·²çŸ¥å‚ç›´æ–¹å‘çš„é€è§†çº¿åœ¨ 3D çº¿ç‰¹å¾å›¾ä¸­è¿›è¡ŒåŸºäºè§†è§‰çš„è½¦è¾†é‡å®šä½</font>
    + è¯ºæ›¼åº•å¤§å­¦
+ [x] **[10]** de Souza MuÃ±oz M E, Menezes M C, de Freitas E P, et al. [**A Parallel RatSlam C++ Library Implementation**](https://link.springer.com/chapter/10.1007/978-3-030-36636-0_13)[C]//Latin American Workshop on Computational Neuroscience. Springer, Cham, **2019**: 173-183.
    + <font color = blue>å¹¶è¡Œ Rat SLAM C++ åº“çš„å®ç°</font>
    
---

#### 2. ä¼ æ„Ÿå™¨èåˆ

+ [x] **[11]** Tschopp F, Riner M, Fehr M, et al. [**VersaVIS: An Open Versatile Multi-Camera Visual-Inertial Sensor Suite**](https://arxiv.org/pdf/1912.02469.pdf)[J]. arXiv preprint arXiv:1912.02469, **2019**.
    + <font color = blue>**VersaVISï¼šå¼€æºå¤šåŠŸèƒ½å¤šç›¸æœºçš„è§†è§‰æƒ¯æ€§ä¼ æ„Ÿå™¨å¥—ä»¶**</font>
    + è‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢ï¼Œ[**ä»£ç å¼€æº**](https://github.com/ethz-asl/versavis)
+ [x] **[12]** Geneva P, Eckenhoff K, Lee W, et al. [**Openvins: A research platform for visual-inertial estimation**](https://pdfs.semanticscholar.org/cb63/60f21255834297e32826bff6366a769b49e9.pdf)[C]//IROS 2019 Workshop on Visual-Inertial Navigation: Challenges and Applications, Macau, China. **IROS 2019**.
    + <font color = blue>**Openvinsï¼šç”¨äºè§†è§‰æƒ¯æ€§ä¼°è®¡çš„ç ”ç©¶å¹³å°**</font>
    + ç‰¹æ‹‰åå¤§å­¦ï¼Œ[**ä»£ç å¼€æº**](https://github.com/rpng/open_vins)
+ [ ] **[13]** Barrau A, Bonnabel S. [**A Mathematical Framework for IMU Error Propagation with Applications to Preintegration**](https://hal-mines-paristech.archives-ouvertes.fr/hal-02394774/document)[J]. **2019**.
    + <font color = blue>IMUé”™è¯¯ä¼ æ’­çš„æ•°å­¦æ¡†æ¶åŠå…¶åœ¨é¢„ç§¯åˆ†ä¸­çš„åº”ç”¨</font>
    + PSL Research University
+ [ ] **[14]** Ke T, Wu K J, Roumeliotis S I. RISE-SLAM: A Resource-aware Inverse Schmidt Estimator for SLAM[C]. **IROS 2019**.
    + <font color = blue>ç”¨äº SLAM çš„èµ„æºæ„ŸçŸ¥çš„é€†æ–½å¯†ç‰¹ä¼°è®¡å™¨</font>
    
---

#### 3. Semantic/Deep SLAM

+ [x] **[15]** Dong Y, Wang S, Yue J, et al. [**A Novel Texture-Less Object Oriented Visual SLAM System**](https://ieeexplore.ieee.org/abstract/document/8901152)[J]. IEEE Transactions on Intelligent Transportation Systems, **2019**.
    + <font color = blue>**ä¸€ç§æ–°å‹çš„é¢å‘ä½çº¹ç†ç‰©ä½“çš„è§†è§‰ SLAM ç³»ç»Ÿ**</font>
    + åŒæµå¤§å­¦ï¼ŒæœŸåˆŠ ä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 6
+ [x] **[16]** Peng J, Shi X, Wu J, et al. [**An Object-Oriented Semantic SLAM System towards Dynamic Environments for Mobile Manipulation**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8868371)[C]//2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (**AIM**). IEEE, **2019**: 199-204.
    + <font color = blue>**ç”¨äºåŠ¨æ€ç¯å¢ƒä¸­ç§»åŠ¨æœºå™¨äººæŠ“å–çš„é¢å‘ç‰©ä½“çš„è¯­ä¹‰ SLAM**</font>
    + ä¸Šæµ·äº¤å¤§æœºæ¢°å­¦é™¢ï¼ŒAIMï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[17]** Kim U H, Kim S H, Kim J H. [**SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation**](https://arxiv.org/pdf/1911.05939.pdf)[J]. arXiv preprint arXiv:1911.05939, **2019**.
    + <font color = blue>SimVODISï¼šåŒæ—¶è¿›è¡Œè§†è§‰é‡Œç¨‹è®¡ã€ç›®æ ‡æ£€æµ‹å’Œè¯­ä¹‰åˆ†å‰²</font>
    + éŸ©å›½é«˜ç­‰ç§‘å­¦æŠ€æœ¯é™¢
+ [ ] **[18]** Howard Mahe, Denis Marraud, Andrew I. Comport. [**Real-time RGB-D semantic keyframe SLAM based on image segmentation learning from industrial CAD models**](https://hal.archives-ouvertes.fr/hal-02391499/file/ICAR19_0187_FI.pdf). International Conference on Advanced Robotics, Dec 2019, Belo Horizonte, Brazil. ffhal-02391499
    + <font color = blue>åŸºäºå·¥ä¸š CAD æ¨¡å‹å›¾åƒåˆ†å‰²å­¦ä¹ çš„å®æ—¶ RGB-D è¯­ä¹‰å…³é”®å¸§ SLAM</font>
    + ç›¸å…³å·¥ä½œï¼š**åŸºäºç¨ å¯†ç±»çº§åˆ«åˆ†å‰²çš„ä»…ä½¿ç”¨è¯­ä¹‰çš„è§†è§‰é‡Œç¨‹è¡¨** MahÃ© H, Marraud D, Comport A I. [**Semantic-only Visual Odometry based on dense class-level segmentation**](https://hal.archives-ouvertes.fr/hal-01874544/document)[C]//2018 24th International Conference on Pattern Recognition (ICPR). IEEE, **2018**: 1989-1995.

---

#### 4. AR/VR/MR

+ [x] **[19]** Chandu S, Bhavan Jasani G J, Manglik A. [**Deleting 3D Objects in Augmented Reality using RGBD-SLAM**](https://bhavanj.github.io/files/SLAM_Final_Report.pdf)[J].**2019**
    + <font color = blue>ä½¿ç”¨ RGBD-SLAM åœ¨å¢å¼ºç°å®ä¸­åˆ é™¤ 3D å¯¹è±¡</font>
    + äºšé©¬é€Š
+ [x] **[20]** Sartipi K, DuToit R C, Cobar C B, et al. [**Decentralized visual-inertial localization and mapping on mobile devices for augmented reality**](http://mars.cs.umn.edu/tr/decentralizedvi19.pdf)[R]. Google, Tech. Rep., Aug. **2019**.[Online]. Available: http://mars. cs. umn. edu/tr/decentralizedvi19. pdf.
    + <font color = blue>ç”¨äºç§»åŠ¨è®¾å¤‡å¢å¼ºç°å®çš„åˆ†å¸ƒå¼è§†è§‰æƒ¯å¯¼å®šä½ä¸å»ºå›¾</font>
    + æ˜å°¼è‹è¾¾å¤§å­¦

---

#### 5. Others

+ [x] **[21]** Yan Z, Zha H. [**Flow-based SLAM: From geometry computation to learning**](https://www.sciencedirect.com/science/article/pii/S2096579619300622)[J]. Virtual Reality & Intelligent Hardware, **2019**, 1(5): 435-460.
    + <font color = blue>**åŸºäºæµçš„ SLAMï¼šä»å‡ ä½•è®¡ç®—åˆ°å­¦ä¹ çš„æ–¹æ³•**</font>
    + åŒ—äº¬å¤§å­¦ï¼Œ[Google Scholar](https://scholar.google.com/citations?user=Rawfmp4AAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[22]** Li J, Liu Y, Yuan X, et al. [**Depth Based Semantic Scene Completion With Position Importance Aware Loss**](https://ieeexplore.ieee.org/abstract/document/8902045)[J]. IEEE Robotics and Automation Letters, **2019**, 5(1): 219-226.
    + <font color = blue>å…·æœ‰ä½ç½®é‡è¦æ€§æ„ŸçŸ¥æŸå¤±çš„åŸºäºæ·±åº¦çš„è¯­ä¹‰åœºæ™¯ç†è§£</font>
    + å—äº¬å¤§å­¦ï¼Œ[**ä»£ç å¼€æº**](https://github.com/UniLauX/PALNet)ï¼Œ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=j-LAMcMh0yg&feature=youtu.be)
+ [x] **[23]** Simonelli A, BulÃ² S R R, Porzi L, et al. [**Disentangling Monocular 3D Object Detection**](https://arxiv.org/pdf/1905.12365.pdf)[J]. arXiv preprint arXiv:1905.12365, **2019**.
    + <font color = blue>**æ­ç§˜å•ç›® 3D ç›®æ ‡æ£€æµ‹**</font>
    + æ„å¤§åˆ©ç‰¹ä¼¦æ‰˜å¤§å­¦ï¼Œä½œè€…å…¶ä»–è®ºæ–‡
        + ä½¿ç”¨è™šæ‹Ÿç›¸æœºè¿›è¡Œå•é˜¶æ®µå•ç›® 3D ç›®æ ‡æ£€æµ‹ï¼šSimonelli A, BulÃ² S R, Porzi L, et al. [**Single-Stage Monocular 3D Object Detection with Virtual Cameras**](https://arxiv.org/pdf/1912.08035.pdf)[J]. arXiv preprint arXiv:1912.08035, **2019**.

---

### 2019 å¹´ 11 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰

#### 1. Geometric SLAM

+ [x] **[1]** Jatavallabhula K M, Iyer G, Paull L. [**gradSLAM: Dense SLAM meets Automatic Differentiation**](https://arxiv.org/pdf/1910.10672.pdf)[J]. arXiv preprint arXiv:1910.10672, **2019**.
    + <font color = blue>**è‡ªåŠ¨åŒºåˆ†çš„ç¨ å¯† SLAM**</font>
    + [é¡¹ç›®ä¸»é¡µ](http://montrealrobotics.ca/gradSLAM/)ï¼Œ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=2ygtSJTmo08&feature=youtu.be)ï¼Œ[**ä»£ç å¼€æº**](https://github.com/montrealrobotics/gradSLAM)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
    + åŠ æ‹¿å¤§è’™ç‰¹å¡å°”å¤§å­¦ï¼ˆ[å®éªŒå®¤ä¸»é¡µ](http://montrealrobotics.ca/)ï¼‰ï¼ŒCMU
+ [x] **[2]** Lee S J, Hwang S S. [**Elaborate Monocular Point and Line SLAM With Robust Initialization**](http://openaccess.thecvf.com/content_ICCV_2019/papers/Lee_Elaborate_Monocular_Point_and_Line_SLAM_With_Robust_Initialization_ICCV_2019_paper.pdf)[C]//**ICCV 2019**: 1121-1129.
    + <font color = blue>**å…·æœ‰é²æ£’åˆå§‹åŒ–çš„å•ç›®ç‚¹çº¿ SLAM**</font>
    + éŸ©å›½éŸ©ä¸œå›½é™…å¤§å­¦
+ [ ] **[3]** Wen F, Ying R, Gong Z, et al. [**Efficient Algorithms for Maximum Consensus Robust Fitting**](https://ieeexplore.ieee.org/abstract/document/8870243)[J]. IEEE Transactions on Robotics, **2019**.
    + <font color = blue>æœ€å¤§ä¸€è‡´æ€§ç¨³å¥æ‹Ÿåˆçš„æœ‰æ•ˆç®—æ³•</font>
    + æœŸåˆŠ ä¸­ç§‘é™¢äºŒåŒºï¼ŒJCRQ1ï¼ŒIF 1.038ï¼Œ[ä»£ç å¼€æº](https://github.com/FWen/emc)
    + ä¸Šæµ·äº¤é€šå¤§å­¦ç”µå­å·¥ç¨‹ç³»/è„‘å¯å‘å¼åº”ç”¨æŠ€æœ¯ä¸­å¿ƒ
+ [ ] **[4]** Civera J, Lee S H. [**RGB-D Odometry and SLAM**](https://link.springer.com/chapter/10.1007/978-3-030-28603-3_6#citeas)[M]//RGB-D Image Analysis and Processing. Springer, Cham, **2019**: 117-144.
    + <font color = blue>RGB-D SLAM ä¸é‡Œç¨‹è®¡</font>
    + ä¸“è‘—
+ [ ] **[5]** Wang H, Li J, Ran M, et al. [**Fast Loop Closure Detection via Binary Content**](https://ieeexplore.ieee.org/abstract/document/8899937)[C]//2019 IEEE 15th International Conference on Control and Automation (**ICCA**). IEEE, **2019**: 1563-1568.
    + <font color = blue>é€šè¿‡äºŒè¿›åˆ¶å†…å®¹è¿›è¡Œå¿«é€Ÿé—­ç¯æ£€æµ‹</font>
    + å—æ´‹ç†å·¥å¤§å­¦ï¼ŒICCV ä¼šè®® 
+ [ ] **[6]** Liu W, Wu S, Wu Z, et al. [**Incremental Pose Map Optimization for Monocular Vision SLAM Based on Similarity Transformation**](https://www.mdpi.com/1424-8220/19/22/4945/htm)[J]. Sensors, **2019**, 19(22): 4945.
    + <font color = blue>åŸºäºç›¸ä¼¼åº¦å˜æ¢çš„å•ç›® SLAM å¢é‡å¼ä½å§¿å›¾ä¼˜åŒ–</font>
    + åŒ—èˆªï¼Œå¼€æºæœŸåˆŠ
+ [ ] **[7]** Wang S, Yue J, Dong Y, et al. [**A synthetic dataset for Visual SLAM evaluation**](https://www.sciencedirect.com/science/article/pii/S0921889019301009#appSB)[J]. Robotics and Autonomous Systems, **2019**: 103336.
    + <font color = blue>ç”¨äºè§†è§‰ SLAM è¯„ä¼°çš„åˆæˆæ•°æ®é›†</font>
    + åŒæµå¤§å­¦ï¼ŒæœŸåˆŠä¸­ç§‘é™¢ä¸‰åŒºï¼Œ JCR Q2ï¼Œ IF 2.809
+ [ ] **[8]** Han B, Li X, Yu Q, et al. [**A Novel Visual Odometry Aided by Vanishing Points in the Manhattan World**](https://www.researchgate.net/profile/Huimin_Lu/publication/336605825_A_Novel_Visual_Odometry_Aided_by_Vanishing_Points_in_the_Manhattan_World/links/5da85553a6fdccdad54c40ae/A-Novel-Visual-Odometry-Aided-by-Vanishing-Points-in-the-Manhattan-World.pdf)[J].
    + <font color = blue>æ›¼å“ˆé¡¿ä¸–ç•Œæ¶ˆå¤±ç‚¹è¾…åŠ©çš„æ–°å‹è§†è§‰é‡Œç¨‹è®¡</font>
    + å›½é˜²ç§‘å¤§ï¼Œä½œè€… [Google Scholar](https://scholar.google.com/citations?user=cp-6u7wAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[9]** Yuan Z, Zhu D, Chi C, et al. [**Visual-Inertial State Estimation with Pre-integration Correction for Robust Mobile Augmented Reality**](http://delivery.acm.org/10.1145/3360000/3351079/p1410-yuan.pdf?ip=219.216.73.1&id=3351079&acc=ACTIVE%20SERVICE&key=BF85BBA5741FDC6E%2E4183B12E3311CD37%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1574657773_86a30bc436c42e34852cca493a18c66c)[C]//Proceedings of the 27th ACM International Conference on Multimedia. ACM, **2019**: 1410-1418.
    + <font color = blue>ç”¨äºé²æ£’çš„ç§»åŠ¨å¢å¼ºç°å®ä¸­åŸºäºé¢„ç§¯åˆ†æ ¡æ­£çš„è§†è§‰æƒ¯æ€§çŠ¶æ€ä¼°è®¡</font>
    + åä¸­ç§‘å¤§ï¼Œä¼šè®® ACM MMï¼šCCF A ç±»ä¼šè®®
+ [ ] **[10]** Zhong D, Han L, Fang L. [**iDFusion: Globally Consistent Dense 3D Reconstruction from RGB-D and Inertial Measurements**](https://dl.acm.org/citation.cfm?id=3351085)[C]//Proceedings of the 27th ACM International Conference on Multimedia. ACM, **2019**: 962-970.
    + <font color = blue>iDFusionï¼šRGB-D å’Œæƒ¯å¯¼çš„å…¨å±€ä¸€è‡´æ€§ç¨ å¯†ä¸‰ç»´å»ºå›¾</font>
    + æ¸…åå¤§å­¦ã€æ¸¯ç§‘ï¼Œä¼šè®® ACM MMï¼šCCF A ç±»ä¼šè®®ï¼Œ[Google Scholar](https://scholar.google.com/citations?user=fI5pZ_cAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[11]** Duhautbout T, Moras J, Marzat J. [**Distributed 3D TSDF Manifold Mapping for Multi-Robot Systems**](https://ieeexplore.ieee.org/abstract/document/8870930)[C]//2019 European Conference on Mobile Robots (**ECMR**). IEEE, **2019**: 1-8.
    + <font color = blue>å¤šæœºå™¨äººç³»ç»Ÿçš„åˆ†å¸ƒå¼ä¸‰ç»´ TSDF æµå½¢å»ºå›¾</font>
    + TSDF å¼€æºåº“ï¼šhttps://github.com/personalrobotics/OpenChisel

---

#### 2. Semantic/Deep SLAM

+ [x] **[12]** Schorghuber M, Steininger D, Cabon Y, et al. [**SLAMANTIC-Leveraging Semantics to Improve VSLAM in Dynamic Environments**](http://openaccess.thecvf.com/content_ICCVW_2019/papers/DL4VSLAM/Schorghuber_SLAMANTIC_-_Leveraging_Semantics_to_Improve_VSLAM_in_Dynamic_Environments_ICCVW_2019_paper.pdf)[C]//**ICCV Workshops**. **2019**: 0-0.
    + <font color = blue>**SLAMANTICï¼šåœ¨åŠ¨æ€ç¯å¢ƒä¸­åˆ©ç”¨è¯­ä¹‰æ¥æ”¹å–„VSLAM**</font>
    + å¥¥åœ°åˆ©ç†å·¥å­¦é™¢, [**ä»£ç å¼€æº**](https://github.com/mthz/slamantic)
+ [ ] **[13]** Lee C Y, Lee H, Hwang I, et al. [**Spatial Perception by Object-Aware Visual Scene Representation**](http://openaccess.thecvf.com/content_ICCVW_2019/papers/DL4VSLAM/Lee_Spatial_Perception_by_Object-Aware_Visual_Scene_Representation_ICCVW_2019_paper.pdf)[C]//**ICCV Workshops 2019**: 0-0.
    + <font color = blue>ç”±ç‰©ä½“è§†è§‰åœºæ™¯è¡¨ç¤ºçš„ç©ºé—´æ„ŸçŸ¥</font>
    + é¦–å°”å¤§å­¦
+ [ ] **[14]** Peng J, Shi X, Wu J, et al. [**An Object-Oriented Semantic SLAM System towards Dynamic Environments for Mobile Manipulation**](https://ieeexplore.ieee.org/abstract/document/8868371)[C]//2019 IEEE/ASME International Conference on Advanced Intelligent Mechatronics (**AIM**). IEEE, **2019**: 199-204.
    + <font color = blue>ç”¨äºåŠ¨æ€ç¯å¢ƒç§»åŠ¨æ“ä½œçš„ç‰©ä½“çº§è¯­ä¹‰ SLAM ç³»ç»Ÿ</font>
    + ä¸Šæµ·äº¤å¤§ï¼ŒAIMï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[15]** Cui L, Ma C. [**SOF-SLAM: A semantic visual SLAM for Dynamic Environments**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8894002)[J]. IEEE Access, **2019**.
    + <font color = blue>ä¸€ç§ç”¨äºåŠ¨æ€ç¯å¢ƒçš„è¯­ä¹‰è§†è§‰é‡Œç¨‹è®¡</font>
    + åŒ—èˆªï¼ŒIEEE Access å¼€æºæœŸåˆŠ
+ [ ] **[16]** Zheng L, Tao W. [**Semantic Object and Plane SLAM for RGB-D Cameras**](https://link.springer.com/chapter/10.1007/978-3-030-31726-3_12)[C]//Chinese Conference on Pattern Recognition and Computer Vision (**PRCV**). Springer, Cham, **2019**: 137-148.
    + <font color = blue>RGB-D ç‰©ä½“è¯­ä¹‰ä¸å¹³é¢çº§ SLAM</font>
    + å¹³é¢åˆ†å‰²é‡‡ç”¨ [PEAC](https://github.com/symao/PEAC)ï¼ŒPRCV ç¬¬äºŒå±Šä»Šå¹´åœ¨è¥¿å®‰ä¸¾åŠçš„é‚£ä¸ª
    + åä¸­ç§‘å¤§
+ [ ] **[17]** Kim U H, Kim S H, Kim J H. [**SimVODIS: Simultaneous Visual Odometry, Object Detection, and Instance Segmentation**](https://arxiv.org/abs/1911.05939)[J]. arXiv preprint arXiv:1911.05939, **2019**.
    + <font color = blue>SimVODISï¼šåŒæ—¶è¿›è¡Œè§†è§‰é‡Œç¨‹è®¡ã€ç›®æ ‡æ£€æµ‹å’Œå®ä¾‹åˆ†å‰²</font>

---

### 2019 å¹´ 10 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ22 ç¯‡ï¼‰

#### 1. Geometric SLAM

+ [x] **[1]** Sumikura S, Shibuya M, Sakurada K. [**OpenVSLAM: A Versatile Visual SLAM Framework**](https://arxiv.org/pdf/1910.01122.pdf)[J]. arXiv preprint arXiv:1910.01122, **2019**.
    + <font color = blue>**OpenVSLAM: é€šç”¨çš„è§†è§‰ SLAM æ¡†æ¶**</font>
    + [ä»£ç å¼€æº](https://github.com/xdspacelab/openvslam)
    + æ—¥æœ¬å›½å®¶å…ˆè¿›å·¥ä¸šç§‘å­¦æŠ€æœ¯ç ”ç©¶æ‰€ï¼Œå…¶ä»–å·¥ä½œï¼šYokozuka M, Oishi S, Simon T, et al. [**VITAMIN-E: VIsual Tracking And Mapping with Extremely Dense Feature Points**](https://arxiv.org/pdf/1904.10324.pdf)[J]. arXiv preprint arXiv:1904.10324, **2019**.
+ [x] **[2]** Chen Y, Huang S, Fitch R, et al. [**On-line 3D active pose-graph SLAM based on key poses using graph topology and sub-maps**](https://ieeexplore.ieee.org/abstract/document/8793632)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 169-175.
    + <font color = blue>**ä½¿ç”¨å›¾å½¢æ‹“æ‰‘å’Œå­å›¾çš„åŸºäºå…³é”®å§¿åŠ¿çš„åœ¨çº¿ 3D æ´»è·ƒä½å§¿å›¾SLAM**</font>
    + æ‚‰å°¼ç§‘æŠ€å¤§å­¦
+ [ ] **[3]** Pfrommer B, Daniilidis K. [**TagSLAM: Robust SLAM with Fiducial Markers**](https://arxiv.org/pdf/1910.00679.pdf)[J]. arXiv preprint arXiv:1910.00679, **2019**.
    + <font color = blue>**TagSLAMMï¼šå…·æœ‰åŸºå‡†æ ‡è®°çš„é²æ£’ SLAM**</font>
    + å®¾å¤•æ³•å°¼äºšå¤§å­¦é€šç”¨æœºå™¨äººï¼Œè‡ªåŠ¨åŒ–ï¼Œæ„Ÿåº”å’Œæ„ŸçŸ¥å®éªŒå®¤ï¼Œ[é¡¹ç›®ä¸»é¡µ](https://berndpfrommer.github.io/tagslam_web/)
+ [ ] **[4]** Lin T Y, Clark W, Eustice R M, et al. [**Adaptive Continuous Visual Odometry from RGB-D Images**](https://arxiv.org/pdf/1910.00713.pdf)[J]. arXiv preprint arXiv:1910.00713, **2019**.
    + <font color = blue>RGB-D å›¾åƒçš„è‡ªé€‚åº”è¿ç»­è§†è§‰é‡Œç¨‹è®¡</font>
    + å¯†è¥¿æ ¹å¤§å­¦
+ [ ] **[5]** Y Yang, P Geneva, K Eckenhoff, G Huang. [**Visual-Inertial Odometry with Point and Line Features**](https://www.researchgate.net/profile/Yulin_Yang3/publication/335821879_Visual-Inertial_Odometry_with_Point_and_Line_Features/links/5d7d3ac892851c87c389c719/Visual-Inertial-Odometry-with-Point-and-Line-Features.pdf), **2019**.
    + <font color = blue>ç‚¹çº¿ VIO</font>
    + ç‰¹æ‹‰åå¤§å­¦
+ [ ] **[6]** Tarrio J J, Smitt C, Pedre S. [**SE-SLAM: Semi-Dense Structured Edge-Based Monocular SLAM**](https://arxiv.org/pdf/1909.03917.pdf)[J]. arXiv preprint arXiv:1909.03917, **2019**.
    + <font color = blue>SE-SLAMï¼šåŸºäºè¾¹çš„å•ç›®åŠç¨ å¯† SLAM</font>
    + é˜¿æ ¹å»·å·´å°”å¡ç½—ç ”ç©¶æ‰€
+ [ ] **[7]** Wu X, Pradalier C. [**Robust Semi-Direct Monocular Visual Odometry Using Edge and Illumination-Robust Cost**](https://arxiv.org/pdf/1909.11362.pdf)[J]. arXiv preprint arXiv:1909.11362, **2019**.
    + <font color = blue>åˆ©ç”¨è¾¹ç¼˜å’Œå…‰ç…§é²æ£’æˆæœ¬çš„å•ç›®åŠç›´æ¥æ³•è§†è§‰é‡Œç¨‹è®¡</font>
    + ä½æ²»äºšç†å·¥å­¦é™¢
+ [ ] **[8]** Pan Z, Chen H, Li S, et al. [**ClusterMap Building and Relocalization in Urban Environments for Unmanned Vehicles**](https://www.mdpi.com/1424-8220/19/19/4252/htm)[J]. Sensors, **2019**, 19(19): 4252.
    + <font color = blue>æ— äººé©¾é©¶è½¦è¾†åœ¨åŸå¸‚ç¯å¢ƒä¸­çš„ ClusterMap æ„å»ºå’Œé‡å®šä½</font>
    + å“ˆå·¥å¤§æ·±åœ³ï¼Œæ¸¯ä¸­æ–‡ï¼ŒæœŸåˆŠ Sensorsï¼šå¼€æºæœŸåˆŠï¼Œä¸­ç§‘é™¢ä¸‰åŒº JCR Q2Q3 IF 3.014
+ [ ] **[9]** Zhang M, Zuo X, Chen Y, et al. [**Localization for Ground Robots: On Manifold Representation, Integration, Re-Parameterization, and Optimization**](https://arxiv.org/pdf/1909.03423.pdf)[J]. arXiv preprint arXiv:1909.03423, **2019**.
    + <font color = blue>åœ°é¢æœºå™¨äººçš„å®šä½ï¼šæµå½¢è¡¨ç¤ºï¼Œç§¯åˆ†ï¼Œé‡æ–°å‚æ•°åŒ–å’Œä¼˜åŒ–</font>
    + é˜¿é‡Œå·´å·´äººå·¥æ™ºèƒ½å®éªŒå®¤
+ [ ] **[10]** Kirsanov P, Gaskarov A, Konokhov F, et al. [**DISCOMAN: Dataset of Indoor SCenes for Odometry, Mapping And Navigation**](https://arxiv.org/pdf/1909.12146.pdf)[J]. arXiv preprint arXiv:1909.12146, **2019**.
    + <font color = blue>DISCOMANï¼šç”¨äºé‡Œç¨‹è®¡ã€åˆ¶å›¾å’Œå¯¼èˆªçš„å®¤å†…åœºæ™¯æ•°æ®é›†</font>
    + ä¸‰æ˜Ÿ AI ä¸­å¿ƒï¼Œæ•°æ®é›†éšè®ºæ–‡æ­£å¼å‘è¡¨æ”¾å‡º

---

#### 2. Semantic/Deep SLAM

+ [x] **[11]** Pire T, Corti J, Grinblat G. [**Online Object Detection and Localization on Stereo Visual SLAM System**](https://www.researchgate.net/profile/Taihu_Pire/publication/335432416_Online_Object_Detection_and_Localization_on_Stereo_Visual_SLAM_System/links/5d663a14a6fdccf343f93830/Online-Object-Detection-and-Localization-on-Stereo-Visual-SLAM-System.pdf)[J]. Journal of Intelligent & Robotic Systems, **2019**: 1-10.
    + <font color = blue>**åœ¨çº¿ç›®æ ‡æ£€æµ‹å’Œå®šä½çš„åŒç›®è§†è§‰ SLAM**</font>
    + é˜¿æ ¹å»·å›½é™…ä¿¡æ¯ç§‘å­¦ä¸­å¿ƒï¼Œ[**ä»£ç å¼€æº**](https://github.com/CIFASIS/object-detection-sptam)
+ [x] **[12]** Rosinol A, Abate M, Chang Y, et al. [**Kimera: an Open-Source Library for Real-Time Metric-Semantic Localization and Mapping**](https://arxiv.org/pdf/1910.02490.pdf)[J]. arXiv preprint arXiv:1910.02490, **2019**.
    + <font color = blue>**Kimeraï¼šç”¨äºå®æ—¶åº¦é‡-è¯­ä¹‰å®šä½å’Œå»ºå›¾çš„å¼€æºåº“**</font>
    + MITï¼Œ[**ä»£ç å¼€æº**](https://github.com/MIT-SPARK/Kimera)ï¼Œ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=3lVD0i-5p10)
+ [ ] **[13]** Feng Q, Meng Y, Shan M, et al. [**Localization and Mapping using Instance-specific Mesh Models**](https://natanaso.github.io/ref/Feng_DeformableMeshModel_IROS19.pdf)[J].**IROS 2019**
    + <font color = blue>ä½¿ç”¨ç‰¹å®šå®ä¾‹ç½‘æ ¼æ¨¡å‹è¿›è¡Œå®šä½å’Œå»ºå›¾</font>
    + åŠ å·å¤§å­¦åœ£åœ°äºšå“¥åˆ†æ ¡è¯­å¢ƒæœºå™¨äººç ”ç©¶æ‰€ï¼Œ[è¯¾é¢˜ç»„](https://existentialrobotics.org/pages/publications.html)
+ [ ] **[14]** Liao Z, Shi J, Qi X, et al. [**Coarse-To-Fine Visual Localization Using Semantic Compact Map**](https://arxiv.org/pdf/1910.04936.pdf)[J]. arXiv preprint arXiv:1910.04936, **2019**.
    + <font color = blue>ä½¿ç”¨è¯­ä¹‰ç´§å‡‘å›¾çš„ä»ç²—ç³™åˆ°ç²¾ç»†çš„è§†è§‰å®šä½</font>
    + åŒ—èˆªï¼Œface++
+ [x] **[15]** Doherty K, Baxter D, Schneeweiss E, et al. [**Probabilistic Data Association via Mixture Models for Robust Semantic SLAM**](https://arxiv.org/pdf/1909.11213.pdf)[J]. arXiv preprint arXiv:1909.11213, **2019**.
    + <font color = blue>**é²æ£’çš„è¯­ä¹‰ SLAM ä¸­æ··åˆæ¨¡å‹çš„æ¦‚ç‡æ•°æ®å…³è”**</font>
    + MITï¼Œå¥½åƒå°±æ˜¯ä¹‹å‰ [ICRA 2019 å¤šæ¨¡æ€æ¦‚ç‡æ•°æ®å…³è”](https://marinerobotics.mit.edu/sites/default/files/doherty_icra2019_revised.pdf)
+ [ ] **[16]** Jung E, Yang N, Cremers D. [**Multi-Frame GAN: Image Enhancement for Stereo Visual Odometry in Low Light**](https://arxiv.org/pdf/1910.06632.pdf)[J]. arXiv preprint arXiv:1910.06632, **2019**.
    + <font color = blue>Multi-Frame GANï¼šå¼±å…‰ç…§åŒç›®è§†è§‰é‡Œç¨‹è®¡çš„å›¾åƒå¢å¼º</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ã€æ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦ï¼ŒArtisense è‡ªåŠ¨é©¾é©¶å…¬å¸ï¼ŒLSDã€DSO ä½œè€…ï¼Œ[Google Scholar](https://scholar.google.com/citations?user=cXQciMEAAAAJ&hl=zh-CN&oi=sra)
+ [x] **[17]** Yu F, Shang J, Hu Y, et al. [**NeuroSLAM: a brain-inspired SLAM system for 3D environments**](https://www.researchgate.net/profile/Fangwen_Yu/publication/336164484_NeuroSLAM_a_brain-inspired_SLAM_system_for_3D_environments/links/5d95f38d458515c1d3908a20/NeuroSLAM-a-brain-inspired-SLAM-system-for-3D-environments.pdf)[J]. Biological Cybernetics, **2019**: 1-31.
    + <font color = blue>**NeuroSLAMï¼šé’ˆå¯¹ 3D ç¯å¢ƒçš„è„‘å¯å‘å¼ SLAM ç³»ç»Ÿ**</font>
    + æ˜†å£«å…°ç§‘æŠ€å¤§å­¦ï¼ŒRat SLAM ä½œè€…ï¼Œ[**ä»£ç å¼€æº**](https://github.com/cognav/NeuroSLAM)
+ [ ] **[18]** Zeng T, Si B. [**A Brain-Inspired Compact Cognitive Mapping System**](https://arxiv.org/pdf/1910.03913.pdfv)[J]. arXiv preprint arXiv:1910.03913, **2019**.
    + <font color = blue>è„‘å¯å‘çš„ç´§å‡‘å‹è®¤çŸ¥åœ°å›¾ç³»ç»Ÿ</font>
    + æ²ˆè‡ªæ‰€

---

#### 3. Learning others

+ [x] **[19]** Zhou Y, Qi H, Huang J, et al. [**NeurVPS: Neural Vanishing Point Scanning via Conic Convolution**](https://arxiv.org/pdf/1910.06316.pdf)[J]. arXiv preprint arXiv:1910.06316, **2019**.
    + <font color = blue>**NeurVPSï¼šé€šè¿‡åœ†é”¥å·ç§¯çš„ç¥ç»æ¶ˆå¤±ç‚¹æ‰«æ**</font>
    + åŠ å·ä¼¯å…‹åˆ©ï¼Œ[ä»£ç å¼€æº](https://github.com/zhou13/neurvps)
+ [ ] **[20]** Alhashim I, Wonka P. [**High Quality Monocular Depth Estimation via Transfer Learning**]()[J]. arXiv preprint arXiv:1812.11941, **2018**.
    + <font color = blue>é€šè¿‡è¿ç§»å­¦ä¹ è¿›è¡Œé«˜è´¨é‡å•çœ¼æ·±åº¦ä¼°è®¡</font>
    + é˜¿åœæœæ‹‰å›½ç‹ç§‘æŠ€å¤§å­¦ï¼Œ[ä»£ç å¼€æº](https://github.com/ialhashim/DenseDepth)

---

#### 4. Others

+ [ ] **[21 ]** Pei L, Liu K, Zou D, et al. [**IVPR: An Instant Visual Place Recognition Approach based on Structural Lines in Manhattan World**](https://ieeexplore.ieee.org/abstract/document/8836504)[J]. IEEE Transactions on Instrumentation and Measurement, **2019**.
    + <font color = blue>IVPRï¼šåŸºäºæ›¼å“ˆé¡¿ä¸–ç•Œä¸­çš„ç»“æ„çº¿çš„å³æ—¶è§†è§‰ä½ç½®è¯†åˆ«æ–¹æ³•</font>
    + ä¸Šäº¤[è£´å‡Œè€å¸ˆ](https://scholar.google.com/citations?user=Vm7d2EkAAAAJ&hl=zh-CN&oi=sra)ï¼ŒæœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒº JCR Q1Q2 IF2.98
+ [ ] **[22]** Sjanic Z. [**Particle Filtering Approach for Data Association**](http://users.isy.liu.se/en/rt/zoran/Publ/fusion2019.pdf)[C]//22nd International Conference on Information Fusion. **2019**.
    + <font color = blue>ç²’å­æ»¤æ³¢ç®—æ³•ç”¨äºæ•°æ®å…³è”</font>

---

### 2019 å¹´ 9 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ24 ç¯‡ï¼‰

#### 1. Geometric SLAM
        
+ [x] **[1]** Elvira R, TardÃ³s J D, Montiel J M M. [**ORBSLAM-Atlas: a robust and accurate multi-map system**](https://arxiv.org/pdf/1908.11585.pdf)[J]. arXiv preprint arXiv:1908.11585, **2019**.
    + <font color = blue>**ORBSLAM-Atlasï¼šä¸€ä¸ªé²æ£’è€Œå‡†ç¡®çš„å¤šåœ°å›¾ç³»ç»Ÿ**</font>
    + è¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ï¼Œ**ORB-SLAM ä½œè€…**
+ [ ] **[2]** Yang Y, Dong W, Kaess M. [**Surfel-Based Dense RGB-D Reconstruction With Global And Local Consistency**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8794355)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 5238-5244.
    + <font color = blue>å…·æœ‰å…¨å±€å’Œå±€éƒ¨ä¸€è‡´æ€§çš„åŸºäºé¢å…ƒçš„ RGB-D ç¨ å¯†é‡å»º</font>
    + CMU æœºå™¨äººç ”ç©¶æ‰€
    + ç›¸å…³ç ”ç©¶ï¼šSchÃ¶ps T, Sattler T, Pollefeys M. [**SurfelMeshing: Online Surfel-Based Mesh Reconstruction**](https://arxiv.gg363.site/pdf/1810.00729)[J]. arXiv preprint arXiv:1810.00729, **2018**.
        + [ä»£ç å¼€æº](https://github.com/YiChenCityU/surfelmeshing)
+ [x] **[3]** Pire T, Corti J, Grinblat G. [**Online Object Detection and Localization on Stereo Visual SLAM System**](https://link.springer.com/article/10.1007/s10846-019-01074-2)[J]. Journal of Intelligent & Robotic Systems, **2019**: 1-10.
    + <font color = blue>**åœ¨çº¿è¿›è¡Œä¸‰ç»´ç›®æ ‡æ£€æµ‹çš„åŒç›®è§†è§‰ SLAM**</font>
    + æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼ŒJCR Q4ï¼ŒIF 2.4
+ [x] **[4]** Ferrer G. [**Eigen-Factors: Plane Estimation for Multi-Frame and Time-Continuous Point Cloud Alignment**](http://sites.skoltech.ru/app/data/uploads/sites/50/2019/07/ferrer2019planes.pdf)[C] **IROS 2019**.
    + <font color = blue>**ç‰¹å¾å› å­ï¼šå¤šå¸§å’Œæ—¶é—´è¿ç»­ç‚¹äº‘å¯¹é½çš„å¹³é¢ä¼°è®¡**</font>
    + ä¿„ç½—æ–¯æ–¯ç§‘å°”ç§‘æ²ƒç§‘æŠ€å­¦é™¢ï¼Œä¸‰æ˜Ÿ &emsp; [**ä»£ç å¼€æº**](https://gitlab.com/gferrer/eigen-factors-iros2019) &emsp; [æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=_1u_c43DFUE&feature=youtu.be)
+ [ ] **[5]** Zhang Y, Yang J, Zhang H, et al. [**Bundle Adjustment for Monocular Visual Odometry Based on Detected Traffic Sign Features**](https://ieeexplore.ieee.org/abstract/document/8803563)[C]//2019 IEEE International Conference on Image Processing (ICIP). IEEE, **2019**: 4350-4354.
    + <font color = blue>åŸºäºäº¤é€šæ ‡å¿—ç‰¹å¾æ£€æµ‹çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡ BA ä¼˜åŒ–</font>
    + åŒ—ç†å·¥ã€åç››é¡¿å¤§å­¦
+ [ ] **[6]** Zhang X, Wang W, Qi X, et al. [**Point-Plane SLAM Using Supposed Planes for Indoor Environments**](https://www.mdpi.com/1424-8220/19/17/3795/htm)[J]. Sensors, **2019**, 19(17): 3795.
    + <font color = blue>å®¤å†…ç¯å¢ƒä¸­ä½¿ç”¨å‡è®¾å¹³é¢çš„ç‚¹-å¹³é¢ SLAM</font>
    + åŒ—äº¬èˆªç©ºèˆªå¤©å¤§å­¦æœºå™¨äººç ”ç©¶æ‰€ &emsp; å¼€æºæœŸåˆŠ
+ [ ] **[7]** Zheng F, Liu Y H. [**Visual-Odometric Localization and Mapping for Ground Vehicles Using SE (2)-XYZ Constraints**](https://ieeexplore.ieee.org/abstract/document/8793928)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 3556-3562.
    + <font color = blue>ä½¿ç”¨ SE(2)-XYZ çº¦æŸç”¨äºåœ°é¢è½¦è¾†å®šä½äºå»ºå›¾çš„è§†è§‰é‡Œç¨‹è®¡</font>
    + é¦™æ¸¯ä¸­æ–‡å¤§å­¦ &emsp;[**ä»£ç å¼€æº**](https://github.com/izhengfan/se2lam)
+ [x] **[8]** Li H, Xing Y, Zhao J, et al. [**Leveraging Structural Regularity of Atlanta World for Monocular SLAM**](https://ieeexplore.ieee.org/abstract/document/8793716)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 2412-2418.
    + <font color = blue>**åˆ©ç”¨äºšç‰¹å…°å¤§ä¸–ç•Œç»“æ„è§„å¾‹çš„å•ç›® SLAM**</font>
    + é¦™æ¸¯ä¸­æ–‡å¤§å­¦[åˆ˜äº‘è¾‰](http://ri.cuhk.edu.hk/yhliu)æ•™æˆè¯¾é¢˜ç»„ï¼ˆä¸Šé¢é‚£ç¯‡çš„éƒ‘å¸†åšå£«æ˜¯ä»–å­¦ç”Ÿï¼‰ï¼Œ[é¦™æ¸¯ä¸­æ–‡å¤§å­¦å¤©çŸ³æœºå™¨äººç ”ç©¶æ‰€](http://ri.cuhk.edu.hk/)
+ [ ] **[9]** Sun J, Wang Y, Shen Y. [**Fully Scaled Monocular Direct Sparse Odometry with A Distance Constraint**](https://ieeexplore.ieee.org/abstract/document/8813371)[C]//2019 5th International Conference on Control, Automation and Robotics (ICCAR). IEEE, **2019**: 271-275.
    + <font color = blue>å…·æœ‰è·ç¦»çº¦æŸçš„å…¨å°ºå¯¸å•ç›®ç›´æ¥ç¨€ç–é‡Œç¨‹è®¡</font>
    + åŒ—ç†å·¥
+ [x] **[10]** Dong J, Lv Z. [**miniSAM: A Flexible Factor Graph Non-linear Least Squares Optimization Framework**](https://arxiv.org/pdf/1909.00903.pdf)[J]. arXiv preprint arXiv:1909.00903, **2019**.
    + <font color = blue>**minisamï¼šä¸€ç§çµæ´»çš„å› å­å›¾éçº¿æ€§æœ€å°äºŒä¹˜ä¼˜åŒ–æ¡†æ¶**</font>
    + Facebookï¼Œ[**ä»£ç å¼€æº**](https://github.com/dongjing3309/minisam)ï¼Œ[Google Scholar](https://scholar.google.com/citations?user=99RVk_MAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[11]** Campos C, MM M J, TardÃ³s J D. [**Fast and Robust Initialization for Visual-Inertial SLAM**](https://arxiv.org/pdf/1908.10653.pdf)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 1288-1294.
    + <font color = blue>è§†æƒ¯ SLAM å¿«é€Ÿé²æ£’çš„åˆå§‹åŒ–</font>
    + è¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ï¼ŒORB-SLAM è¯¾é¢˜ç»„
+ [ ] **[12]** He L, Yang M, Li H, et al. [**Graph Matching Pose SLAM based on Road Network Information**](https://ieeexplore.ieee.org/abstract/document/8814093)[C]//2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, **2019**: 1274-1279.
    + <font color = blue>åŸºäºè·¯ç½‘ä¿¡æ¯çš„å›¾åŒ¹é…ä½å§¿ SLAM</font>
    + **ä¸Šæµ·äº¤é€šå¤§å­¦**ç³»ç»Ÿæ§åˆ¶ä¸ä¿¡æ¯å¤„ç†æ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤
+ [ ] **[13]** Gu T, Yan R. [**An Improved Loop Closure Detection for RatSLAM**](https://ieeexplore.ieee.org/abstract/document/8813378)[C]//2019 5th International Conference on Control, Automation and Robotics (ICCAR). IEEE, **2019**: 884-888.
    + <font color = blue>ä¸€ç§æ”¹è¿›çš„ RatSLAM é—­ç¯æ£€æµ‹æ–¹æ³•</font>
    + å››å·å¤§å­¦

---

#### 2. Semantic/Deep SLAM

+ [x] **[14]** Zhang J, Gui M, Wang Q, et al. [**Hierarchical Topic Model Based Object Association for Semantic SLAM**](https://ieeexplore.ieee.org/abstract/document/8794595)[J]. IEEE transactions on visualization and computer graphics, **2019**.
    + <font color = blue>**åŸºäºå±‚æ¬¡ä¸»é¢˜æ¨¡å‹çš„è¯­ä¹‰ SLAM å¯¹è±¡å…³è”** </font>
    + æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼Œ JCR Q1ï¼ŒIF 3.78
+ [ ] **[15]** GÃ¤hlert N, Wan J J, Weber M, et al. [**Beyond Bounding Boxes: Using Bounding Shapes for Real-Time 3D Vehicle Detection from Monocular RGB Images**](https://ieeexplore.ieee.org/abstract/document/8814036)[C]//2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, **2019**: 675-682.
    + <font color = blue>è¶…è¶Šè¾¹ç•Œæ¡†ï¼šä½¿ç”¨è¾¹ç•Œå½¢çŠ¶ä»å•ç›® RGB å›¾åƒä¸­è¿›è¡Œå®æ—¶ 3D è½¦è¾†æ£€æµ‹</font>
    + å¾·å›½è€¶æ‹¿å¤§å­¦
+ [ ] **[16]** Yang N, Wang R, Stuckler J, et al. [**Deep virtual stereo odometry: Leveraging deep depth prediction for monocular direct sparse odometry**](http://openaccess.thecvf.com/content_ECCV_2018/papers/Nan_Yang_Deep_Virtual_Stereo_ECCV_2018_paper.pdf)[C]//Proceedings of the European Conference on Computer Vision (**ECCV**). **2018**: 817-833.
    + <font color = blue>æ·±åº¦è™šæ‹ŸåŒç›®é‡Œç¨‹è®¡ï¼šåˆ©ç”¨å•ç›®ç›´æ¥ç¨€ç–é‡Œç¨‹è®¡çš„æ·±åº¦é¢„æµ‹</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ï¼›[ä½œè€…ä¸»é¡µ](https://vision.in.tum.de/members/yangn)ï¼Œ[é¡¹ç›®ä¸»é¡µ](https://vision.in.tum.de/research/vslam/dvso)
+ [ ] **[17]** Liu H, Ma H, Zhang L. [**Visual Odometry based on Semantic Supervision**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8803013)[C]//2019 IEEE International Conference on Image Processing (**ICIP**). IEEE, **2019**: 2566-2570.
    + <font color = blue>**åŸºäºè¯­ä¹‰ç›‘ç£çš„è§†è§‰é‡Œç¨‹è®¡** </font>
    + æ¸…åå¤§å­¦ï¼›ä¼šè®® ICIPï¼šCCF è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ C ç±»ä¼šè®®
+ [ ] **[18]** Wald J, Avetisyan A, Navab N, et al. [**RIO: 3D Object Instance Re-Localization in Changing Indoor Environments**](https://arxiv.org/abs/1908.06109)[J]. arXiv preprint arXiv:1908.06109, **2019**.
    + <font color = blue>RIO:æ”¹å˜å®¤å†…ç¯å¢ƒçš„ 3D ç‰©ä½“å®ä¾‹é‡å®šä½</font>
    + TUMï¼ŒGoogleï¼Œ[é¡¹ç›®ä¸»é¡µ](https://waldjohannau.github.io/RIO/)ï¼Œå¼€æ”¾æ•°æ®é›†

---

#### 3. AR & MR & VR

+ [ ] **[19]** Su Y, Rambach J, Minaskan N, et al. [**Deep Multi-State Object Pose Estimation for Augmented Reality Assembly**](https://www.researchgate.net/profile/Jason_Rambach/publication/335207627_Deep_Multi-State_Object_Pose_Estimation_for_Augmented_Reality_Assembly/links/5d56c09d92851cb74c714724/Deep-Multi-State-Object-Pose-Estimation-for-Augmented-Reality-Assembly.pdf)[C]. IEEE International Symposium on Mixed and Augmented Reality (**ISMAR**) **2019**
    + <font color = blue>å¢å¼ºç°å®è£…å¤‡çš„æ·±åº¦å¤šçŠ¶æ€ç›®æ ‡å§¿æ€ä¼°è®¡</font>
    + å¾·å›½äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒ
    
---

#### 4. Learning others

+ [ ] **[20]** Huang X, Dai Z, Chen W, et al. [**Improving Keypoint Matching Using a Landmark-Based Image Representation**](https://ieeexplore.ieee.org/abstract/document/8794420)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 1281-1287.
    + <font color = blue>åˆ©ç”¨åŸºäºåœ°æ ‡çš„å›¾åƒè¡¨ç¤ºæ”¹è¿›å…³é”®ç‚¹åŒ¹é…</font>
    + å¹¿ä¸œå·¥ä¸šå¤§å­¦
+ [ ] **[21]** Puscas M M, Xu D, Pilzer A, et al. [**Structured Coupled Generative Adversarial Networks for Unsupervised Monocular Depth Estimation**](https://arxiv.org/pdf/1908.05794.pdf)[J]. arXiv preprint arXiv:1908.05794, **2019**.
    + <font color = blue>æ— ç›‘ç£å•ç›®æ·±åº¦ä¼°è®¡çš„ç»“æ„è€¦åˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œ</font>
    + åä¸ºã€ç‰›æ´¥å¤§å­¦ &emsp;[**ä»£ç å¼€æº**](https://github.com/mihaipuscas/3dv---coupled-crf-disparity)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
    
---

#### 5. Others

+ [x] **[22]** Yang B, Xu X, Li J, et al. [**Landmark Generation in Visual Place Recognition Using Multi-Scale Sliding Window for Robotics**](https://www.mdpi.com/2076-3417/9/15/3146)[J]. Applied Sciences, **2019**, 9(15): 3146.
    + <font color = blue>**åŸºäºå¤šå°ºåº¦æ»‘åŠ¨çª—å£çš„æœºå™¨äººè§†è§‰åœ°ç‚¹è¯†åˆ«ä¸­çš„åœ°æ ‡ç”Ÿæˆ** </font>
    + ä¸œå—å¤§å­¦ &emsp;æœŸåˆŠï¼šå¼€æºæœŸåˆŠï¼Œä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q3
+ [x] **[23]** Hofstetter I, Sprunk M, Schuster F, et al. [**On Ambiguities in Feature-Based Vehicle Localization and their A Priori Detection in Maps**](https://ieeexplore.ieee.org/abstract/document/8813978)[C]//2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, **2019**: 1192-1198.
    + <font color = blue>**åŸºäºç‰¹å¾çš„è½¦è¾†æ¨¡ç³Šå®šä½åŠå…¶åœ¨åœ°å›¾ä¸­çš„å…ˆéªŒæ£€æµ‹** </font>
    + SLAM ä¸­çš„ç‰©ä½“æ•°æ®å…³è”å¯å‚è€ƒ
+ [ ] **[24]** KÃ¼mmerle J, Sons M, Poggenhans F, et al. [**Accurate and Efficient Self-Localization on Roads using Basic Geometric Primitives**](https://ieeexplore.ieee.org/abstract/document/8793497)[C]//2019 International Conference on Robotics and Automation (**ICRA**). IEEE, **2019**: 5965-5971.
    + <font color = blue>åŸºäºå‡ ä½•å…ƒç´ åœ¨é“è·¯ä¸­è¿›è¡Œå‡†ç¡®æœ‰æ•ˆçš„è‡ªå®šä½</font>
    + å¾·å›½å¡å°”æ–¯é²å„ç†å·¥å­¦é™¢

---

### 2019 å¹´ 8 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ26 ç¯‡ï¼‰

#### 1. Geometric SLAM

+ [ ] **[1]** Wei X, Huang J, Ma X. [**Real-Time Monocular Visual SLAM by Combining Points and Lines**](https://ieeexplore.ieee.org/abstract/document/8784968/authors#authors)[C]//2019 IEEE International Conference on Multimedia and Expo (**ICME**). IEEE, **2019**: 103-108.
    + <font color = blue>ç‚¹çº¿ç»“åˆçš„å•ç›®è§†è§‰ SLAM</font>
    + ä¸­å›½ç§‘å­¦é™¢ä¸Šæµ·é«˜ç­‰ç ”ç©¶é™¢ &emsp; ICMEï¼šCCF è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ B ç±»ä¼šè®®
+ [ ] **[2]** Fu Q, Yu H, Lai L, et al. [**A Robust RGB-D SLAM System with Points and Lines for Low Texture Indoor Environments**](https://ieeexplore.ieee.org/abstract/document/8756267)[J]. IEEE Sensors Journal, **2019**.
    + <font color = blue>ä½çº¹ç†å®¤å†…ç¯å¢ƒçš„ç‚¹çº¿è”åˆçš„é²æ£’ RGB-D SLAM ç³»ç»Ÿ</font>
    + æ¹–å—å¤§å­¦æœºå™¨äººè§†è§‰æ„ŸçŸ¥ä¸æ§åˆ¶å›½å®¶å·¥ç¨‹å®éªŒå®¤ &emsp; æœŸåˆŠ IEEE Sensors Journalï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q1Q2ï¼ŒIF 2.69
+ [ ] **[3]** Zhao W, Qian K, Ma Z, et al. [**Stereo Visual SLAM Using Bag of Point and Line Word Pairs**](https://link.springer.com/chapter/10.1007/978-3-030-27538-9_56#Footnotes)[C]//International Conference on Intelligent Robotics and Applications. Springer, Cham, **2019**: 651-661.
    + <font color = blue>åˆ©ç”¨ç‚¹çº¿è¯è¢‹å¯¹çš„åŒç›® SLAM</font>
    + ä¸œå—å¤§å­¦
+ [x] **[4]** Hachiuma R, Pirchheim C, Schmalstieg D, et al. [**DetectFusion: Detecting and Segmenting Both Known and Unknown Dynamic Objects in Real-time SLAM**](https://arxiv.org/pdf/1907.09127.pdf)[C]//Proceedings British Machine Vision Conference (**BMVC**). **2019**.
    + <font color = blue>**DetectFusionï¼šåœ¨å®æ—¶çš„ SLAM ä¸­æ£€æµ‹å’Œåˆ†å‰²å·²çŸ¥ä¸æœªçŸ¥çš„åŠ¨æ€å¯¹è±¡**</font>
    + æ—¥æœ¬åº†åº”ä¹‰å¡¾å¤§å­¦ã€æ ¼æ‹‰èŒ¨ç†å·¥å¤§å­¦ &emsp; BMVCï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
    + ç›¸å…³è®ºæ–‡ï¼š
        + [**Co-Fusion: Real-time Segmentation, Tracking and Fusion of Multiple Objects**](https://github.com/martinruenz/co-fusion), Martin RÃ¼nz and Lourdes Agapito, 2017 IEEE International Conference on Robotics and Automation (ICRA)
        + Ishikawa Y, Hachiuma R, Ienaga N, et al. [**Semantic Segmentation of 3D Point Cloud to Virtually Manipulate Real Living Space**](https://lclab.org/wp-content/uploads/2019/03/apmar2019_semanticsegmentation.pdf)[C]//2019 12th Asia Pacific Workshop on Mixed and Augmented Reality (APMAR). IEEE, 2019: 1-7.
+ [ ] **[5]** Prokhorov D, Zhukov D, Barinova O, et al. [**Measuring robustness of Visual SLAM**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8758020)[C]//2019 16th International Conference on Machine Vision Applications (MVA). IEEE, **2019**: 1-6.
    + <font color = blue>è§†è§‰ SLAM çš„é²æ£’æ€§è¯„ä¼°</font>
    + ä¸‰æ˜Ÿ AI ç ”ç©¶ä¸­å¿ƒ &emsp; MVAï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[6]** Ryohei Y, Kanji T, Koji T. [**Invariant Spatial Information for Loop-Closure Detection**](https://ieeexplore.ieee.org/abstract/document/8757889/keywords#keywords)[C]//2019 16th International Conference on Machine Vision Applications (MVA). IEEE, **2019**: 1-6.
    + <font color = blue>ç”¨äºé—­ç¯æ£€æµ‹çš„ä¸å˜ç©ºé—´ä¿¡æ¯</font>
    + æ—¥æœ¬ç¦äº•å¤§å­¦ &emsp; MVAï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[7]** Yang B, Xu X, Li J. [**Keyframe-Based Camera Relocalization Method Using Landmark and Keypoint Matching**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8746595)[J]. IEEE Access, **2019**, 7: 86854-86862.
    + <font color = blue>ä½¿ç”¨è·¯æ ‡å’Œå…³é”®ç‚¹åŒ¹é…åŸºäºå…³é”®å¸§çš„ç›¸æœºé‡å®šä½æ–¹æ³•</font>
    + ä¸œå—å¤§å­¦ &emsp; æœŸåˆŠ IEEE Accessï¼šå¼€æºæœŸåˆŠ

---

#### 2. Semantic/Deep SLAM

+ [x] **[8]** Ganti P, Waslander S. [**Network Uncertainty Informed Semantic Feature Selection for Visual SLAM**](https://ieeexplore.ieee.org/abstract/document/8781616)[C]//2019 16th Conference on Computer and Robot Vision (CRV). IEEE, **2019**: 121-128.
    + <font color = blue>**è§†è§‰ SLAM ä¸­ç½‘ç»œä¸ç¡®å®šæ€§è¯­ä¹‰ä¿¡æ¯çš„é€‰æ‹©** </font>
    + æ»‘é“å¢å¤§å­¦ã€å¤šä¼¦å¤šå¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/navganti/SIVO)
    + ç›¸å…³è®ºæ–‡ï¼ˆå­¦ä½è®ºæ–‡ï¼‰ï¼šGanti P. [**SIVO: Semantically Informed Visual Odometry and Mapping**](https://uwspace.uwaterloo.ca/bitstream/handle/10012/14111/Ganti_Pranav.pdf?sequence=1&isAllowed=y)[D]. University of Waterloo, **2018**.
+ [x] **[9]** Yu H, Lee B. [**Not Only Look But Observe: Variational Observation Model of Scene-Level 3D Multi-Object Understanding for Probabilistic SLAM**](https://arxiv.org/pdf/1907.09760.pdf)[J]. arXiv preprint arXiv:1907.09760, **2019**.
    + <font color = blue>**ä¸ä»…çœ‹åˆ°è€Œä¸”è¿˜è§‚å¯Ÿï¼šåŸºäºåœºæ™¯çº§ä¸‰ç»´å¤šç›®æ ‡ç†è§£çš„æ¦‚ç‡ SLAM å˜åˆ†è§‚æµ‹æ¨¡å‹**</font>
    + é¦–å°”å›½ç«‹å¤§å­¦ &emsp;[ä»£ç å¼€æº](https://github.com/bogus2000/NOLBO)  &emsp; [Google Scholr](https://scholar.google.com/citations?user=pa6RBYkAAAAJ&hl=zh-CN&oi=sra)
+ [x] **[10]** Hu L, Xu W, Huang K, et al. [**Deep-SLAM++: Object-level RGBD SLAM based on class-specific deep shape priors**](https://arxiv.org/pdf/1907.09691.pdf)[J]. arXiv preprint arXiv:1907.09691, **2019**.
    + <font color = blue>**åŸºäºç‰¹å®šç±»æ·±åº¦å½¢çŠ¶å…ˆéªŒçš„å¯¹è±¡çº§ RGBD SLAM** </font>
    + ä¸Šæµ·ç§‘æŠ€å¤§å­¦ &emsp;
+ [x] **[11]** Torres CÃ¡mara J M. Map Slammer. [**Densifying Scattered KSLAM 3D Maps with Estimated Depth**](https://rua.ua.es/dspace/bitstream/10045/94751/1/SLAM_usando_tecnicas_de_deep_learning_Torres_Camara_Jose_Miguel.pdf)[J]. **2019**.
    + <font color = blue>**åˆ©ç”¨æ·±åº¦ä¼°è®¡çš„åŠ å¯†åˆ†æ•£çš„å…³é”®å¸§ SLAM ä¸‰ç»´åœ°å›¾** </font>
    + **è¥¿ç­ç‰™é˜¿åˆ©åç‰¹å¤§å­¦å­¦ä½è®ºæ–‡** &emsp;[**ä»£ç å¼€æº**](https://github.com/jmtc7/mapSlammer)  &emsp;[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=b74P3ykYE34&feature=youtu.be) 
+ [x] **[12]** Cieslewski T, Bloesch M, Scaramuzza D. **Matching Features without Descriptors: Implicitly Matched Interest Points (IMIPs)**[C]// 2019 British Machine Vision Conference.**2018**.
    + <font color = blue>**æ²¡æœ‰æè¿°ç¬¦çš„ç‰¹å¾åŒ¹é…ï¼šéšå«åŒ¹é…çš„å…´è¶£ç‚¹**</font>
    + è‹é»ä¸–ç†å·¥ã€å¸å›½ç†å·¥ &emsp;[**ä»£ç å¼€æº**](https://github.com/uzh-rpg/imips_open)  &emsp; BMVCï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[13]** Zheng J, Zhang J, Li J, et al. [**Structured3D: A Large Photo-realistic Dataset for Structured 3D Modeling**](https://arxiv.org/pdf/1908.00222.pdf)[J]. arXiv preprint arXiv:1908.00222, **2019**.
    + <font color = blue>ç”¨äºç»“æ„åŒ–ä¸‰ç»´å»ºæ¨¡çš„å¤§å‹ç…§ç‰‡æ‹ŸçœŸæ•°æ®é›†</font>
    + ä¸Šæµ·ç§‘æŠ€å¤§å­¦ &emsp;[ä»£ç ã€æ•°æ®é›†å¼€æº](https://github.com/bertjiazheng/Structured3D)  &emsp; [é¡¹ç›®ä¸»é¡µ](https://structured3d-dataset.org/)
+ [ ] **[14]** Jikai Lu, Jianhui Chen, James J. Little, [**Pan-tilt-zoom SLAM for Sports Videos**](https://arxiv.org/pdf/1907.08816.pdf).[C]//British Machine Vision Conference (BMVC) **2019**.
    + <font color = blue>ç”¨äºä½“è‚²è§†é¢‘çš„å¹³ç§» - å€¾æ–œ - ç¼©æ”¾SLAM</font>
    + ä¸åˆ—é¢ å“¥ä¼¦æ¯”äºšå¤§å­¦ &emsp;[ä»£ç å¼€æº](https://github.com/lulufa390/Pan-tilt-zoom-SLAM)
    
---

#### 3. AR & MR & VR

+ [ ] **[15]** Saran V, Lin J, Zakhor A. [**Augmented Annotations: Indoor Dataset Generation with Augmented Reality**](http://www-video.eecs.berkeley.edu/papers/jameslin/augmented_annotation.pdf)[J]. ISPRS-International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences, **2019**, 4213: 873-879.
    + <font color = blue>å¢å¼ºæ³¨é‡Šï¼šå…·æœ‰å¢å¼ºç°å®çš„å®¤å†…æ•°æ®é›†ç”Ÿæˆ</font>
    + åŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ &emsp;[é¡¹ç›®ä¸»é¡µ](https://www.6d.ai/)
+ [ ] **[16]** Liao M, Song B, He M, et al. [**SynthText3D: Synthesizing Scene Text Images from 3D Virtual Worlds**](https://arxiv.org/pdf/1907.06007.pdf)[J]. arXiv preprint arXiv:1907.06007, **2019**.
    + <font color = blue>ä»3Dè™šæ‹Ÿä¸–ç•Œåˆæˆåœºæ™¯æ–‡æœ¬å›¾åƒ</font>
    + åä¸­ç§‘å¤§ã€åŒ—å¤§ã€Face++ &emsp;[**ä»£ç å¼€æº**](https://github.com/MhLiao/SynthText3D)
+ [ ] **[17]** [**Shooting Labels by Virtual Reality**](https://static1.squarespace.com/static/5c3f69e1cc8fedbc039ea739/t/5d01638662182d0001b6f7f6/1560372111582/9_CVPR_2019_VR.pdf). **2019**
    + <font color = blue>åˆ©ç”¨è™šæ‹Ÿç°å®æ‹æ‘„è¯­ä¹‰æ ‡ç­¾</font>
    + æ„å¤§åˆ©åšæ´›å°¼äºšå¤§å­¦ &emsp;[**ä»£ç å¼€æº**](https://github.com/pierlui92/Shooting-Labels)

---

#### 4. Learning others
+ [x] **[18]** Ku J, Pon A D, Waslander S L. [**Monocular 3D Object Detection Leveraging Accurate Proposals and Shape Reconstruction**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Ku_Monocular_3D_Object_Detection_Leveraging_Accurate_Proposals_and_Shape_Reconstruction_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **CVPR 2019**: 11867-11876.
    + <font color = blue>**åˆ©ç”¨ç²¾ç¡®çš„ææ¡ˆå’Œå½¢çŠ¶é‡å»ºè¿›è¡Œå•ç›®ä¸‰ç»´ç‰©ä½“æ£€æµ‹**</font>
    + å¤šä¼¦å¤šå¤§å­¦
    + ç›¸å…³ç ”ç©¶ï¼šKu J, Pon A D, Walsh S, et al. [Improving 3D Object Detection for Pedestrians with Virtual Multi-View Synthesis Orientation Estimation](https://arxiv.org/pdf/1907.06777.pdf)[J]. arXiv preprint arXiv:1907.06777, 2019.
+ [x] **[19]** Chiang H, Lin Y, Liu Y, et al. [**A Unified Point-Based Framework for 3D Segmentation**](https://arxiv.org/pdf/1908.00478.pdf)[J]. arXiv preprint arXiv:1908.00478, **2019**.
    + <font color = blue>**ä¸€ç§ç»Ÿä¸€çš„åŸºäºç‚¹çš„ä¸‰ç»´åˆ†å‰²æ¡†æ¶**</font>
    + å›½ç«‹å°æ¹¾å¤§å­¦ã€äºšé©¬é€Š &emsp;[**ä»£ç å¼€æº**](https://github.com/ken012git/joint_point_based)
+ [ ] **[20]** Zhang Y, Lu Z, Xue J H, et al. [**A New Rotation-Invariant Deep Network for 3D Object Recognition**](https://ieeexplore.ieee.org/abstract/document/8784918)[C]//2019 IEEE International Conference on Multimedia and Expo (ICME). **IEEE**, **2019**: 1606-1611.
    + <font color = blue>ä¸€ç§æ–°çš„å…·æœ‰æ—‹è½¬ä¸å˜æ€§çš„ä¸‰ç»´ç‰©ä½“è¯†åˆ«æ·±åº¦ç½‘ç»œ</font>
    + æ¸…åå¤§å­¦ &emsp; ICMEï¼šCCF è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ B ç±»ä¼šè®®
+ [x] **[21]** Gao Y, Yuille A L. Estimation of 3D Category-Specific Object Structure: Symmetry, Manhattan and/or Multiple Images[J]. International Journal of Computer Vision, **2019**: 1-26.
    + <font color = blue>**3D ç±»åˆ«ç‰¹å®šå¯¹è±¡ç»“æ„çš„ä¼°è®¡ï¼šå¯¹ç§°æ€§ï¼Œæ›¼å“ˆé¡¿å’Œæˆ–å¤šå›¾åƒ**</font>
    + ä¸­ç§‘å¤§ &emsp; æœŸåˆŠ International Journal of Computer Visionï¼š**ä¸­ç§‘é™¢ä¸€åŒºï¼ŒJCR Q1ï¼ŒIF 12.389**
+ [ ] **[22]** Palazzi A, Bergamini L, Calderara S, et al. [**Semi-parametric Object Synthesis**](https://arxiv.org/pdf/1907.10634.pdf)[J]. arXiv preprint arXiv:1907.10634, **2019**.
    + <font color = blue>åŠå‚æ•°çš„å¯¹è±¡åˆæˆ</font>
    + æ‘©å¾·çº³å¤§å­¦ &emsp; [ä»£ç å¼€æº](https://github.com/ndrplz/semiparametric)
+ [ ] **[23]** Christiansen P H, Kragh M F, Brodskiy Y, et al. [**UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor**](https://arxiv.org/pdf/1907.04011.pdf)[J]. arXiv preprint arXiv:1907.04011, **2019**.
    + <font color = blue>**ç«¯åˆ°ç«¯æ— ç›‘ç£å…´è¶£ç‚¹æ£€æµ‹å™¨å’Œæè¿°ç¬¦**</font>
    + åœŸè€³å…¶å“ˆå¡ç‰¹å¤§å­¦ &emsp; **ç›¸å…³ä»£ç **ï¼š[SuperPointPretrainedNetwork](https://github.com/MagicLeapResearch/SuperPointPretrainedNetwork)ï¼Œ[lf-net-release](https://github.com/vcg-uvic/lf-net-release)
+ [x] **[24]** Chen B X, Tsotsos J K. [**Fast Visual Object Tracking with Rotated Bounding Boxes**](https://arxiv.org/pdf/1907.03892.pdf)[J]. arXiv preprint arXiv:1907.03892, **2019**.
    + <font color = blue>**å¸¦æ—‹è½¬è¾¹ç•Œæ¡†çš„å¿«é€Ÿè§†è§‰ç›®æ ‡è·Ÿè¸ª**</font>
    + çº¦å…‹å¤§å­¦ã€å¤šä¼¦å¤šå¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/baoxinchen/siammask_e)
+ [ ] **[25]** Brazil G, Liu X. [**M3D-RPN: Monocular 3D Region Proposal Network for Object Detection**](https://arxiv.org/pdf/1907.06038.pdf)[J]. arXiv preprint arXiv:1907.06038, **2019**.
    + <font color = blue>ç”¨äºç‰©ä½“æ£€æµ‹çš„å•ç›® 3D åŒºåŸŸæè®®ç½‘ç»œ</font>
    + å¯†æ­‡æ ¹å·ç«‹å¤§å­¦

---

#### 5. Others

+ [ ] **[26]** Zhou Q, Sattler T, Pollefeys M, et al. [**To Learn or Not to Learn: Visual Localization from Essential Matrices**](https://arxiv.org/pdf/1908.01293.pdf)[J]. arXiv preprint arXiv:1908.01293, 2019.
    + <font color = blue>å­¦ä¹ æˆ–éå­¦ä¹ çš„æ–¹æ³•ï¼šåŸºç¡€çŸ©é˜µç”¨äºè§†è§‰å®šä½</font>
    + æ…•å°¼é»‘ã€è‹é»ä¸–

---

### 2019 å¹´ 7 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ36 ç¯‡ï¼‰

#### 1. Geometric SLAM

+ [x] **[1]** Schenk F, Fraundorfer F. [**RESLAM: A real-time robust edge-based SLAM system**](https://pure.tugraz.at/ws/portalfiles/portal/23662547/schenk_icra_2019.pdf)[C]//IEEE International Conference on Robotics and Automation(**ICRA**) 2019. **2019**.
    + <font color = blue>ä¸€ç§å®æ—¶ç¨³å¥çš„åŸºäºè¾¹ç¼˜çš„SLAMç³»ç»Ÿ</font>
    + å¥¥åœ°åˆ©æ ¼æ‹‰èŒ¨ç§‘æŠ€å¤§å­¦ &emsp;[Google Scholar](https://scholar.google.com/citations?user=IbK5KvYAAAAJ&hl=zh-CN&oi=sra)
    + [**ä»£ç å¼€æº**](https://github.com/fabianschenk/RESLAM)  &emsp; [é¡¹ç›®ä¸»é¡µ](https://graz.pure.elsevier.com/en/publications/reslam-a-real-time-robust-edge-based-slam-system)
+ [ ] **[2]** Christensen K, Hebert M. [**Edge-Direct Visual Odometry**](https://arxiv.org/pdf/1906.04838.pdf)[J]. arXiv preprint arXiv:1906.04838, **2019**.
    + <font color = blue>è¾¹ç¼˜ç›´æ¥æ³•è§†è§‰é‡Œç¨‹è®¡</font>
    + CMU
+ [ ] **[3]** Dong E, Xu J, Wu C, et al. [**Pair-Navi: Peer-to-Peer Indoor Navigation with Mobile Visual SLAM**](https://ieeexplore.ieee.org/abstract/document/8737640)[C]//IEEE **INFOCOM** 2019-IEEE Conference on Computer Communications. IEEE, **2019**: 1189-1197.
    + <font color = blue>ä½¿ç”¨ç§»åŠ¨è§†è§‰ SLAM è¿›è¡Œç‚¹å¯¹ç‚¹å®¤å†…å¯¼èˆª</font>
    + æ¸…åå¤§å­¦ &emsp; [Google Scholar](https://scholar.google.com/citations?user=ZbqGJ_YAAAAJ&hl=zh-CN&oi=sra)
    + ä¼šè®®ï¼šIEEE INFOCOMï¼šCCF è®¡ç®—æœºç½‘ç»œ **A ç±»ä¼šè®®**
+ [ ] **[4]** Zhou H, Fan H, Peng K, et al. [**Monocular Visual Odometry Initialization With Points and Line Segments**](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8728034)[J]. IEEE Access, **2019**, 7: 73120-73130.
    + <font color = blue>åˆ©ç”¨ç‚¹çº¿åˆå§‹åŒ–çš„å•ç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + å›½é˜²ç§‘å¤§ã€æ¸…åå¤§å­¦ã€æ¸¯ä¸­æ–‡
    + IEEE Accessï¼šå¼€æºæœŸåˆŠ
+ [ ] **[5]** He M, Zhu C, Huang Q, et al. [**A review of monocular visual odometry**](https://link.springer.com/article/10.1007/s00371-019-01714-6)[J]. The Visual Computer, **2019**: 1-13.
    + <font color = blue>å•ç›®è§†è§‰é‡Œç¨‹è®¡ç»¼è¿°</font>
    + æ²³æµ·å¤§å­¦
    + æœŸåˆŠ The Visual Computerï¼šä¸­ç§‘é™¢å››åŒºï¼ŒJCR  Q3ï¼ŒIF 1.39
+ [ ] **[6]** Bujanca M, Gafton P, Saeedi S, et al. [**SLAMBench 3.0: Systematic Automated Reproducible Evaluation of SLAM Systems for Robot Vision Challenges and Scene Understanding**](https://www.sajad-saeedi.ca/uploads/3/8/5/9/38597021/sb3.pdf)[C]//IEEE International Conference on Robotics and Automation (**ICRA**). **2019**.
    + <font color = blue>ç”¨äºæœºå™¨äººè§†è§‰æŒ‘æˆ˜å’Œåœºæ™¯ç†è§£çš„ SLAM ç³»ç»Ÿè‡ªåŠ¨å¯é‡å¤æ€§è¯„ä¼°</font>
    + çˆ±ä¸å ¡å¤§å­¦ï¼Œä¼¦æ•¦å¸å›½ç†å·¥å­¦é™¢
+ [ ] **[7]** Wang Y, Zell A. [**Improving Feature-based Visual SLAM by Semantics**](https://ieeexplore.ieee.org/abstract/document/8708875)[C]//2018 IEEE International Conference on Image Processing, Applications and Systems (IPAS). IEEE, **2018**: 7-12.
    + <font color = blue>åˆ©ç”¨è¯­ä¹‰ä¿¡æ¯æé«˜ç‰¹å¾ç‚¹æ³•çš„ SLAM</font>
    + å›¾å®¾æ ¹å¤§å­¦
+ [x] **[8]** Mo J, Sattar J. [**Extending Monocular Visual Odometry to Stereo Camera System by Scale Optimization**](https://arxiv.org/pdf/1905.12723.pdf)[C]. International Conference on Intelligent Robots and Systems (**IROS**), **2019**.
    + <font color = blue>é€šè¿‡å°ºåº¦ä¼˜åŒ–å°†å•ç›®è§†è§‰é‡Œç¨‹è®¡æ‰©å±•åˆ°åŒç›®ç›¸æœºç³»ç»Ÿ</font>
    + [æ˜å°¼è‹è¾¾å¤§å­¦äº¤äº’å¼æœºå™¨äººå’Œè§†è§‰å®éªŒå®¤](http://irvlab.cs.umn.edu/publication)
    + [**ä»£ç å¼€æº**](https://github.com/jiawei-mo/scale_optimization)
+ [ ] **[9]** [**A Modular Optimization framework for Localization and mApping (MOLA)**](http://ingmec.ual.es/~jlblanco/papers/blanco2019mola_rss2019.pdf). **2019**
    + <font color = blue>ç”¨äºå®šä½å’Œå»ºå›¾çš„æ¨¡å—åŒ–ä¼˜åŒ–æ¡†æ¶</font>
    + è¥¿ç­ç‰™é˜¿å°”æ¢…åˆ©äºšå¤§å­¦
    + [**ä»£ç å¼€æº**](https://github.com/MOLAorg/mola)
+ [ ] **[10]** Ye W, Zhao Y, Vela P A. [**Characterizing SLAM Benchmarks and Methods for the Robust Perception Age**](https://arxiv.org/pdf/1905.07808.pdf)[J]. arXiv preprint arXiv:1905.07808, **2019**.
    + <font color = blue>è¡¨å¾é²æ£’æ„ŸçŸ¥æ—¶ä»£çš„ SLAM åŸºå‡†å’Œæ–¹æ³•</font>
    + ä¹”æ²»äºšç†å·¥å­¦é™¢
+ [ ] **[11]** BÃ¼rki M, Cadena C, Gilitschenski I, et al. [**Appearanceâ€based landmark selection for visual localization**](https://onlinelibrary.wiley.com/doi/full/10.1002/rob.21870)[J]. Journal of Field Robotics. **2019**
    + <font color = blue>åŸºäºå¤–è§‚çš„ç”¨äºè§†è§‰å®šä½çš„è·¯æ ‡é€‰æ‹©</font>
    + ETHï¼ŒMIT &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR  Q1ï¼ŒIF 5.0
+ [ ] **[12]** Hsiao M, Kaess M. [**MH-iSAM2: Multi-hypothesis iSAM using Bayes Tree and Hypo-tree**](http://www.cs.cmu.edu/~kaess/pub/Hsiao19icra.pdf)[J]. **2019**.
    + <font color = blue>MH-iSAM2ï¼šä½¿ç”¨è´å¶æ ‘å’Œ Hypo æ ‘çš„å¤šå‡è®¾ iSAM</font>
    + CMU &emsp; [**ä»£ç å¼€æº**](https://bitbucket.org/rpl_cmu/mh-isam2_lib/src/master/)
+ [x] **[13]** Schops T, Sattler T, Pollefeys M. [**BAD SLAM: Bundle Adjusted Direct RGB-D SLAM**](http://openaccess.thecvf.com/content_CVPR_2019/papers/Schops_BAD_SLAM_Bundle_Adjusted_Direct_RGB-D_SLAM_CVPR_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **2019**: 134-144.
    + <font color = blue>ç›´æ¥æ³• RGB-D SLAM ä¸­çš„ BA</font>
    + ETH 
    + [**ä»£ç å¼€æº**](https://github.com/ETH3D) &emsp; [**é¡¹ç›®ä¸»é¡µ**](https://www.eth3d.net/) 
+ [x] **[14]** Wang K, Gao F, Shen S. [**Real-time Scalable Dense Surfel Mapping**](https://wang-kx.github.io/al-folio/assets/pdf/icra2019.pdf)[C]//Proc. of the IEEE Intl. Conf. on Robot. and Autom.(**ICRA**). **2019**.
    + <font color = blue>å®æ—¶å¯æ‹“å±•çš„è¡¨é¢é‡å»º</font>
    + æ¸¯ç§‘å¤§æ²ˆé‚µåŠ¼è¯¾é¢˜ç»„
    + [**ä»£ç å¼€æº**](https://github.com/HKUST-Aerial-Robotics/DenseSurfelMapping) 
+ [ ] **[15]** Zhao Y, Xu S, Bu S, et al. [**GSLAM: A General SLAM Framework and Benchmark**](https://arxiv.org/pdf/1902.07995.pdf)[J]. arXiv preprint arXiv:1902.07995, **2019**.
    + <font color = blue>é€šç”¨SLAMæ¡†æ¶å’ŒåŸºå‡†</font>
    + è¥¿åŒ—å·¥ä¸šå¤§å­¦ï¼Œè‡ªåŠ¨åŒ–æ‰€ &emsp; [**ä»£ç å¼€æº**](https://github.com/zdzhaoyong/GSLAM)
+ [ ] **[16]** Nellithimaru A K, Kantor G A. [**ROLS: Robust Object-Level SLAM for Grape Counting**](http://openaccess.thecvf.com/content_CVPRW_2019/papers/CVPPP/Nellithimaru_ROLS__Robust_Object-Level_SLAM_for_Grape_Counting_CVPRW_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. **CVPR2019**: 0-0.
    + <font color = blue>ç”¨äºè‘¡è„è®¡æ•°çš„é²æ£’çš„ç‰©ä½“çº§ SLAM</font>
    + CMU
+ [ ] **[17]** Nejad Z Z, Ahmadabadian A H. [**ARM-VO: an efficient monocular visual odometry for ground vehicles on ARM CPUs**](https://link.springer.com/article/10.1007/s00138-019-01037-5)[J]. Machine Vision and Applications, **2019**: 1-10.
    + <font color = blue>ARM CPUä¸Šåœ°é¢è½¦è¾†çš„é«˜æ•ˆå•ç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + ä¼Šæœ—å¾·é»‘å…°æ‰˜è¥¿æŠ€æœ¯å¤§å­¦ 
    + [**ä»£ç å¼€æº**](https://github.com/zanazakaryaie/ARM-VO) &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼ŒJCR  Q2Q3ï¼ŒIF 1.3
+ [ ] **[18]** Aloise I, Della Corte B, Nardi F, et al. [**Systematic Handling of Heterogeneous Geometric Primitives in Graph-SLAM Optimization**](http://rss2019.informatik.uni-freiburg.de/papers/0285_FI.pdf)[J]. IEEE Robotics and Automation Letters, **2019**, 4(3): 2738-2745.
    + <font color = blue>SLAM å›¾ä¼˜åŒ–ä¸­å¼‚æ„å‡ ä½•åŸºå…ƒçš„ç³»ç»Ÿå¤„ç†</font>
    + ç½—é©¬å¤§å­¦
    + [**ä»£ç å¼€æº**](https://srrg.gitlab.io/sashago-website/index.html)
+ [ ] **[19]** Guo R, Peng K, Fan W, et al. [**RGB-D SLAM Using Pointâ€“Plane Constraints for Indoor Environments**](https://www.mdpi.com/1424-8220/19/12/2721)[J]. Sensors, **2019**, 19(12): 2721.
    + <font color = blue>å®¤å†…ç¯å¢ƒä¸­ä½¿ç”¨ç‚¹-å¹³é¢çº¦æŸçš„ RGB-D SLAM</font>
    + å›½é˜²ç§‘å¤§ &emsp; æœŸåˆŠï¼šå¼€æºæœŸåˆŠï¼Œä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2Q3ï¼ŒIF 3.0
+ [ ] **[20]** Laidlow T, Czarnowski J, Leutenegger S. [**DeepFusion: Real-Time Dense 3D Reconstruction for Monocular SLAM using Single-View Depth and Gradient Predictions**](https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dyson-robotics-lab/tlaidlow_etal_icra2019.pdf)[J].**2019**.
    + <font color = blue>DeepFusionï¼šä½¿ç”¨å•è§†å›¾æ·±åº¦å’Œæ¢¯åº¦é¢„æµ‹çš„å•çœ¼SLAMå®æ—¶å¯†é›†ä¸‰ç»´é‡å»º</font>
    + [å¸å›½ç†å·¥å­¦é™¢çš„æˆ´æ£®æœºå™¨äººå®éªŒå®¤](https://www.imperial.ac.uk/dyson-robotics-lab/publications/)
+ [x] **[21]** Saeedi S, Carvalho E, Li W, et al. [**Characterizing Visual Localization and Mapping Datasets**](https://www.sajad-saeedi.ca/uploads/3/8/5/9/38597021/saeedi_icra2019.pdf)[C]//2019 IEEE International Conference on Robotics and Automation (**ICRA**). **2019**.
    + <font color = blue>æè¿°å¯è§†åŒ–å®šä½äºå»ºå›¾çš„æ•°æ®é›†</font>
    + [å¸å›½ç†å·¥å­¦é™¢è®¡ç®—æœºç³»](https://www.imperial.ac.uk/computing/research/visual-computing/) &emsp; [æ•°æ®é›†åœ°å€](http://wbli.me/lmdata/)
+ [ ] **[22]** Sun T, Sun Y, Liu M, et al. [**Movable-Object-Aware Visual SLAM via Weakly Supervised Semantic Segmentation**](https://arxiv.org/pdf/1906.03629.pdf)[J]. arXiv preprint arXiv:1906.03629, **2019**.
    + <font color = blue>é€šè¿‡å¼±ç›‘ç£è¯­ä¹‰åˆ†å‰²çš„å¯ç§»åŠ¨å¯¹è±¡æ„ŸçŸ¥è§†è§‰SLAM</font>
    + æ¸¯ç§‘å¤§
+ [x] **[23]** Ghaffari M, Clark W, Bloch A, et al. [**Continuous Direct Sparse Visual Odometry from RGB-D Images**](https://arxiv.org/pdf/1904.02266.pdf)[J]. arXiv preprint arXiv:1904.02266, **2019**.
    + <font color = blue>RGB-Då›¾åƒè¿ç»­ç›´æ¥ç¨€ç–è§†è§‰é‡Œç¨‹è®¡</font>
    + å¯†æ­‡æ ¹å¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/MaaniGhaffari/cvo-rgbd)
+ [ ] **[24]** Houseago C, Bloesch M, Leutenegger S. [**KO-Fusion: Dense Visual SLAM with Tightly-Coupled Kinematic and Odometric Tracking**](https://www.imperial.ac.uk/media/imperial-college/research-centres-and-groups/dyson-robotics-lab/chouseago_etal_icra2019.pdf)[J]. **2019**
    + <font color = blue>KO-Fusionï¼šå…·æœ‰ç´§è€¦åˆè¿åŠ¨å’Œæµ‹è·è·Ÿè¸ªçš„ç¨ å¯†è§†è§‰SLAM</font>
    + å¸å›½ç†å·¥å­¦é™¢æˆ´æ£®æœºå™¨äººå®éªŒå®¤
+ [x] **[25]** Iqbal A, Gans N R. [**Localization of Classified Objects in SLAM using Nonparametric Statistics and Clustering**](http://comoros.cs.columbia.edu:8080/IROS_2018_proceedings/media/files/1203.pdf)[C]//2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (**IROS**). IEEE, **2018**: 161-168.
    + <font color = blue>åœ¨éå‚æ•°å’Œèšç±»çš„ SLAM ä¸­ä½¿ç”¨ç±»åˆ«ç‰©ä½“è¿›è¡Œå®šä½</font>
    + å¾·å…‹è¨æ–¯å¤§å­¦è®¡ç®—æœºå·¥ç¨‹å­¦é™¢
+ [x] **[26]** [**Semantic Mapping for View-Invariant Relocalization**](http://www.cim.mcgill.ca/~jimmyli/pubs/li2019icra.pdf). **2019**
    + <font color = blue>ç”¨äºè§†è§’ä¸å˜é‡å®šä½çš„è¯­ä¹‰åœ°å›¾</font>
    + åŠ æ‹¿å¤§è’™ç‰¹åˆ©å°”éº¦å‰å°”å¤§å­¦
+ [ ] **[27]** Hou Z, Ding Y, Wang Y, et al. [**Visual Odometry for Indoor Mobile Robot by Recognizing Local Manhattan Structures**](https://link.springer.com/chapter/10.1007/978-3-030-20873-8_11)[C]//Asian Conference on Computer Vision. Springer, Cham, **ACCV2018**: 168-182.
    + <font color = blue>é€šè¿‡è¯†åˆ«æ›¼å“ˆé¡¿ç»“æ„çš„å®¤å†…æœºå™¨äººè§†è§‰é‡Œç¨‹è®¡</font>
    + å—äº¬ç†å·¥å¤§å­¦
    
---

#### 2. Semantic/Deep SLAM

+ [ ] [28] Guclu O, Caglayan A, Burak Can A. [**RGB-D Indoor Mapping Using Deep Features**](http://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Guclu_RGB-D_Indoor_Mapping_Using_Deep_Features_CVPRW_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. **CVPR 2019**: 0-0.
    + <font color = blue>ä½¿ç”¨æ·±åº¦ç‰¹å¾çš„ RGB-D å®¤å†…å»ºå›¾</font>
    + åœŸè€³å…¶ Ahi Evran University
+ [x] [29] Sualeh M, Kim G W. [**Simultaneous Localization and Mapping in the Epoch of Semantics: A Survey**](https://link.springer.com/content/pdf/10.1007%2Fs12555-018-0130-x.pdf)[J]. International Journal of Control, Automation and Systems, **2019**, 17(3): 729-742.
    + <font color = blue>è¯­ä¹‰æ—¶ä»£çš„ SLAM ç»¼è¿°</font>
    + éŸ©å›½å¿ åŒ—å›½ç«‹å¤§å­¦
    
---

#### 3. AR & MR & VR

+ [ ] [30] Guerra W, Tal E, Murali V, et al. FlightGoggles: Photorealistic Sensor Simulation for Perception-driven Robotics using Photogrammetry and Virtual Reality[J]. arXiv preprint arXiv:1905.11377, 2019.
    + <font color = blue>ä½¿ç”¨æ‘„å½±æµ‹é‡å’Œè™šæ‹Ÿç°å®æ„ŸçŸ¥é©±åŠ¨æœºå™¨äººçš„é€¼çœŸä¼ æ„Ÿå™¨æ¨¡æ‹Ÿ</font>
    + MIT  &emsp;[**ä»£ç å¼€æº**](https://github.com/mit-fast/FlightGoggles) &emsp; [é¡¹ç›®ä¸»é¡µ](https://flightgoggles.mit.edu/)
+ [ ] [31] Stotko P, Krumpen S, Hullin M B, et al. [**SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for Immersive Multi-Client Live Telepresence**](https://arxiv.org/pdf/1805.03709.pdf)[J]. IEEE transactions on visualization and computer graphics, **2019**, 25(5): 2102-2112.
    + <font color = blue>SLAMCastï¼šç”¨äºæ²‰æµ¸å¼å¤šå®¢æˆ·ç«¯å®æ—¶è¿œç¨‹å‘ˆç°çš„å¤§è§„æ¨¡å®æ—¶3Dé‡å»ºå’Œæµåª’ä½“</font>
    + æ³¢æ©å¤§å­¦ &emsp; [Google Scholar](https://scholar.google.com/citations?user=nMLx_s0AAAAJ&hl=zh-CN&oi=sra)

---

#### 4. Learning others

+ [ ] [32] JÃ¶rgensen E, Zach C, Kahl F. [**Monocular 3D Object Detection and Box Fitting Trained End-to-End Using Intersection-over-Union Loss**](https://arxiv.org/pdf/1906.08070.pdf)[J]. arXiv preprint arXiv:1906.08070, **2019**.
    + <font color = blue>å•ç›®ä¸‰ç»´ç‰©ä½“æ£€æµ‹å’Œä½¿ç”¨äº¤å‰è”åˆæŸå¤±çš„ç«¯åˆ°ç«¯ç«‹æ–¹æ¡†æ‹Ÿåˆ</font>
    + ç‘å…¸æŸ¥å°”å§†æ–¯ç†å·¥å¤§å­¦ &emsp;[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=G3Aqhd5K2GE&list=PL4jJwJr7UjMb4bzLwUGHdVmhfNS2Ads_x)
+ [ ] [33] Wang B H, Chao W L, Wang Y, et al. [**LDLS: 3-D Object Segmentation Through Label Diffusion From 2-D Images**](https://ieeexplore.ieee.org/abstract/document/8735751)[J]. IEEE Robotics and Automation Letters, **2019**, 4(3): 2902-2909.
    + <font color = blue>é€šè¿‡äºŒç»´å›¾åƒçš„æ ‡ç­¾æ‰©æ•£è¿›è¡Œä¸‰ç»´ç‰©ä½“åˆ†å‰²</font>
    + åº·å¥ˆå°”å¤§å­¦
    + [**ä»£ç å¼€æº**](https://github.com/brian-h-wang/LDLS) &emsp; æœŸåˆŠï¼šIEEE Robotics and Automation ä¸­ç§‘é™¢äºŒåŒº ï¼ŒJCR Q1Q2 ï¼ŒIF 4.8
+ [x] [34] Yang B, Wang J, Clark R, et al. [**Learning Object Bounding Boxes for 3D Instance Segmentation on Point Clouds**](https://arxiv.org/pdf/1906.01140.pdf)[J]. arXiv preprint arXiv:1906.01140, **2019**.
    + <font color = blue>å­¦ä¹ ç‚¹äº‘ä¸Šä¸‰ç»´å®ä¾‹åˆ†å‰²çš„ç›®æ ‡ 3D è¾¹ç•Œæ¡†</font>
    + ç‰›æ´¥å¤§å­¦ &emsp; [Google Scholar](https://scholar.google.com/citations?user=VqUAqz8AAAAJ&hl=zh-CN&oi=sra)
    + [**ä»£ç å¼€æº**](https://github.com/Yang7879/3D-BoNet) 
+ [ ] [35] Ahmed, Mariam. (**2019**). [**Pushing Boundaries with 3D Boundaries for Object Recognition**](https://www.researchgate.net/publication/333676944_Pushing_Boundaries_with_3D_Boundaries_for_Object_Recognition). 10.13140/RG.2.2.33079.98728. 
    + <font color = blue>åˆ©ç”¨ä¸‰ç»´è¾¹ç•Œæ¡†æ¨åŠ¨è¾¹ç•Œè¿›è¡Œç‰©ä½“æ£€æµ‹</font>
    + æ–°åŠ å¡å›½ç«‹å¤§å­¦
+ [ ] [36] Wu D, Zhuang Z, Xiang C, et al. [**6D-VNet: End-To-End 6-DoF Vehicle Pose Estimation From Monocular RGB Images**](http://openaccess.thecvf.com/content_CVPRW_2019/papers/WAD/Wu_6D-VNet_End-To-End_6-DoF_Vehicle_Pose_Estimation_From_Monocular_RGB_Images_CVPRW_2019_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. **CVPR2019**: 0-0.
    + <font color = blue>6D-VNetï¼šå•ç›® RGB å›¾åƒçš„ç«¯åˆ°ç«¯ 6 è‡ªç”±åº¦è½¦è¾†å§¿æ€ä¼°è®¡</font>
    + æ·±åœ³å¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/stevenwudi/6DVNET)

---

### 2019 å¹´ 6 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ21 ç¯‡ï¼‰

#### 1. Geometric SLAM

+ [x] **[1]** [**A Modular Optimization Framework for Localization and Mapping**](https://ingmec.ual.es/~jlblanco/papers/blanco2019mola_rss2019.pdf). [C] RSS **2019**
    + <font color = blue>ç”¨äºè·Ÿè¸ªä¸å»ºå›¾çš„æ¨¡å—åŒ–ä¼˜åŒ–æ¡†æ¶</font>
    + è¥¿ç­ç‰™é˜¿å°”æ¢…é‡Œäºšå¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?hl=zh-CN&user=bhDtzKgAAAAJ)
    + [**ä»£ç å¼€æº**](https://github.com/MOLAorg/mola)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰ &emsp; [æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=Bb92aMBJR44)
+ [x] **[2]** Wang C, Guo X. [**Efficient Plane-Based Optimization of Geometry and Texture for Indoor RGB-D Reconstruction**](https://arxiv.org/pdf/1905.08853.pdf)[J]. arXiv preprint arXiv:1905.08853, **2019**.
    + <font color = blue>ç”¨äºå®¤å†… RGB-D é‡å»ºçš„åŸºäºå¹³é¢çš„å‡ ä½•å’Œçº¹ç†ä¼˜åŒ–</font>
    + å¾·å…‹è¨æ–¯å¤§å­¦è¾¾æ‹‰æ–¯åˆ†æ ¡ &emsp; [Google Scholor](https://scholar.google.com/citations?user=PXm3u3gAAAAJ&hl=zh-CN&oi=sra)
    + [**ä»£ç å¼€æº**](https://github.com/chaowang15/plane-opt-rgbd)
+ [x] **[3]** Wang J, Song J, Zhao L, et al. [**A submap joining algorithm for 3D reconstruction using an RGB-D camera based on point and plane features**](https://www.sciencedirect.com/science/article/pii/S0921889018302033)[J]. Robotics and Autonomous Systems, **2019**.
    + <font color = blue>ä¸€ç§åŸºäºç‚¹ç‰¹å¾å’Œå¹³é¢ç‰¹å¾çš„RGB-Dç›¸æœºä¸‰ç»´é‡å»ºå­åœ°å›¾è¿æ¥ç®—æ³•</font>
    + æ‚‰å°¼ç§‘æŠ€å¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=rf8d0o4AAAAJ&hl=zh-CN&oi=sra) &emsp; ä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2ï¼ŒIF 2.809
+ [x] **[4]** Joshi N, Sharma Y, Parkhiya P, et al. [**Integrating Objects into Monocular SLAM: Line Based Category Specific Models**](https://arxiv.org/pdf/1905.04698.pdf)[J]. arXiv preprint arXiv:1905.04698, **2019**.
    + <font color = blue>å°†ç‰©ä½“é›†æˆåˆ°å•ç›® SLAM ä¸­ï¼šåŸºäºçº¿çš„ç‰¹å®šç±»åˆ«æ¨¡å‹</font>
    + å°åº¦æµ·å¾·æ‹‰å·´å¤§å­¦
    + Parkhiya P, Khawad R, Murthy J K, et al. **Constructing Category-Specific Models for Monocular Object-SLAM**[C]//2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018: 1-9.
+ [ ] **[5]** Niu J, Qian K. [**A hand-drawn map-based navigation method for mobile robots using objectness measure**](https://journals.sagepub.com/doi/pdf/10.1177/1729881419846339)[J]. International Journal of Advanced Robotic Systems, **2019**, 16(3): 1729881419846339.
    + <font color = blue>ä¸€ç§åŸºäºæ‰‹ç»˜åœ°å›¾çš„ä½¿ç”¨ç‰©ä½“åº¦é‡ç§»åŠ¨æœºå™¨äººå¯¼èˆªæ–¹æ³•</font>
    + ä¸œå—å¤§å­¦ &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼Œ JCR Q4ï¼ŒIF 1.0
+ [x] **[6]** [**Robust Object-based SLAM for High-speed Autonomous Navigation**](http://groups.csail.mit.edu/rrg/papers/OkLiu19icra.pdf). **2019**
    + <font color = blue>åŸºäºé²æ£’çš„ç‰©ä½“ SLAM çš„é«˜é€Ÿå¯¼èˆªç³»ç»Ÿ</font>
    + MIT
+ [ ] **[7]** Cheng J, Sun Y, Meng M Q H. [**Robust Semantic Mapping in Challenging Environments**](https://www.cambridge.org/core/journals/robotica/article/robust-semantic-mapping-in-challenging-environments/19F9EA5C806AFC10377F13ABDEDA68EE)[J]. Robotica, 1-15, **2019**.
    + <font color = blue>å…·æœ‰æŒ‘æˆ˜ç¯å¢ƒä¸‹çš„é²æ£’çš„è¯­ä¹‰å»ºå›¾</font>
    + é¦™æ¸¯ä¸­æ–‡å¤§å­¦ï¼Œé¦™æ¸¯ç§‘æŠ€å¤§å­¦ &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢å››åŒºï¼Œ JCR Q4ï¼ŒIF 1.267
+ [x] **[8]** Sun B, Mordohai P. [**Oriented Point Sampling for Plane Detection in Unorganized Point Clouds**](https://arxiv.org/pdf/1905.02553.pdf)[J]. arXiv preprint arXiv:1905.02553, **2019**.
    + <font color = blue>æ— ç»„ç»‡ç‚¹äº‘ä¸­å¹³é¢æ£€æµ‹çš„å®šå‘ç‚¹é‡‡æ ·</font>
    + ç¾å›½å²è’‚æ–‡æ–¯ç†å·¥å­¦é™¢
+ [x] **[9]** Palazzolo E, Behley J, Lottes P, et al. [**ReFusion: 3D Reconstruction in Dynamic Environments for RGB-D Cameras Exploiting Residuals**](https://arxiv.org/pdf/1905.02082.pdf)[J]. arXiv preprint arXiv:1905.02082, **2019**.
    + <font color = blue>ReFusion åˆ©ç”¨æ®‹å·®çš„ RGB-D ç›¸æœºåŠ¨æ€ç¯å¢ƒä¸‹çš„ä¸‰ç»´é‡å»º</font>
    + å¾·å›½æ³¢æ©å¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/PRBonn/refusion)
    
---

#### 2. Semantic/Deep SLAM

+ [x] **[10]** Goldman M, Hassner T, Avidan S. [**Learn Stereo, Infer Mono: Siamese Networks for Self-Supervised, Monocular, Depth Estimation**](https://arxiv.org/pdf/1905.00401.pdf)[J]. arXiv preprint arXiv:1905.00401, **2019**.
    + <font color = blue>å­¦ä¹ åŒç›®ï¼Œæ¨æ–­å•ç›®ï¼šç”¨äºè‡ªæˆ‘ç›‘ç£ï¼Œå•ç›®ï¼Œæ·±åº¦ä¼°è®¡çš„è¿ä½“ç½‘ç»œ</font>
    + ä»¥è‰²åˆ—ç‰¹æ‹‰ç»´å¤«å¤§å­¦ &emsp; [**ä»£ç å¼€æº**](https://github.com/mtngld/lsim)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [ ] [11] Mukherjee A, Chakaborty S, Saha S K. [**Detection of loop closure in SLAM: A DeconvNet based approach**](https://www.sciencedirect.com/science/article/pii/S1568494619302339)[J]. Applied Soft Computing, **2019**.
    + <font color = blue>åŸºäº DeconvNet çš„ SLAM é—­ç¯æ£€æµ‹æ–¹æ³•</font>
    + å°åº¦è´¾è¾¾æ™®å¤§å­¦ &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 4.0

---

#### 3. ä¼ æ„Ÿå™¨èåˆ

+ [ ] **[12]** Huang K, Xiao J, Stachniss C. [**Accurate Direct Visual-Laser Odometry with Explicit Occlusion Handling and Plane Detection**](http://www.ipb.uni-bonn.de/wp-content/papercite-data/pdf/huang2019icra.pdf)[C]//Proceedings of the IEEE International Conference on Robotics and Automation (**ICRA**). **2019**.
    + <font color = blue>å…·æœ‰æ˜¾å¼é®æŒ¡å¤„ç†å’Œå¹³é¢æ£€æµ‹çš„ç²¾ç¡®ç›´æ¥è§†è§‰æ¿€å…‰æµ‹è·</font>
    + å›½é˜²ç§‘å¤§
+ [x] **[13]** Shan Z, Li R, Schwertfeger S. [**RGBD-Inertial Trajectory Estimation and Mapping for Ground Robots**](https://www.mdpi.com/1424-8220/19/10/2251)[J]. Sensors, **2019**, 19(10): 2251.
    + <font color = blue>ç”¨äºåœ°é¢æœºå™¨äººçš„ RGBD-æƒ¯å¯¼è½¨è¿¹ä¼°è®¡ä¸å»ºå›¾</font>
    + ä¸Šæµ·ç§‘æŠ€å¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=Y2olJ9kAAAAJ&hl=zh-CN&oi=sra)
    + [ä»£ç å¼€æº](https://github.com/STAR-Center/VINS-RGBD) &emsp; [æ¼”ç¤ºè§†é¢‘](https://robotics.shanghaitech.edu.cn/datasets/VINS-RGBD) &emsp; æœŸåˆŠï¼šå¼€æºï¼Œä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2Q3
+ [ ] **[14]** Xiong X, Chen W, Liu Z, et al. [**DS-VIO: Robust and Efficient Stereo Visual Inertial Odometry based on Dual Stage EKF**](https://arxiv.org/pdf/1905.00684.pdf)[J]. arXiv preprint arXiv:1905.00684, **2019**.
    + <font color = blue>DS-VIOï¼šåŸºäºåŒé‡ EKF çš„ç¨³å¥é«˜æ•ˆçš„åŒç›®è§†è§‰æƒ¯æ€§æµ‹è·ä»ª</font>
    + å“ˆå·¥å¤§ &emsp; [Google Scholor](https://scholar.google.com/citations?user=Dhnz264AAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[15]** Xing B Y, Pan F, Feng X X, et al. [**Autonomous Landing of a Micro Aerial Vehicle on a Moving Platform Using a Composite Landmark**](https://www.hindawi.com/journals/ijae/2019/4723869/abs/)[J]. International Journal of Aerospace Engineering, 2019, **2019**.
    + <font color = blue>ä½¿ç”¨å¤åˆè·¯æ ‡çš„åœ¨ç§»åŠ¨å¹³å°ä¸Šè‡ªä¸»ç€é™†çš„å¾®å‹é£è¡Œå™¨</font>
    + åŒ—äº¬ç†å·¥å¤§å­¦

---

#### 4. AR & MR & VR

+ [x] **[16]** Ozawa T, Nakajima Y, Saito H. [**Simultaneous 3D Tracking and Reconstruction of Multiple Moving Rigid Objects**](https://ieeexplore.ieee.org/abstract/document/8709158)[C]//2019 12th Asia Pacific Workshop on Mixed and Augmented Reality (APMAR). IEEE, **2019**: 1-5.
    + <font color = blue>å¤šè¿åŠ¨åˆšä½“è¿åŠ¨ä¸‰ç»´è·Ÿè¸ªä¸é‡å»º</font>
    + æ—¥æœ¬åº†åº”ä¹‰å¡¾å¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=lX5lY8YAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[17]** Ens B, Lanir J, Tang A, et al. [**Revisiting Collaboration through Mixed Reality: The Evolution of Groupware**](https://www.sciencedirect.com/science/article/pii/S1071581919300606)[J]. International Journal of Human-Computer Studies, **2019**.
    + <font color = blue>é€šè¿‡æ··åˆç°å®é‡æ¸©åä½œ:ç¾¤ä»¶çš„å‘å±•</font>
    + æ¾³å¤§åˆ©äºšè«çº³ä»€å¤§å­¦ï¼Œä»¥è‰²åˆ—æµ·æ³•å¤§å­¦ï¼ŒåŠ æ‹¿å¤§å¡å°”åŠ é‡Œå¤§å­¦ &emsp; æœŸåˆŠï¼šä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q1Q2ï¼ŒIF 2.3

---

#### 5. Learning others

+ [x] **[18]** Song S, Yu F, Zeng A, et al. [**Semantic scene completion from a single depth image**](http://openaccess.thecvf.com/content_cvpr_2017/papers/Song_Semantic_Scene_Completion_CVPR_2017_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **CVPR 2017**: 1746-1754.
    + <font color = blue>ä»å•ä¸ªæ·±åº¦å›¾åƒå®Œæˆè¯­ä¹‰åœºæ™¯ç†è§£</font>
    + æ™®æ—æ–¯é¡¿å¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=5031vK4AAAAJ&hl=zh-CN&oi=sra)
    + [**ä»£ç å¼€æº**](https://github.com/shurans/sscnet) &emsp;[é¡¹ç›®ä¸»é¡µ](http://sscnet.cs.princeton.edu/)
    + Wang H, Sridhar S, Huang J, et al. [**Normalized Object Coordinate Space for Category-Level 6D Object Pose and Size Estimation**](https://arxiv.org/pdf/1901.02970.pdf)[J]. arXiv preprint arXiv:1901.02970, **2019**.
    + [**A Survey of 3D Indoor Scene Synthesis**](https://www.researchgate.net/publication/333135099_A_Survey_of_3D_Indoor_Scene_Synthesis)[J]. Journal of Computer Science and Technology 34(3):594-608 Â· May **2019**
+ [ ] **[19]** Howard-Jenkins H, Li S, Prisacariu V. [**Thinking Outside the Box: Generation of Unconstrained 3D Room Layouts**](https://arxiv.org/pdf/1905.03105.pdf)[C]//Asian Conference on Computer Vision. Springer, Cham, **ACCV2018**: 432-448.
    + <font color = blue>è·³å‡ºè¾¹ç•Œæ¡†çš„æ€è€ƒï¼šæ— çº¦æŸ 3D æˆ¿é—´å¸ƒå±€çš„ç”Ÿæˆ</font>
    + ç‰›æ´¥å¤§å­¦

---

#### 6. Others

+ [ ] **[20]** Qian Y, Ramalingam S, Elder J H. [**LS3D: Single-View Gestalt 3D Surface Reconstruction from Manhattan Line Segments**](http://www.elderlab.yorku.ca/wp-content/uploads/2018/12/LS3DACCV18.pdf)[C]//Asian Conference on Computer Vision. Springer, Cham, **ACCV 2018**: 399-416.
    + <font color = blue>LS3D: å•è§†å›¾æ ¼å¼å¡”ä¸‰ç»´è¡¨é¢é‡å»ºæ›¼å“ˆé¡¿çº¿æ®µ</font>
    + è‹±å›½çº¦å…‹å¤§å­¦ï¼Œç¾å›½çŠ¹ä»–å¤§å­¦  &emsp; [Google Scholor](https://scholar.google.com/citations?user=gmpm0a8AAAAJ&hl=zh-CN&oi=sra)  &emsp; ä¼šè®® ACCVï¼šCCF äººå·¥æ™ºèƒ½ C ç±»ä¼šè®®
+ [ ] **[21]** Deng X, Mousavian A, Xiang Y, et al. [**PoseRBPF: A Rao-Blackwellized Particle Filter for 6D Object Pose Tracking**](https://arxiv.org/pdf/1905.09304.pdf)[J]. arXiv preprint arXiv:1905.09304, **2019**.
    + <font color = blue>ä¸€ç§ç”¨äº6Dç›®æ ‡å§¿æ€è·Ÿè¸ªçš„ Rao-Blackwellized ç²’å­æ»¤æ³¢å™¨</font>
    + è‹±ä¼Ÿè¾¾ï¼Œåç››é¡¿å¤§å­¦ï¼Œæ–¯å¦ç¦å¤§å­¦  &emsp; [æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=lE5gjzRKWuA&feature=youtu.be)

---

### 2019 å¹´ 5 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ51 ç¯‡ï¼‰

#### 1. Geometric SLAM
+ [x] **[1]** Delmas P, Gee T. [**Stereo camera visual odometry for moving urban environments**](https://content.iospress.com/articles/integrated-computer-aided-engineering/ica190598)[J]. Integrated Computer-Aided Engineering, **2019** (Preprint): 1-14.
    + <font color = blue>ç”¨äºç§»åŠ¨åŸå¸‚ç¯å¢ƒçš„åŒç›®é‡Œç¨‹è®¡</font>
    + å¥¥å…‹å…°å¤§å­¦ &emsp; ä¸­ç§‘é™¢äºŒåŒº JCR Q2
+ [ ] **[2]** Guo R, Zhou D, Peng K, et al. [**Plane Based Visual Odometry for Structural and Low-Texture Environments Using RGB-D Sensors**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8679500)[C]//**2019** IEEE International Conference on Big Data and Smart Computing (BigComp). IEEE, 2019: 1-4.
    + <font color = blue>ç”¨äºç»“æ„åŒ–ä½çº¹ç†çš„å¹³é¢ RGB-D è§†è§‰é‡Œç¨‹è®¡</font>
    + å›½é˜²ç§‘å¤§
+ [ ] **[3]** X Wang, F Xue, Z Yan, W Dong, Q Wang, H Zha. [**Continuous-time Stereo Visual Odometry Based on Dynamics Modelâ‹†**](https://www.researchgate.net/profile/Wang_Xin122/publication/332103736_Continuous-time_Stereo_Visual_Odometry_Based_on_Dynamics_Model/links/5ca04e17299bf11169521b1e/Continuous-time-Stereo-Visual-Odometry-Based-on-Dynamics-Model.pdf) **2019**
    + <font color = blue>åŸºäºåŠ¨åŠ›å­¦æ¨¡å‹çš„è¿ç»­æ—¶é—´çš„åŒç›®è§†è§‰é‡Œç¨‹è®¡</font>
    + åŒ—äº¬å¤§å­¦ï¼Œä¸Šæµ·äº¤å¤§
+ [x] **[4]** Strecke M, StÃ¼ckler J. [**EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association**](https://arxiv.org/pdf/1904.11781.pdf)[J]. arXiv preprint arXiv:1904.11781, **2019**.
    + <font color = blue>å…·æœ‰æ¦‚ç‡æ•°æ®å…³è”çš„**åŠ¨æ€ç‰©ä½“çº§ SLAM**</font>
    + å¾·å›½é©¬å…‹æ–¯æ™®æœ—å…‹æ™ºèƒ½ç³»ç»Ÿç ”ç©¶æ‰€ &emsp; [å®éªŒå®¤ä¸»é¡µ](https://ev.is.tuebingen.mpg.de/)
    + Usenko V, Demmel N, Schubert D, et al. [Visual-Inertial Mapping with Non-Linear Factor Recovery](https://arxiv.org/pdf/1904.06504.pdf)[J]. arXiv preprint arXiv:1904.06504, 2019.
    + å¥¥å…‹å…°å¤§å­¦ &emsp; æœŸåˆŠ Integrated Computer-Aided Engineering ä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 3.667
+ [ ] **[5]** Guclu O, Can A B. [**k-SLAM: A fast RGB-D SLAM approach for large indoor environments**](https://www.sciencedirect.com/science/article/pii/S107731421930058X)[J]. Computer Vision and Image Understanding, **2019**.
    + <font color = blue>å¤§å‹å®¤å†…ç¯å¢ƒçš„å¿«é€Ÿ RGB-D SLAM æ–¹æ³•</font>
    + åœŸè€³å…¶å“ˆè¥¿å¾·ä½©å¤§å­¦ &emsp; JCR Q2ï¼ŒIF 2.776
+ [ ] **[6]** Yokozuka M, Oishi S, Simon T, et al. [**VITAMIN-E: VIsual Tracking And Mapping with Extremely Dense Feature Points**](https://arxiv.org/pdf/1904.10324.pdf)[J]. arXiv preprint arXiv:1904.10324, **2019**.
    + <font color = blue>å…·æœ‰**æåº¦å¯†åº¦çš„ç‰¹å¾ç‚¹**çš„è§†è§‰è·Ÿè¸ªä¸å»ºå›¾</font>
    + æ—¥æœ¬å›½å®¶å…ˆè¿›å·¥ä¸šç§‘å­¦æŠ€æœ¯ç ”ç©¶æ‰€ &emsp; 
+ [x] **[7]** Zubizarreta J, Aguinaga I, Montiel J M M. [**Direct Sparse Mapping**](https://arxiv.org/pdf/1904.06577.pdf)[J]. arXiv preprint arXiv:1904.06577, **2019**.
    + <font color = blue>**ç›´æ¥æ³•ç¨€ç–å»ºå›¾**</font>
    + è¥¿ç­ç‰™è¨æ‹‰æˆˆè¨å¤§å­¦ &emsp; [ä»£ç å¼€æº](https://github.com/jzubizarreta/dsm)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
    + ä½œè€… 2018 å¹´ ECCV ä¸€ç¯‡æ–‡ç« ï¼šå¯å˜å½¢è´´å›¾ä¸­ SLAM çš„ç›¸æœºè·Ÿè¸ª [Camera Tracking for SLAM in Deformable Maps](http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Lamarca_Camera_Tracking_for_SLAM_in_Deformable_Maps_ECCVW_2018_paper.pdf)
+ [ ] **[8]** Feng G, Ma L, Tan X. [**Line Model-Based Drift Estimation Method for Indoor Monocular Localization**](https://ieeexplore.ieee.org/abstract/document/8690676)[C]//2018 IEEE 88th Vehicular Technology Conference (VTC-Fall). IEEE, **2019**: 1-5.
    + <font color = blue>åŸºäº**çº¿æ¨¡å‹**çš„å®¤å†…å•ç›®å®šä½æ¼‚ç§»ä¼°è®¡æ–¹æ³•</font>
    + å“ˆå·¥å¤§ &emsp;  VTC æ— çº¿é€šä¿¡ä¼šè®®ï¼Œä¸€å¹´ä¸¤å±Š
+ [x] **[9]** Castro G, Nitsche M A, Pire T, et al. [**Efficient on-board Stereo SLAM through constrained-covisibility strategies**](https://www.researchgate.net/profile/Gaston_Castro/publication/332147108_Efficient_on-board_Stereo_SLAM_through_constrained-covisibility_strategies/links/5cacb327a6fdccfa0e7c3e4b/Efficient-on-board-Stereo-SLAM-through-constrained-covisibility-strategies.pdf)[J]. Robotics and Autonomous Systems, 2019.
    + <font color = blue>é€šè¿‡çº¦æŸ-åˆä½œç­–ç•¥å®ç°é«˜æ•ˆåŒç›®SLAM</font>
    + é˜¿æ ¹å»·å¸ƒå®œè¯ºæ–¯è‰¾åˆ©æ–¯å¤§å­¦åšå£« &emsp; åŒç›® PTAM ä½œè€…
+ [ ] **[10]** Canovas B, Rombaut M, NÃ¨gre A, et al. [**A Coarse and Relevant 3D Representation for Fast and Lightweight RGB-D Mapping**](https://hal.archives-ouvertes.fr/hal-02068740/document)[C]//VISAPP 2019-International Conference on Computer Vision Theory and Applications. 2019.
    + <font color = blue>åº”ç”¨äºå¿«é€Ÿç²—ç³™çš„ RGB-D å»ºå›¾çš„ç²—ç³™çš„ç›¸å…³ 3D è¡¨ç¤º</font>
    + æ ¼å‹’è¯ºå¸ƒå°”è®¡ç®—æœºç§‘å­¦å®éªŒå®¤
+ [x] **[11]** Ziquan Lan, Zi Jian Yew, Gim Hee Lee. [**Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes**](http://219.216.82.193/cache/5/03/www.cvlibs.net/297109a6b31ef7d288d6637f31249bd2/Barsan2018ICRA.pdf)[C], **ICRA 2019**.
    + <font color = blue>**é²æ£’çš„å®¤å¤–å¤§åœºæ™¯ç‚¹äº‘é‡å»º**</font>
    + æ–°åŠ å¡å›½ç«‹  &emsp; [ä»£ç å¼€æº](https://github.com/ziquan111/RobustPCLReconstruction)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [ ] **[12]** Shi T, Shen S, Gao X, et al. [**Visual Localization Using Sparse Semantic 3D Map**](https://arxiv.org/pdf/1904.03803.pdf)[J]. arXiv preprint arXiv:1904.03803, **2019**.
    + <font color = blue>åˆ©ç”¨**ç¨€ç–è¯­ä¹‰ä¸‰ç»´åœ°å›¾**è¿›è¡Œå¯è§†åŒ–å®šä½</font>
    + ä¸­å›½ç§‘å­¦é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€æ¨¡å¼è¯†åˆ«å›½å®¶é‡ç‚¹å®éªŒå®¤
+ [x] **[13]** Yang S, Kuang Z F, Cao Y P, et al. [**Probabilistic Projective Association and Semantic Guided Relocalization for Dense Reconstruction**](https://cg.cs.tsinghua.edu.cn/papers/ICRA-2019-densemapping.pdf)[C]//**ICRA 2019**.
    + <font color = blue>ç¨ å¯†é‡å»ºçš„**æ¦‚ç‡æŠ•å½±å…³è”**å’Œ**è¯­ä¹‰å¼•å¯¼é‡å®šä½**</font>
    + æ¸…åå¤§å­¦ &emsp; [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=50194vkAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[14]** Xiao L, Wang J, Qiu X, et al. [**Dynamic-SLAM: Semantic monocular visual localization and mapping based on deep learning in dynamic environment**](https://www.researchgate.net/profile/Linhui_Xiao/publication/332149941_Dynamic-SLAM_Semantic_monocular_visual_localization_and_mapping_based_on_deep_learning_in_dynamic_environment/links/5cb201d04585156cd7949b7b/Dynamic-SLAM-Semantic-monocular-visual-localization-and-mapping-based-on-deep-learning-in-dynamic-environment.pdf)[J]. Robotics and Autonomous Systems, **2019**.
    + <font color = blue>åŸºäºåŠ¨æ€ç¯å¢ƒæ·±åº¦å­¦ä¹ çš„å•ç›® SLAM </font>
    + ä¸­å›½ç§‘å­¦é™¢ç”µå­ç ”ç©¶æ‰€ä¼ æ„Ÿå™¨æŠ€æœ¯å›½å®¶é‡ç‚¹å®éªŒå®¤ &emsp; æœŸåˆŠ ä¸­ç§‘é™¢ä¸‰åŒº JCR Q2
+ [x] **[15]** Zhou L, Wang S, Ye J, et al. [**Do not Omit Local Minimizer: a Complete Solution for Pose Estimation from 3D Correspondences**](https://arxiv.org/pdf/1904.01759.pdf)[J]. arXiv preprint arXiv:1904.01759, **2019**.
    + <font color = blue>ä¸è¦å¿½ç•¥å±€éƒ¨æœ€å°åŒ–ï¼šä¸€ç§å®Œæ•´çš„ 3D å¯¹åº”å§¿æ€ä¼°è®¡è§£å†³æ–¹æ¡ˆ</font>
    + CMU
+ [ ] **[16]** Miraldo P, Saha S, Ramalingam S. [**Minimal Solvers for Mini-Loop Closures in 3D Multi-Scan Alignment**](https://arxiv.org/pdf/1904.03941.pdf)[C]. **CVPR 2019**.
    + <font color = blue>ä¸‰ç»´å¤šè§†è§’å¯¹é½ä¸­**å¾®å‹é—­ç¯**çš„æœ€å°æ±‚è§£å™¨</font>
    + ç¾å›½çŠ¹ä»–å¤§å­¦
    + ICRA 2019ï¼š[POSEAMM: A Unified Framework for Solving Pose Problems using an Alternating Minimization Method](https://arxiv.org/pdf/1904.04858.pdf)
    
---

#### 2. AR/MR ç›¸å…³
+ [x] **[17]** Piao J C, Kim S D. [**Real-time Visual-Inertial SLAM Based on Adaptive Keyframe Selection for Mobile AR Applications**](https://ieeexplore.ieee.org/abstract/document/8698793)[J]. IEEE Transactions on Multimedia, **2019**.
    + <font color = blue>åŸºäºè‡ªé€‚åº”å…³é”®å¸§é€‰æ‹©çš„**ç§»åŠ¨å¢å¼ºç°å®**åº”ç”¨çš„å®æ—¶è§†è§‰æƒ¯æ€§ SLAM</font>
    + ä¸­å›½å»¶è¾¹å¤§å­¦ï¼ŒéŸ©å›½å»¶ä¸–å¤§å­¦ &emsp; æœŸåˆŠ ä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q2ï¼ŒIF 4.368
+ [ ] **[18]** Puigvert J R, Krempel T, Fuhrmann A. [**Localization Service Using Sparse Visual Information Based on Recent Augmented Reality Platforms**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8699237)[C]//2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**: 415-416.
    + <font color = blue>åŸºäºæœ€è¿‘å¢å¼ºç°å®å¹³å°çš„ç¨€ç–è§†è§‰ä¿¡æ¯å®šä½æœåŠ¡</font>
    + Cologne Intelligence  &emsp; ISMARï¼šAR é¢†åŸŸé¡¶çº§ä¼šè®®
+ [ ] **[19]** Zillner J, Mendez E, Wagner D. [**Augmented Reality Remote Collaboration with Dense Reconstruction**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8699225)[C]//2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**: 38-39.
    + <font color = blue>å…·æœ‰ç¨ å¯†é‡å»ºçš„å¢å¼ºç°å®è¿œç¨‹åä½œ</font>
    + DAQRI æ™ºèƒ½çœ¼é•œï¼šhttps://daqri.com/products/smart-glasses/  &emsp; ISMARï¼šCCF è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ B ç±»ä¼šè®®
+ [ ] **[20]** Grandi, JerÃ´nimo & Debarba, Henrique & Maciel, Anderson. [**Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities**](https://www.researchgate.net/profile/Jeronimo_Grandi/publication/332353699_Characterizing_Asymmetric_Collaborative_Interactions_in_Virtual_and_Augmented_Realities/links/5cafa2b492851c8d22e51246/Characterizing-Asymmetric-Collaborative-Interactions-in-Virtual-and-Augmented-Realities.pdf). IEEE Conference on Virtual Reality and 3D User Interfaces. **2019**.
    + <font color = blue>è¡¨å¾è™šæ‹Ÿç°å®å’Œå¢å¼ºç°å®ä¸­çš„éå¯¹ç§°åä½œäº¤äº’</font>
    + å·´è¥¿å—é‡Œå¥¥æ ¼å…°å¾·è”é‚¦å¤§å­¦ &emsp; [æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=6RbXc222spc)
+ [ ] **[21]** Chen Y S, Lin C Y. [**Virtual Object Replacement Based on Real Environments: Potential Application in Augmented Reality Systems**](https://www.mdpi.com/2076-3417/9/9/1797)[J]. Applied Sciences, **2019**, 9(9): 1797.
    + <font color = blue>åŸºäºçœŸå®ç¯å¢ƒçš„è™šæ‹Ÿå¯¹è±¡æ›¿æ¢ï¼šåœ¨å¢å¼ºç°å®ç³»ç»Ÿä¸­çš„æ½œåœ¨åº”ç”¨</font>
    + å°æ¹¾ç§‘æŠ€å¤§å­¦ &emsp; Applied Sciences å¼€æºæœŸåˆŠ
+ [ ] **[22]** Ferraguti F, Pini F, Gale T, et al. [**Augmented reality based approach for on-line quality assessment of polished surfaces**](https://www.sciencedirect.com/science/article/pii/S0736584518305131)[J]. Robotics and Computer-Integrated Manufacturing, **2019**, 59: 158-167.
    + <font color = blue>åŸºäº**å¢å¼ºç°å®**çš„æŠ›å…‰è¡¨é¢åœ¨çº¿è´¨é‡è¯„ä¼°æ–¹æ³•</font>
    + æ„å¤§åˆ©æ‘©å¾·çº³å¤§å­¦ &emsp; ä¸­ç§‘é™¢äºŒåŒºï¼ŒJCR Q1ï¼ŒIF 4.031
+ [ ] **[23]** Wang J, Liu H, Cong L, et al. [**CNN-MonoFusion: Online Monocular Dense Reconstruction Using Learned Depth from Single View**](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8699273)[C]//2018 IEEE International Symposium on Mixed and Augmented Reality Adjunct (ISMAR-Adjunct). IEEE, **2019**: 57-62.
    + <font color = blue>å•è§†å›¾ä¸­å­¦ä¹ æ·±åº¦çš„åœ¨çº¿å•ç›®å¯†é›†é‡å»º</font>
    + ç½‘æ˜“ AR ç ”ç©¶æ‰€ &emsp; ISMARï¼šAR é¢†åŸŸé¡¶çº§ä¼šè®®ï¼ŒCCF è®¡ç®—æœºå›¾å½¢å­¦ä¸å¤šåª’ä½“ B ç±»ä¼šè®®
+ [ ] **[24]** He Z, Rosenberg K T, Perlin K. [**Exploring Configuration of Mixed Reality Spaces for Communication**](http://delivery.acm.org/10.1145/3320000/3312761/LBW0222.pdf?ip=219.216.73.1&id=3312761&acc=OPEN&key=BF85BBA5741FDC6E%2E4183B12E3311CD37%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1557107120_3c56b43468e3911f8111789083753416)[C]//Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM, **2019**: LBW0222.
    + <font color = blue>æ¢ç´¢æ··åˆç°å®ç©ºé—´çš„é€šä¿¡é…ç½®</font>
    + çº½çº¦å¤§å­¦ &emsp; CHIï¼šCCF äººæœºäº¤äº’ä¸æ™®é€‚è®¡ç®— A ç±»ä¼šè®®
#### 3. Semantic/Deep SLAM
+ [ ] **[25]** von Stumberg L, Wenzel P, Khan Q, et al. [**GN-Net: The Gauss-Newton Loss for Deep Direct SLAM**](https://arxiv.org/pdf/1904.11932.pdf)[J]. arXiv preprint arXiv:1904.11932, **2019**.
    + <font color = blue>GN-Netï¼šé«˜æ–¯ç‰›é¡¿æŸå¤±çš„æ·±åº¦ç›´æ¥æ³• SLAM </font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=jBgFEukAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[26]** Wang R, Yang N, Stueckler J, et al. [**DirectShape: Photometric Alignment of Shape Priors for Visual Vehicle Pose and Shape Estimation**](https://arxiv.org/pdf/1904.10097.pdf)[J]. arXiv preprint arXiv:1904.10097, 2019.
    + <font color = blue>DirectShapeï¼šè§†è§‰è½¦è¾†å§¿æ€å½¢çŠ¶ä¼°è®¡çš„å½¢çŠ¶å…ˆéªŒå…‰åº¦å¯¹å‡†</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=buN3yw8AAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[27]** Feng M, Hu S, Ang M, et al. [**2D3D-MatchNet: Learning to Match Keypoints Across 2D Image and 3D Point Cloud**](https://arxiv.org/pdf/1904.09742.pdf)[J]. arXiv preprint arXiv:1904.09742, 2019.
    + <font color = blue>2D3D-MatchNetï¼šå­¦ä¹ åŒ¹é… 2D å›¾åƒå’Œ 3D ç‚¹äº‘çš„å…³é”®ç‚¹</font>
    + æ–°åŠ å¡å›½ç«‹å¤§å­¦
+ [x] **[28]** Wei Y, Liu S, Zhao W, et al. [**Conditional Single-view Shape Generation for Multi-view Stereo Reconstruction**](https://arxiv.org/pdf/1904.06699.pdf)[J]. arXiv preprint arXiv:1904.06699, **2019**.
    + <font color = blue>å¤šè§†è§’ç«‹ä½“é‡å»ºçš„æ¡ä»¶å•è§†å›¾å¤–å½¢ç”Ÿæˆ</font>
    + æ¸…åå¤§å­¦ &emsp; [ä»£ç å¼€æº](https://github.com/weiyithu/OptimizeMVS)
+ [ ] **[29]** Behl A, Paschalidou D, DonnÃ© S, et al. [**Pointflownet: Learning representations for rigid motion estimation from point clouds**](http://ww.cvlibs.net/publications/Behl2019CVPR.pdf)[C]. **CVPR 2019**.
    + <font color = blue>Pointflownetï¼šä»ç‚¹äº‘å­¦ä¹ åˆšä½“è¿åŠ¨ä¼°è®¡çš„è¡¨ç¤º</font>
    + å›¾å®¾æ ¹å¤§å­¦ &emsp; å³å°†[å¼€æºä»£ç ](https://github.com/aseembehl/pointflownet)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
+ [ ] **[30]** Xue F, Wang X, Li S, et al. [**Beyond Tracking: Selecting Memory and Refining Poses for Deep Visual Odometry**](https://arxiv.org/pdf/1904.01892.pdf)[J]. arXiv preprint arXiv:1904.01892, **2019**.
    + <font color = blue>ä¸ºæ·±åº¦è§†è§‰æµ‹è·é€‰æ‹©è®°å¿†å’Œç»†åŒ–ä½å§¿</font>
    + åŒ—äº¬å¤§å­¦
    
#### 4. learning others
+ [ ] **[31]** Hou J, Dai A, NieÃŸner M. [**3D-SIC: 3D Semantic Instance Completion for RGB-D Scans**](https://arxiv.org/pdf/1904.12012.pdf)[J]. arXiv preprint arXiv:1904.12012, **2019**.
    + <font color = blue>RGB-Dæ‰«æçš„ 3D è¯­ä¹‰å®ä¾‹</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦
+ [ ] **[32]** Phalak A, Chen Z, Yi D, et al. [**DeepPerimeter: Indoor Boundary Estimation from Posed Monocular Sequences**](https://arxiv.org/pdf/1904.11595.pdf)[J]. arXiv preprint arXiv:1904.11595, **2019**.
    + <font color = blue>DeepPerimeterï¼šå•ç›®åºåˆ—å®¤å†…è¾¹ç•Œä¼°è®¡</font>
    + Magic Leap &emsp; [Google Scholor](https://scholar.google.com/citations?user=ji6BSBoAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[33]** Yang Z, Liu S, Hu H, et al. [**RepPoints: Point Set Representation for Object Detection**](https://arxiv.org/pdf/1904.11490.pdf)[J]. arXiv preprint arXiv:1904.11490, **2019**.
    + <font color = blue>RepPointsï¼šç›®æ ‡æ£€æµ‹çš„ç‚¹é›†è¡¨ç¤º</font>
    + åŒ—äº¬å¤§å­¦
+ [ ] **[34]** Jiang S, Xu T, Li J, et al. [**Foreground Feature Enhancement for Object Detection**](https://ieeexplore.ieee.org/abstract/document/8684952/authors#authors)[J]. IEEE Access, 2019, 7: 49223-49231.
    + <font color = blue>ç›®æ ‡æ£€æµ‹çš„å‰æ™¯ç‰¹å¾å¢å¼º</font>
    + åŒ—äº¬ç†å·¥å¤§å­¦
+ [ ] **[35]** Zakharov S, Shugurov I, Ilic S. [**DPOD: 6D Pose Object Detector and Refiner**]()[J]. **2019**.
    + <font color = blue>DPOD:6 è‡ªç”±åº¦ç‰©ä½“å§¿æ€æ£€æµ‹ä¸ç»†åŒ–</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ï¼Œè¥¿é—¨å­
+ [ ] **[36]** Liu C, Yang Z, Xu F, et al. [**Image Generation from Bounding Box-represented Semantic Labels**](https://www.sciencedirect.com/science/article/pii/S0097849319300329)[J]. Computers & Graphics, **2019**.
    + <font color = blue>ä»è¾¹ç•Œæ¡†è¡¨ç¤ºçš„è¯­ä¹‰æ ‡ç­¾ä¸­ç”Ÿæˆå›¾åƒ</font>
    + æ¸…åå¤§å­¦ &emsp; Computers & Graphics ä¸­ç§‘é™¢å››åŒºï¼ŒJCR Q3ï¼Œ IF 1.352
+ [ ] **[37]** Qiu Z, Yan F, Zhuang Y, et al. [**Outdoor Semantic Segmentation for UGVs Based on CNN and Fully Connected CRFs**](https://ieeexplore.ieee.org/abstract/document/8620277)[J]. IEEE Sensors Journal, **2019**.
    + <font color = blue>åŸºäº CNN å’Œå…¨è¿é€š CRF çš„ UGV å®¤å¤–è¯­ä¹‰åˆ†å‰²</font>
    + å¤§è¿ç†å·¥å¤§å­¦ &emsp; [ç‚¹äº‘å¤„ç†ä»£ç ](https://pan.baidu.com/s/1AofGdJwUSUcsMwgm9IMC-Q#list/path=%2F) &emsp; ä¸­ç§‘é™¢ä¸‰åŒºï¼ŒJCR Q2ï¼ŒIF 2.698
+ [ ] **[38]** Ma X, Wang Z, Li H, et al. [**Accurate Monocular 3D Object Detection via Color-Embedded 3D Reconstruction for Autonomous Driving**](https://arxiv.org/pdf/1903.11444.pdf)[J]. arXiv preprint arXiv:1903.11444, **2019**.
    + <font color = blue>ç”¨äºè‡ªåŠ¨é©¾é©¶çš„å½©è‰²åµŒå…¥å¼ä¸‰ç»´é‡å»ºç²¾å‡†å•ç›®ä¸‰ç»´ç‰©ä½“æ£€æµ‹</font>
    + å¤§è¿ç†å·¥å¤§å­¦
+ [ ] **[39]** Sindagi V A, Zhou Y, Tuzel O. [**MVX-Net: Multimodal VoxelNet for 3D Object Detection**](https://arxiv.org/pdf/1904.01649.pdf)[J]. arXiv preprint arXiv:1904.01649, **2019**.
    + <font color = blue>ç”¨äº**ä¸‰ç»´ç‰©ä½“æ£€æµ‹**çš„å¤šæ¨¡æ€ VoxelNet</font>
    + ç¾å›½çº¦ç¿°æ–¯Â·éœæ™®é‡‘æ–¯å¤§å­¦ &emsp; [ä¸ªäººä¸»é¡µ](http://www.vishwanathsindagi.com/)
+ [ ] **[40]** Li J, Lee G H. [**USIP: Unsupervised Stable Interest Point Detection from 3D Point Clouds**](https://arxiv.org/pdf/1904.00229.pdf)[J]. arXiv preprint arXiv:1904.00229, **2019**.
    + <font color = blue>ä¸‰ç»´ç‚¹äº‘çš„æ— ç›‘ç£ç¨³å®šå…´è¶£ç‚¹æ£€æµ‹</font>
    + æ–°åŠ å¡å›½ç«‹å¤§å­¦ &emsp; å³å°†[å¼€æºä»£ç ](https://github.com/lijx10/USIP)ï¼ˆè¿˜æœªæ”¾å‡ºï¼‰
#### 5. event
+ [x] **[41]** Scheerlinck C, Rebecq H, Stoffregen T, et al. [**CED: Color event camera dataset**](https://arxiv.org/pdf/1904.10772.pdf)[J]. arXiv preprint arXiv:1904.10772, **CVPRW 2019**.
    + <font color = blue>å½©è‰²**äº‹ä»¶ç›¸æœº**</font>
    + è‹é»ä¸–å¤§å­¦ &emsp; [é¡¹ç›®ä¸»é¡µ](http://rpg.ifi.uzh.ch/CED.html) &emsp; [Google Scholor](https://scholar.google.com/citations?user=zveWLBkAAAAJ&hl=zh-CN&oi=sra)
    + åŸºäºäº‹ä»¶çš„è§†è§‰ç ”ç©¶ï¼š[Event-based Vision: A Survey](https://arxiv.org/pdf/1904.08405.pdf). CVPR 2019
    + [Focus is all you need: Loss functions for event-based vision](https://arxiv.org/pdf/1904.07235.pdf). 2019
+ [ ] **[42]** Stoffregen T, Gallego G, Drummond T, et al. [**Event-based motion segmentation by motion compensation**](https://arxiv.org/pdf/1904.01293.pdf)[J]. arXiv preprint arXiv:1904.01293, **2019**.
    + <font color = blue>åŸºäº**äº‹ä»¶**çš„è¿åŠ¨è¡¥å¿è¿åŠ¨åˆ†å‰²</font>
    + æ¾³å¤§åˆ©äºšæœºå™¨äººè§†è§‰ä¸­å¿ƒï¼Œè‹é»ä¸–å¤§å­¦
    
#### 6. ä¼ æ„Ÿå™¨èåˆ
+ [ ] **[43]** Xiao Y, Ruan X, Chai J, et al. [**Online IMU Self-Calibration for Visual-Inertial Systems**](https://www.mdpi.com/1424-8220/19/7/1624/htm)[J]. Sensors, **2019**, 19(7): 1624.
    + <font color = blue>è§†è§‰æƒ¯æ€§ç³»ç»Ÿ **IMU åœ¨çº¿æ ‡å®š**</font>
    + åŒ—äº¬å·¥ä¸šå¤§å­¦ &emsp; Sensors å¼€æºæœŸåˆŠ
+ [x] **[44]** Eckenhoff K, Geneva P, Huang G. [**Closed-form preintegration methods for graph-based visualâ€“inertial navigation**](http://sage.cnpereading.com/paragraph/article/10.1177/0278364919835021)[J]. The International Journal of Robotics Research, 2018.
    + <font color = blue>åŸºäºå›¾çš„è§†è§‰æƒ¯æ€§å¯¼èˆªçš„**å°é—­å¼é¢„ç§¯åˆ†**æ–¹æ³•</font>
    + ç‰¹æ‹‰åå¤§å­¦ &emsp; [ä»£ç å¼€æº](https://github.com/rpng/cpi)
+ [ ] **[45]** Joshi B, Rahman S, Kalaitzakis M, et al. [**Experimental Comparison of Open Source Visual-Inertial-Based State Estimation Algorithms in the Underwater Domain**](https://arxiv.org/pdf/1904.02215.pdf)[J]. arXiv preprint arXiv:1904.02215, **2019**.
    + <font color = blue>å¼€æºè§†è§‰æƒ¯å¯¼ SLAM åœ¨æ°´ä¸‹çš„çŠ¶æ€ä¼°è®¡æ¯”è¾ƒ</font>
    + ç¾å›½å—å¡ç½—æ¥çº³å¤§å­¦å“¥ä¼¦æ¯”äºšåˆ†æ ¡ &emsp; [Google Scholor](https://scholar.google.com/citations?user=Izlp7JsAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[46]** Xia L, Meng Q, Chi D, et al. [**An Optimized Tightly-Coupled VIO Design on the Basis of the Fused Point and Line Features for Patrol Robot Navigation**](https://www.mdpi.com/1424-8220/19/9/2004/htm)[J]. Sensors, **2019**, 19(9): 2004.
    + <font color = blue>åŸºäºç‚¹çº¿ç‰¹å¾èåˆçš„å·¡æ£€æœºå™¨äººç´§è€¦åˆçš„ VIO </font>
    + ä¸œåŒ—ç”µåŠ›å¤§å­¦ &emsp; Sensors å¼€æºæœŸåˆŠ
+ [ ] **[47]** Ye H, Chen Y, Liu M. [**Tightly Coupled 3D Lidar Inertial Odometry and Mapping**](https://arxiv.org/pdf/1904.06993.pdf)[J]. arXiv preprint arXiv:1904.06993, 2019.
    + <font color = blue>ç´§è€¦åˆçš„**æ¿€å…‰æƒ¯æ€§**é‡Œç¨‹è®¡ä¸å»ºå›¾</font>
    + é¦™æ¸¯ç§‘æŠ€å¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=CdV5LfQAAAAJ&hl=zh-CN&oi=sra)
    + [Focal loss in 3d object detection](https://arxiv.org/pdf/1809.06065.pdf) [J]IEEE Robotics and Automation Letters 4 (2), 1263-1270, **2019**.
+ [ ] **[48]** Usenko V, Demmel N, Schubert D, et al. [**Visual-Inertial Mapping with Non-Linear Factor Recovery**](https://arxiv.org/pdf/1904.06504.pdf)[J]. arXiv preprint arXiv:1904.06504, **2019**.
    + <font color = blue>å…·æœ‰éçº¿æ€§å› å­æ¢å¤çš„**è§†è§‰-æƒ¯å¯¼å»ºå›¾**</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=APTNKjoAAAAJ&hl=zh-CN&oi=sra)
+ [ ] **[49]** Qiu X, Zhang H, Fu W, et al. [**Monocular Visual-Inertial Odometry with an Unbiased Linear System Model and Robust Feature Tracking Front-End**](https://www.mdpi.com/1424-8220/19/8/1941/htm)[J]. Sensors, **2019**, 19(8): 1941.
    + <font color = blue>å…·æœ‰æ— åå·®çº¿æ€§æ¨¡å‹å’Œå‰ç«¯é²æ£’ç‰¹å¾è·Ÿè¸ªçš„**å•ç›®è§†è§‰æƒ¯å¯¼é‡Œç¨‹è®¡**</font>
    + å¤šä¼¦å¤šå¤§å­¦ &emsp; [Google Scholor](https://scholar.google.com/citations?user=lSlo1RgAAAAJ&hl=zh-CN&oi=sra) &emsp; Sensors å¼€æºæœŸåˆŠ

#### 7. Others
+ [ ] **[50]** Liu Y, Knoll A, Chen G. [**A New Method for Atlanta World Frame Estimation**](https://arxiv.org/pdf/1904.12717.pdf)[J]. arXiv preprint arXiv:1904.12717, **2019**.
    + <font color = blue>äºšç‰¹å…°å¤§ä¸–ç•Œæ¡†æ¶ä¼°è®¡çš„ä¸€ç§æ–°æ–¹æ³•</font>
    + æ…•å°¼é»‘å·¥ä¸šå¤§å­¦
+ [x] **[51]** Zhao Y, Qi J, Zhang R. [**CBHE: Corner-based Building Height Estimation for Complex Street Scene Images**](https://arxiv.org/pdf/1904.11128.pdf)[J]. arXiv preprint arXiv:1904.11128, **2019**.
    + <font color = blue>åŸºäºè§’ç‚¹çš„å¤æ‚è¡—æ™¯å›¾åƒ**å»ºç­‘ç‰©é«˜åº¦ä¼°è®¡**</font>
    + å¢¨å°”æœ¬å¤§å­¦
	
---

### 2019 å¹´ 4 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ17 ç¯‡ï¼‰

+ [x] **[1]** Rambach J, Lesur P, Pagani A, et al. [**SlamCraft: Dense Planar RGB Monocular SLAM**](https://www.researchgate.net/profile/Jason_Rambach/publication/331832804_SlamCraft_Dense_Planar_RGB_Monocular_SLAM/links/5c8f6a9a299bf14e7e82d880/SlamCraft-Dense-Planar-RGB-Monocular-SLAM.pdf)[C]. International Conference on Machine Vision Applications MVA **2019**.
    + SlamCraftï¼š**å•ç›®å¹³é¢ç¨ å¯†** SLAM
    + å¾·å›½äººå·¥æ™ºèƒ½ç ”ç©¶ä¸­å¿ƒ &emsp;[**ä½œè€…ä¸»é¡µ**](https://av.dfki.de/members/rambach/) &emsp;[**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=1l4G16AAAAAJ&hl=zh-CN&authuser=1&oi=sra) &emsp;**å¢å¼ºç°å®**åº”ç”¨
+ [ ] **[2]** Liu C, Yang J, Ceylan D, et al. [**Planenet: Piece-wise planar reconstruction from a single rgb image**](http://openaccess.thecvf.com/content_cvpr_2018/papers/Liu_PlaneNet_Piece-Wise_Planar_CVPR_2018_paper.pdf)[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **CVPR 2018**: 2579-2588.
    + **PlaneNet**ï¼šä»å•å¼  RGB å›¾åƒè¿›è¡Œ**å¹³é¢é‡æ„**
    + åç››é¡¿å¤§å­¦ &emsp;[**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=Jy3u1_wAAAAJ&hl=zh-CN&authuser=1&oi=sra)  &emsp;[**Github ä»£ç å¼€æº**](https://github.com/art-programmer/PlaneNet) 
+ [ ] **[3]** Weng X, Kitani K. [**Monocular 3D Object Detection with Pseudo-LiDAR Point Cloud**](https://arxiv.org/pdf/1903.09847.pdf)[J]. arXiv preprint arXiv:1903.09847, **2019**.
    + åˆ©ç”¨ä¼ªæ¿€å…‰ç‚¹äº‘è¿›è¡Œå•ç›® **3D ç‰©ä½“æ£€æµ‹**
    + CMU  &emsp;[**è°·æ­Œå­¦æœ¯**](https://scholar.google.com/citations?user=yv3sH74AAAAJ&hl=zh-CN&oi=sra) 
+ [ ] **[4]** Hassan M., Mohamed & Hemayed, Elsayed.. [**A Fast Linearly Back-End SLAM for Navigation Based on Monocular Camera**](https://www.researchgate.net/publication/331832850_A_Fast_Linearly_Back-End_SLAM_for_Navigation_Based_on_Monocular_Camera). International Journal of Civil Engineering and Technology **2018**. 627-645. 
    + å•ç›® SLAM çš„å¿«é€Ÿ**çº¿æ€§åç«¯ä¼˜åŒ–**
    + åŸƒåŠæ³•å°¤å§†å¤§å­¦
+ [ ] **[5]** Chen B, Yuan D, Liu C, et al. [**Loop Closure Detection Based on Multi-Scale Deep Feature Fusion**](https://www.mdpi.com/2076-3417/9/6/1120/htm)[J]. Applied Sciences, **2019**, 9(6): 1120.
    + åŸºäºå¤šå°ºåº¦**æ·±åº¦ç‰¹å¾èåˆ**çš„**é—­ç¯æ£€æµ‹**
    + ä¸­å—å¤§å­¦è‡ªåŠ¨åŒ–å­¦é™¢
+ [x] **[6]** Ling Y, Shen S. [**Realâ€time dense mapping for online processing and navigation**](https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.21868)[J]. Journal of Field Robotics.
    + ç”¨äºåœ¨çº¿å¤„ç†å’Œå¯¼èˆªçš„å®æ—¶**å¯†é›†å»ºå›¾**
    + **æ²ˆé‚µåŠ¼**è€å¸ˆå›¢é˜Ÿ &emsp;[**Github ä»£ç å¼€æº**](https://github.com/ygling2008/dense_mapping) 
+ [x] **[7]** Chen-Hsuan Lin, Oliver Wang et al.[**Photometric Mesh Optimization for Video-Aligned 3D Object Reconstruction**](https://arxiv.org/pdf/1903.08642.pdf)[C].IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**), **2019**
    + ç”¨äºè§†é¢‘**ä¸‰ç»´ç‰©ä½“é‡å»º**çš„å…‰åº¦ç½‘æ ¼ä¼˜åŒ–
    + CMU åœ¨è¯»åšå£« &emsp;[**ä¸ªäººä¸»é¡µ**](https://chenhsuanlin.bitbucket.io/) &emsp;[**Github ä»£ç å¼€æº**](https://github.com/chenhsuanlin/photometric-mesh-optim)
+ [x] **[8]** Tang F, Li H, Wu Y. [**FMD Stereo SLAM: Fusing MVG and Direct Formulation Towards Accurate and Fast Stereo SLAM**](https://www.researchgate.net/publication/331198086_FMD_Stereo_SLAM_Fusing_MVG_and_Direct_Formulation_Towards_Accurate_and_Fast_Stereo_SLAM)[J]. **2019**.
    + èåˆå¤šè§†å›¾å‡ ä½•ä¸ç›´æ¥æ³•çš„å¿«é€Ÿç²¾å‡†**åŒç›® SLAM**
    + **ä¸­ç§‘é™¢è‡ªåŠ¨åŒ–ç ”ç©¶æ‰€**ï¼Œæ¨¡å¼è¯†åˆ«å›½å®¶é‡ç‚¹å®éªŒå®¤ï¼Œå´æ¯…çº¢å›¢é˜Ÿ
+ [ ] **[9]** Duff T, Kohn K, Leykin A, et al. [**PLMP-Point-Line Minimal Problems in Complete Multi-View Visibility**](https://arxiv.org/pdf/1903.10008.pdf)[J]. arXiv preprint arXiv:1903.10008, **2019**.
    + PLMPï¼šå¤šè§†å›¾ä¸­çš„**ç‚¹çº¿æœ€å°åŒ–**
    + ä½æ²»äºšç†å·¥å­¦é™¢
+ [x] **[10]** Seong Hun Lee, Javier Civera. [**Loosely-Coupled Semi-Direct Monocular SLAM**](https://ieeexplore.ieee.org/abstract/document/8584894)[J] IEEE Robotics and Automation Letters. **2019**
    + æ¾è€¦åˆçš„**åŠç›´æ¥æ³•å•ç›® SLAM**
    + è¨æ‹‰æˆˆè¨å¤§å­¦ï¼Œ[è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?user=FeMFP7EAAAAJ&hl=zh-CN&oi=sra)ï¼Œ[**ä»£ç å¼€æº**](https://github.com/wuxiaolang/LCSD_SLAM)ï¼Œ[æ¼”ç¤ºè§†é¢‘](https://www.youtube.com/watch?v=j7WnU7ZpZ8c&feature=youtu.be)
    + Lee S H, de Croon G. [**Stability-based scale estimation for monocular SLAM**](https://www.researchgate.net/profile/Seong_Hun_Lee3/publication/322260802_Stability-based_Scale_Estimation_for_Monocular_SLAM/links/5b3def9b0f7e9b0df5f42d67/Stability-based-Scale-Estimation-for-Monocular-SLAM.pdf)[J]. IEEE Robotics and Automation Letters, **2018**, 3(2): 780-787.
    + Lee S H, Civera J. [**Closed-Form Optimal Triangulation Based on Angular Errors**](https://arxiv.org/pdf/1903.09115.pdf)[J]. arXiv preprint arXiv:1903.09115, **2019**.
+ [x] **[12]** Jinyu Li, Bangbang Yang, Danpeng Chen, Nan Wang, Guofeng Zhang*, Hujun Bao*. [**Survey and Evaluation of Monocular Visual-Inertial SLAM Algorithms for Augmented Reality**](http://vr-ih.com/vrih/resource/latest_accept/267415796648960.pdf)[J] Journal of Virtual Reality & Intelligent Hardware **2019**.
    + åº”ç”¨äº**å¢å¼ºç°å®**çš„**å•ç›® VI-SLAM** ç®—æ³•è°ƒç ”ä¸è¯„ä¼°
    + ç« å›½é”‹æ•™æˆå›¢é˜Ÿï¼Œ[å·¥ç¨‹åœ°å€](http://www.zjucvg.net/eval-vislam/)ï¼Œ[Github-è¯„ä¼°å·¥å…·](https://github.com/zju3dv/eval-vislam)
+ [x] **[13]** Pablo Speciale, Johannes L. Schonberg, Sing Bing Kang. [**Privacy Preserving Image-Based Localization**](https://arxiv.org/pdf/1903.05572.pdf)[J] **2019**.
    + åˆ©ç”¨**çº¿äº‘**è¿›è¡ŒåŸºäºå›¾åƒçš„å®šä½
    + **è‹é»ä¸–**è”é‚¦ç†å·¥ã€å¾®è½¯ï¼Œ[ä½œè€…ä¸»é¡µ](http://people.inf.ethz.ch/sppablo/)ï¼Œ[å·¥ç¨‹åœ°å€](https://www.cvg.ethz.ch/research/secon/)
    + Speciale P, Pani Paudel D, Oswald M R, et al. **Consensus maximization with linear matrix inequality constraints**[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. **CVPR 2017**: 4941-4949. æœ€å¤§åŒ–çº¿æ€§çŸ©é˜µä¸ç­‰å¼çº¦æŸ [[PDF](https://www.cvg.ethz.ch/research/conmax/paper/PSpeciale2017CVPR.pdf)] [[Code](https://www.cvg.ethz.ch/research/conmax/paper/PSpeciale2017CVPR_code_sample.tar.gz)] [[Video](https://www.youtube.com/watch?v=yV1wXknpG1U)] [[Project Page](https://www.cvg.ethz.ch/research/conmax)]
+ [ ] **[14]** Li M, Zhang W, Shi Y, et al. [**Bionic Visual-based Data Conversion for SLAM**](https://ieeexplore.ieee.org/abstract/document/8665130)[C]//2018 IEEE International Conference on Robotics and Biomimetics (**ROBIO**). IEEE, **2018**: 1607-1612.
    + åŸºäº**ä»¿ç”Ÿè§†è§‰**çš„ SLAM æ•°æ®è½¬æ¢
    + **åŒ—äº¬ç†å·¥å¤§å­¦**ä»¿ç”Ÿæœºå™¨äººä¸ç³»ç»Ÿæ•™è‚²éƒ¨é‡ç‚¹å®éªŒå®¤
+ [ ] **[15]** Cheng J, Sun Y, Chi W, et al. [**An Accurate Localization Scheme for Mobile Robots Using Optical Flow in Dynamic Environments**](https://ieeexplore.ieee.org/abstract/document/8664893)[C]//2018 IEEE International Conference on Robotics and Biomimetics (**ROBIO**). IEEE, **2018**: 723-728.
    + **åŠ¨æ€ç¯å¢ƒ**ä¸‹ä½¿ç”¨**å…‰æµ**çš„ç§»åŠ¨æœºå™¨äººç²¾ç¡®å®šä½æ–¹æ¡ˆ
    + **é¦™æ¸¯ä¸­æ–‡å¤§å­¦**ï¼Œ[**å®éªŒå®¤ä¸»é¡µ**](http://www.ee.cuhk.edu.hk/~qhmeng/index.html)
+ [x] **[16]** Zichao Zhang, Davide Scaramuzza, [**Beyond Point Clouds: Fisher Information Field for Active Visual Localization**](https://www.ifi.uzh.ch/dam/jcr:5a26a3b4-ce01-4647-8cf6-f6a24e579a85/ICRA19_Zhang.pdf).[C], IEEE International Conference on Robotics and Automation (**ICRA**), **2019**.
    + **è¶…è¶Šç‚¹äº‘**:ç”¨äºä¸»åŠ¨è§†è§‰å®šä½çš„ Fisher ä¿¡æ¯
    + è‹é»ä¸–å¤§å­¦å¼ å­æ½®ï¼Œ[è§†é¢‘](https://www.youtube.com/watch?v=q3YqIyaFUVE&feature=youtu.be)ï¼Œ[**ä»£ç å¼€æº**](https://github.com/uzh-rpg/rpg_information_field)ï¼Œ[é¡¹ç›®ä¸»é¡µ](http://rpg.ifi.uzh.ch/research_active_vision.html)
+ [x] **[17]** Georges Younes, Daniel Asmar, John Zelek. [**A Unified Formulation for Visual Odometry**](https://arxiv.org/abs/1903.04253)[J]. arXiv preprint arXiv:1903.04253, **2019**.
    + ä¸€ç§ç»Ÿä¸€çš„**è§†è§‰é‡Œç¨‹è®¡**æ–¹æ³•
    + åŠ æ‹¿å¤§æ»‘é“å¢å¤§å­¦ï¼Œè´é²ç‰¹ç¾å›½å¤§å­¦ï¼› [è°·æ­Œå­¦æœ¯](https://scholar.google.com/citations?hl=zh-CN&user=4Xy_9NQAAAAJ)
    + Younes G, Asmar D, Shammas E, et al. [Keyframe-based monocular SLAM: design, survey, and future directions](https://www.sciencedirect.com/science/article/pii/S0921889017300647)[J]. Robotics and Autonomous Systems, **2017**, 98: 67-88.
    + 2018ï¼š[Fdmo: Feature assisted direct monocular odometry](https://arxiv.org/pdf/1804.05422.pdf)

---

### 2019 å¹´ 3 æœˆè®ºæ–‡æ›´æ–°ï¼ˆ13 ç¯‡ï¼‰

+ [ ] **[1]** Han L, Gao F, Zhou B, et al. [**FIESTA: Fast Incremental Euclidean Distance Fields for Online Motion Planning of Aerial Robots**](https://arxiv.org/abs/1903.02144)[J]. arXiv preprint arXiv:1903.02144, **2019**.
    + åŸºäºå¿«é€Ÿ**å¢é‡å¼æ¬§æ°è·ç¦»åœº**çš„é£è¡Œå™¨å®æ—¶è¿åŠ¨è§„åˆ’
    + æ²ˆé‚µåŠ¼è€å¸ˆå›¢é˜Ÿ
+ [x] **[2]** ICRA **2019** ï¼š[**Multimodal Semantic SLAM with Probabilistic Data Association**](https://marinerobotics.mit.edu/multimodal-semantic-slam-probabilistic-data-association)
    + å…·æœ‰æ¦‚ç‡æ•°æ®å…³è”çš„**å¤šæ¨¡æ€è¯­ä¹‰SLAM**
    + éº»çœç†å·¥å­¦é™¢æµ·æ´‹æœºå™¨äººå›¢é˜Ÿ
+ [x] **[3]** Zhang F, Rui T, Yang C, et al. [**LAP-SLAM: A Line-Assisted Point-Based Monocular VSLAM**](https://www.mdpi.com/2079-9292/8/2/243/htm)[J]. **Electronics**, **2019**, 8(2): 243.
    + **çº¿è¾…åŠ©**çš„ç‚¹ SLAM
    + è§£æ”¾å†›é™†å†›å·¥ç¨‹å¤§å­¦
+ [ ] **[4]** Zhang H, Jin L, Zhang H, et al. [**A Comparative Analysis of Visual-Inertial SLAM for Assisted Wayfinding of the Visually Impaired**](https://ieeexplore.ieee.org/abstract/document/8658410)[C]//2019 IEEE Winter Conference on Applications of Computer Vision (**WACV**). IEEE, **2019**: 210-217.
    + **VI-SLAM** è¾…åŠ©å¯»å¾„å¯¹æ¯”åˆ†æ
    + å¼—å‰å°¼äºšè”é‚¦å¤§å­¦
+ [x] **[5]** Chen Z, Liu L. [**Creating Navigable Space from Sparse Noisy Map Points**](https://arxiv.org/pdf/1903.01503.pdf)[J]. arXiv preprint arXiv:1903.01503, **2019**.
    + ä»**ç¨€ç–åœ°å›¾ç‚¹**ä¸­åˆ›å»º**å¯å¯¼èˆª**ç©ºé—´
+ [x] **[6]** Antigny N, Uchiyama H, ServiÃ¨res M, et al. [**Solving monocular visual odometry scale factor with adaptive step length estimates for pedestrians using handheld devices**](https://www.mdpi.com/1424-8220/19/4/953)[J]. **Sensors**, **2019**, 19(4): 953.
    + è¡Œäººæ­¥é•¿ä¼°è®¡è¾…åŠ©**å•ç›®æ‰‹æŒå¼è§†è§‰é‡Œç¨‹è®¡å°ºåº¦ä¼°è®¡** **åŸå¸‚ç¯å¢ƒ AR**åº”ç”¨
    + æ³•å›½è¿è¾“\è§„åˆ’å’Œç½‘ç»œç§‘å­¦ä¸æŠ€æœ¯ç ”ç©¶æ‰€ï¼Œ[**researchgate**](https://www.researchgate.net/profile/Nicolas_Antigny) &emsp; [**YouTube**](https://www.youtube.com/channel/UC7zmj2Eonwi5kCfoe1r68dA/featured)
+ [x] **[7]** Zhou D, Dai Y, Li H. [**Ground Plane based Absolute Scale Estimation for Monocular Visual Odometry**](https://ieeexplore.ieee.org/abstract/document/8708682/)[J]. arXiv preprint arXiv:1903.00912, **2019**.
    + åŸºäºåœ°å¹³é¢çš„**å•ç›® SLAM ç»å¯¹å°ºåº¦ä¼°è®¡**
    + ç™¾åº¦
+ [ ] **[8]** Duong N D, Kacete A, Soladie C, et al. [**Accurate Sparse Feature Regression Forest Learning for Real-Time Camera Relocalization**](https://ieeexplore.ieee.org/abstract/document/8491017)[C]//2018 International Conference on 3D Vision (3DV). IEEE, **2018**: 643-652.
    + åŸºäºéšæœºæ£®æ—å­¦ä¹ çš„å®æ—¶**ç›¸æœºé‡å®šä½**
    + [è§†é¢‘](https://www.youtube.com/watch?v=2oOL-BSemmQ&feature=youtu.be)
+ [ ] **[9]** Patra S, Gupta K, Ahmad F, et al. [**EGO-SLAM: A Robust Monocular SLAM for Egocentric Videos**](https://ieeexplore.ieee.org/abstract/document/8658783/)[C]//2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, **2019**: 31-40.
    + **è§†é¢‘åºåˆ—**é²æ£’çš„**å•ç›® SLAM**
    + å°åº¦ç†å·¥å­¦é™¢
+ [x] **[10]** Rosinol A, Sattler T, Pollefeys M, et al. [**Incremental Visual-Inertial 3D Mesh Generation with Structural Regularities**](https://arxiv.org/pdf/1903.01067.pdf)[J]. arXiv preprint arXiv:1903.01067, **2019**.
    + å¢é‡å¼ VI-SLAM **ä¸‰ç»´ç½‘æ ¼**ç”Ÿæˆ
    + éº»çœç†å·¥å­¦é™¢ä¿¡æ¯ä¸å†³ç­–ç³»ç»Ÿå®éªŒå®¤ï¼Œ[é¡¹ç›®ä¸»é¡µ](https://www.mit.edu/~arosinol/research/struct3dmesh.html)
+ [ ] **[11]** Wang Z. [**Structure from Motion with Higher-level Environment Representations**](https://openresearch-repository.anu.edu.au/handle/1885/157021)[J]. **2019**.
    + å…·æœ‰**é«˜çº§ç¯å¢ƒ**è¡¨ç¤ºçš„ **SFM**
    + æ¾³å¤§åˆ©äºšå›½ç«‹å¤§å­¦ ç¡•å£«å­¦ä½
+ [x] **[12]** Vakhitov A, Lempitsky V. [**Learnable Line Segment Descriptor for Visual SLAM**](https://ieeexplore.ieee.org/abstract/document/8651490)[J]. IEEE Access, **2019**.
    + è§†è§‰SLAMä¸­çš„å¯å­¦ä¹ **çº¿æ®µæè¿°**ï¼ŒåŸºäº ORB-SLAM2
    + Samsung AI Center, Moscow
+ [ ] **[13]** Grinvald M, Furrer F, Novkovic T, et al. [**Volumetric Instance-Aware Semantic Mapping and 3D Object Discovery**](https://arxiv.org/abs/1903.00268)[J]. arXiv preprint arXiv:1903.00268, **2019**.
    + **è¯­ä¹‰**æ„ŸçŸ¥å»ºå›¾ä¸**ä¸‰ç»´ç‰©ä½“æ¢ç´¢**ï¼ŒåŸºäº mask-RCNN
    + è‹é»ä¸–è”é‚¦ç†å·¥å­¦é™¢

---
> wuyanminmax@gmail.com    
